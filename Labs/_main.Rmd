---
title: "Introduction to R Markdown"
author: "Intro to Data Science for Biology"
date: "September 8, 2016"
output: html_document
---
## This is what we want to avoid
<iframe width="560" height="315" src="https://www.youtube.com/embed/s3JldKoA0zw" frameborder="0" allowfullscreen></iframe>

## Why RMarkdown?
One of the central goals of data analysis is to communicate just what in the heck you have found. As you create analyses in any coded language, you'll undoubtedly produce a number of awesome results that will need a wee bit of explanation. Rather than just sent collaborators your R code and say 'run it' (presuming they can run R at all), you want a dynamic document where you can discuss results.

More than that, as you bounce your work back and forth among colleagues (or with reviewers), you're going to need to quickly make changes - both to the analyses and accompanying text. You might even re-run the whole shebang with a whole new set of data.

Enter [RMarkdown](http://rmarkdown.rstudio.com/)

RMarkdown allows you to swiftly and with little to no new knowledge incorporate R into documents - HTML, PDF, Word, etc. By embedding R alongside written text with nice formatting, you can easily communicate your results, and provide a fully reproducible workflow.

##So you want to setup a markdown document 
So how does RMarkdown work? Let's start a markdown document, and you can see.  First, create a new markdown document.

![](images/01/new_markdown.jpg)

This will lead you to a dialogue box about what kind of markdown document you want to create. Typically, you'll just be whipping up a word, pdf, or html document. As you can see, though, there are a lot of options - from slides to web apps with Shiny to any number of cutomized formats that you can install packages for at http://rmarkdown.rstudio.com/formats.html - for example, the slides I build using Reveal.js are from a custom library.

![](images/01/markdown_setup.jpg)

Let's create an HTML document called `My First Markdown`

## YAML
At the top of the document you'll see a block like
```{r eval=FALSE}
---
title: "My First Markdown"
author: "Intro to Data Science for Biology"
date: "September 8, 2016"
output: html_document
---
```
This is header information in the YAML (Yet Another Markup Language) format. There are a lot of ways to modify YAML, many of which are covered in specific sections of http://rmarkdown.rstudio.com/ - although for now, you'll likely not need to change too much. Note the output type. Indeed, all you need to do is change that line, and you can go from HTML to Word formats seamlessly.

## The markdown
Next you see the actual document which will look something like.

```{r, eval=FALSE}
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. 

```

OK, there's a lot here First, note the top with the `##` - that's markdown syntax! It says, make the rest of the line bold. As opposed to the ``**knit**` which just bolds the word Knit.

## What about R code?

Next, note the embedded code

```{r, eval=FALSE}
You can embed an R code chunk like this:

```{r cars}
summary(cars)
```
```

The backtick and bracket notation defines a code block. The code block also is given a name - cars. This is so that as figures, etc., get generated, or if you cache an analysis, you have a good identifier for the output. It also helps with finding a code block later on.

## Including Plots

You can also embed plots, for example:

```{r plotblock, eval=FALSE}
```{r pressure, echo=FALSE}
plot(pressure)
```
```

Which produces

```{r pressureplot, echo=FALSE}
plot(pressure)
```

Note two things here - first, PLOTS! Second, the `echo` argument. You can put in a number of arguments - such as `echo=FALSE`, eval, fig.height, fig.width, and many more. For more arguments, see http://yihui.name/knitr/options/


## That's really it....
At this point, armed with the cheat sheet, that's most of what you'll need to know about RMarkdown. Try knitting the current default document that's created. Then try adding a few code chunks, as well as several kinds of syntax from the cheat sheet. Try italics, emdash, and a list! But the beauty of Rmarkdown - and markdown in general - is its simplicity.

<!--chapter:end:01_markdown_intro.Rmd-->

---
title: "Simulation for Estimation of Parameter Uncertainty"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Setting the stage

### 1.1 Pipes

Often in R, what we kludge together different pieces of code to generate a sequence of events. For example, let's say I wanted to take a vector , find out it's length, and get the square root. Then get the log of that. There are a few ways I could do this.

```{r nonpipe}
vec <- 1:10
len <- length(vec)
sq <- sqrt(len)
log(sq)
```

This makes sense, and is very logical, but I keep saving new variables as I go along. I could write some nested piece of code that does the same thing with no variables.

```{r nonpipe2}
log(sqrt(length(1:10)))
```

Which is efficient, but, man, I hope I can keep track of parentheses. It's also ugly as hell. And super difficult for future-me to understand.

This idea of doing one thing, then the next, then the next is often how we treat our data. We take a piece of data, sift it, transform it, summarize it, etc. And this step-by-step style of coding has a name - **functional programming**. It lies at the core of much of what we do in R.

So, is there a better way? Yes!  What if we could take the output of one function, and pass it to the next? This is known as piping. And it is implemented in R by the `magrittr` library.  (Pause. Get the joke. Laugh at it. Move on.) `magrittr` introduces a funny new operator `%>%` which says, take the output of the thing on the right hand side, and pass it to the next function as its first argument. 

As a simple example, let's sum one through ten using pipes.

```{r pipesum}
#If you haven't, install.packages("magrittr") first
library(magrittr)
1:10 %>% sum()
```

Notice that we supplied no argument to sum. Now, what if wanted to take the square root of the result?

```{r pipe_sum_sqrt}
1:10 %>% 
  sum() %>%
  sqrt()
```

Notice how I separated different steps on different lines? This is a more readable code. Future me can track what's going on, line by line, instead of having to read a looooong single line of code.

Let's try a few exercises with pipes.

a) Use pipes to sum the log of 100:200.

b) Use pipes to take the square root of the mean of 100 random uniform numbers.

c) Let's dive into the guts of R. Using the `mtcars` data frame, get it's summary and str that summary. What do you get back?


### 1.2 Plot

The last basic thing we'll need today is plotting, as we'll want to see the results of our simulations. Next week we'll learn `ggplot2` and get fancy. For the moment, I just want you to know a few things in baseplot.

First, here's how to make a histogram.

```{r hist}
#just some random numbers
vals <- runif(50, 5, 10)

#the histogram
hist(vals)
```

Easy, no?

Now, what if I wanted to plot x by y? A scatterplot. R uses an equation-like syntax for this, where I plot `y ~ x`. We'll use this again and again as we fit linear models. For now, here's an example scatterplot.

```{r scatterplot}
#first, a data frame
my_df <- data.frame(x=1:10, y=10:1)

#note how data is used
#as well as column names in the equation
plot(y ~ x, data=my_df)
```

Now, you can supply all sorts of arguments on plot - look at the help file. We're not really going to worry about those today.

Let's try some examples.

a) Using `mtcars`, plot mpg by cyl. What happens if you set pch to 19 or 5?

b) `boxplot()` works like plot. Use it for `mpg ~ vs`. Now try the col argument. Give it a vector of two colors, like `c("red", "blue")`.

## 2. Dplyr

`dplyr` will change your life. It is the data munging tool you didn't know you needed until now. To learn our way around it, we'll use it with the `mtcars` data frame. `dplyr` is a collection of functions that manipulate data frames (or data tables, or tibbles, or other objects that work like data frames - more on those later). It's functions largely take the data frame object as the first argument, and then return data frames as the output, so pipes are a natural fit for `dplyr`.  Indeed, if you load `dplyr`, `magrittr` is loaded by default.

### 2.1 `mutate()`

Often, we want to make a column that uses information from other columns. For this, we have mutate. For example, what if we wanted to make a log transformed column of `mpg`? Note, I'm going to wrap the whole thing in head, so we only get the first 6 lines of the result

```{r mutate_mpg}
library(dplyr)

mtcars2 <-  mutate(mtcars, log_mpg = log(mpg)) 

head(mtcars2)
```

OK, the first argument was the data frame. Then, we just specified a new column from whole cloth, but used one of the columns from mtcars to make it. How easy! It's even easier with pipes.

```{r mutate_mpg_pipe}
mtcars2 <- mtcars %>%
    mutate(log_mpg = log(mpg))

head(mtcars2)
```

### 2.2 `group_by()`
One of the ways we use `dplyr` the most is `group_by()`. This lets us group by different unique values of a column - think treatments or categories - and do something to those sub-data-frames.  For example, let's say we wanted to group by number of gears, and then use mutate to make a new column that's the average of the mpg values.

```{r group}
mtcars_group <- mtcars %>%
  group_by(gear) %>%
  mutate(avg_mpg = mean(mpg)) %>%
  ungroup()

head(mtcars_group)
```

Notice the `ungroup()` at the end. That's provided as some functions bork if you give them a grouped data frame. We want to remove that structure.

Now see that final column - each row with the same number of gears has the same avg_mpg value.

### 2.3 `summarize()`

Often, you don't want to have a data frame with summarized information repeated. You just want to reduce your data frame down to summarized information. In Excel, this is pivot tables, basically. For that, we use a combination of `group_by()` and `summarize()`  Let's do the same thing as the previous example, only let's add the standard deviation, and return just the summarized information.

```{r mtcars_summary}
mtcars %>%
  group_by(gear) %>%
  summarize(avg_mpg = mean(mpg),
            sd_mpg = sd(mpg)) %>% 
  ungroup()
```

Whoah!  Cool!

You try it, but group by gear and vs, and look at how that alters weight. Yes, you can group by multiple things in the same `group_by` statement - just separate them with a comma!

### 2.4 `select()`
Often when working with a data frame, you end up with some column you no longer want. Maybe it was intermediate to some calculation. Or maybe you just want only a few columns to save memory. Either way, select is a way to include or exclude columns. 

```{r select}
head(select(mtcars2, -mpg))
```

Uh oh! Where did mpg go? Gone!

Or

```{r select2}
mtcars3 <- mtcars %>%
  select(wt, am, gear)

head(mtcars3)
```

One can even use some more imprecise matches if need be.

```{r}
head(select(mtcars2, contains("mpg")))
```


### 2.5 `filter()`

We can also exclude by row values using filter. This function takes all manner of comparisons, and returns only those rows for which the comparison is true. For example, to get rid of 3 cylinder cars:

```{r filter1}
mtcars_filter <- mtcars %>%
  filter(cyl != 3)

head(mtcars_filter)
```

I'll often use this to remove rows with `NA` values for some column, such as 
```{r filter2, eval=FALSE}
mtcars %>%
  filter(!is.na(cyl))
```


## 3. Simulation and Sample Size



### 3.1 Random Numbers and `sample()`

There are a number of ways to get random numbers in R from a variety of distributions.  For our simulations, let's start with a sample of 40 individuals that's from a population with a mean value of 10, a SD of 3.

```{r samp}
set.seed(323)
samp <- rnorm(n = 40, mean = 10, sd = 3)
```

Note the `set.seed()` function. Random number generators on computers aren't REALLY random - they use some nugget of information that changes constantly to be pseudo-random. By setting that nugget - the seed - we make sure that all random numbers generated from this point on are the same.

Unsettling, no?

So, we can pull a random sample from a perfect population anytime we'd like. How do we get a bootstrap sample from a population?

Why, with the sample function. Let's sample the sample we've greated above, and draw five different elements.

```{r sample1}
sample(samp, 5)
```

This is *incredibly* useful in a wide variety of context. For example, let's say we wanted five rows of mtcars.

```{r sample_mtcars}
mtcars[sample(1:nrow(mtcars), 5), ]
```

Of course, for that kind of thing, `dplyr` has you covered with 

```{r sample_dplyr}
sample_n(mtcars, 5)
```

Or, you can even use a porportions - so, here's a random sampling of 10% of mtcars

```{r sample_dplyr_frac}
sample_frac(mtcars, 0.1)
```

But that is neither here nor there. 

One thing that all of these sample functions let you do is sample with or without replacement. If we're assuming a sample is representative of a population and we want to bootstrap resample it, we're going to need to sample **with replacement**. Fortunately, that's just one more argument.

```{r sample_replace}
sample(1:3, 15, replace = TRUE)
```

This will come in handy shortly!

### 3.2 Using `group_by()` for simulation

The other tool we'll need to simulate sampling a population is some output. Let's go through this step by step. First, let's say we are going to have 10 samples per sample size, and try sample sizes from 3 to 50. We begin by creating a data frame with this information, as we're going to want to plot this information later.

```{r start_n}
#use rep to repeat the sample sizes 3 through 50 10 times
sampSim <- data.frame(samp_size = rep(3:50, times = 10))
```

We also want to keep track of simulation number, as we're going to want to perform independent actions for each sim.

```{r setup_sim}
sampSim$sim_number = 1:nrow(sampSim)
```

Now, we're ready to go! What we want to do is, for each simulation, generate a sample population and take a mean. So, something like

```{r take_mean, eval=FALSE}
mean(rnorm(n = 10, mean = 10, sd = 3))
```

Or replace that rnorm bit with the `sample` function and vector of interest. We'll do it both ways.

So, we want to work this into a `dplyr` workflow.  Now, what I often do it write out, in comments, what I want to do. Then implement that in `dplyr`. For example.

```{r comment_dplyr}
# Take the data frame
# For each individual simulation
# Use samp_size to get the mean of a random normal population with mean 10 and sd 3
# Also draw a mean from the samp vector with n = samp_size, with replacement
```

Reasonable, no? You could have written this out. Now, let's operationalize this. We'll group by `sim_number`, as we're doing to do the same thing for each replicate simulation.
```{r comment_dplyr_nsim}
# Take the data frame
sampSim <- sampSim %>%
# For each individual simulation
  group_by(sim_number) %>%
# Use samp_size to get the mean of a random normal population with mean 10 and sd 3
  mutate(mean_pop = mean(rnorm(samp_size, mean = 10, sd = 3)),
         
# Also draw a mean from the samp vector with n = samp_size, with replacement
         mean_sample = mean(sample(samp, size = samp_size, replace=T))) %>%
# Cleanup
    ungroup()
```

#### 3.3.1 Faded Examples.
Let's try this out, and have you fill in what's missing in these faded examples.
```{r faded_sim, eval=FALSE}
#Some preperatory material
set.seed(42)
samp <- rnorm(100, 10, 3)
sampSim <- data.frame(samp_size = rep(3:50, times = 10))
sampSim$sim_number = 1:nrow(sampSim)

#Mean simulations
sampSim %>%
  group_by(sim_number) %>%
  mutate(mean_pop = mean(rnorm(samp_size, mean = 10, sd = 3)),
         mean_sample = mean(sample(samp, size = samp_size, replace=T))) %>%
  ungroup()

#Now the faded examples! Fill in the ___

#Median simulations
sampSim %>%
  group_by(sim_number) %>%
  mutate(median_pop = ____(rnorm(samp_size, mean = 10, sd = 3)),
         median_sample = ____(sample(samp, size = samp_size, replace=T))) %>%
  ungroup()

#SD simulations
sampSim %>%
  group_by(____) %>%
  ____(sd_pop = ____(rnorm(____, mean = 10, sd = 3)),
         sd_sample = ____(sample(samp, size = ____, ____)) %>%
  ungroup()
  
  
  
#IQR simulations
#function for interquartile range is IQR
sampSim %>%
  ____(____) %>%
  ____(IQE_pop = ____(____(____, mean = 10, sd = 3)),
         IQR_sample = ____(____(samp, size = ____, ____)) %>%
  ungroup()
```

### 3.3 Determining Optimal Sample Size with `plot` and `summarize`

Great, so we've done the simulation! How do we determine sample size?  The first way is a plot.

```{r plot_n}
plot(mean_pop ~ samp_size, data=sampSim)
```

We can eyeball this result and see a leveling out > 20 or so. OK, that's great, but...totally imprecise.

Better might be to see where the SD of the mean levels off. Let's pseudocode that out in comments to see what we might need to do.

```{r comment pseudo_sam}
# Take our sampSim data
# Group it by sample size
# Take the SD of the sample size
# ungroup the resulting data
# Sort it by SD
# Look at the top 5 entries.
```

A few new things here. First, we're grouping by sample size, not sim number. Second, we're summarizing. We are reducing our data down - boiling it to its essence. For this, `dplyr` has a function called - wait for it - `summarize()`. Second, we're going to have to do some arranging. With a `dplyr` function called - and this is a shocker - `arrange`. `sort` was already taken by the base package.  OK, let's walk through the resulting pipeline in two pieces. First, the summarization

```{r comment pseudo_sam_do}
# Take our sampSim data
sampSim_summary <- sampSim %>%
  # Group it by sample size
  group_by(samp_size) %>%
  # Take the SD of the sample size
  summarize(pop_mean_sd = sd(mean_pop))

```

Neat - this gives us a much reduced piece of data to work with. But - what about the arranging?
```{r psuedo_sam_do_2}
sampSim_summary <- sampSim_summary %>%
  # ungroup the resulting data
  ungroup() %>%
  # Sort it by SD
  arrange(pop_mean_sd)

sampSim_summary
```

Now, notice that I don't have to use `head` or chop of the top few rows (there's a dplyr function for that - `slice()`).  That's because dplyr creates a data frame-like object called a `tibble`.  We'll talk about this in more detail later. For now, tibbles work just like data frames, but they print out a little differently. If we REALLY wanted a data frame, we could just use `as.data.frame()`.

As for the result, we can see that, eh, 46 has the lowest SD, but things bounce around, with even some of the mid 20s sample sizes having nice properties. Now, to be sure, we should probably repeat this with 100 or even 1000 simulations per 

### 3.4 Exercises.

a) Look at the resulting simulations using the sample. Would you chose a different optimal sample size?

b) Repeat this process, but for the sample median. What is the optimal sample size?


## 4. Standard Errors
### 4.1 Basic Sample Properties

So, as we launch into things like standard errors, I wanted to pause and hit a few key functions we need to describe a sample. Let's use the `samp` vector from above as adata source.  First, mean, sd, and median.

```{r samp_properties}
mean(samp)

sd(samp)

median(samp)
```

This is pretty great. There are a ton of functions like these. Such as

```{r iqr}
IQR(samp)
```

Although there is no standard error of the mean or confidence interval function. There are functions for skew, kurtosis, and more in other packages. Last, for arbitrary percentiles, there's the 'quantile()' function.

```{r quantiles}
#quartiles by defeault
quantile(samp)

#or, get what you want
quantile(samp, probs = c(0.05, 0.1, 0.5, 0.9, 0.95))
```

You can get individual values by specifying the percentile you want with probs.

### 4.2 Standard Error of a Median via Simulation
Let's look at the standard error of a median from our samp vector. And let's do it via simulation. One way we can do this is to use the 'replicate()` function. This lets us replicate some statement over and over again. So, let's replicate taking the median of a sample with replacement of the samp vector. Then we can get the standard deviation of that.

```{r se_sim}
median_sims <- replicate(n = 100, median(sample(samp, size=10, replace=TRUE)))

sd(median_sims)
```

This is nice, but, man, that's a lot of nested statements. The other way we could do this is with `dplyr`. Again, we can start with a blank data frame with 100 simulations, waiting to take flight.

```{r se_dplyr_start}
median_sims <- data.frame(sims=1:100)
```

Now, we can take these, and for each simulation, draw a median from a sample of our `samp` vector.

```{r se_dplyr}
median_sims <- median_sims %>%
  group_by(sims) %>%
  mutate(median_sim = median(sample(samp, size=10, replace = TRUE))) %>%
  ungroup()

median_sims
```

Perfect! From this, we can get the standard error of our median quite simply.

```{r se_median}
sd(median_sims$median_sim)
```

Done!

#### 4.2.1 Faded Examples

To take this for a spin, let's try some faded examples.

```{r se_faded, eval=FALSE}
#some preperatory material
se_sims <- data.frame(sample_size=rep(5:50, 100))

median_se_sims <- se_sims %>%
  group_by(1:n()) %>%
  mutate(median_sim = median(sample(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(sample_size) %>%
  summarize(median_se = sd(median_sim))

plot(median_se ~ sample_size, data=median_se_sims, type="l")

#mean se sims
mean_se_sims <- se_sims %>%
  ___(1:n()) %>%
  mutate(mean_sim = mean(___(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(___) %>%
  summarize(mean_se = sd(___))

plot(mean_se ~ sample_size, data=___, type="l")


#iqr se sims
iqr_se_sims <- se_sims %>%
  ___(1:n()) %>%
  ___(iqr_sim = IQR(___(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(___) %>%
  summarize(iqr_se = sd(___))

plot(___ ~ ___, data=___, type="l")



#sd se sims
sd_se_sims <- se_sims %>%
  ___(1:n()) %>%
  ___(sd_sim = sd(___(samp, size = ___, ___=___))) %>%
  ungroup() %>%
  ___(___) %>%
  ___(sd_se = sd(___))

plot(___ ~ ___, data=___, type="l")


```

<!--chapter:end:02_sim_samp.607git.Rmd-->

---
title: "Simulation for Estimation of Parameter Uncertainty"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Setting the stage

### 1.1 Pipes

Often in R, what we kludge together different pieces of code to generate a sequence of events. For example, let's say I wanted to take a vector , find out it's length, and get the square root. Then get the log of that. There are a few ways I could do this.

```{r nonpipe}
vec <- 1:10
len <- length(vec)
sq <- sqrt(len)
log(sq)
```

This makes sense, and is very logical, but I keep saving new variables as I go along. I could write some nested piece of code that does the same thing with no variables.

```{r nonpipe2}
log(sqrt(length(1:10)))
```

Which is efficient, but, man, I hope I can keep track of parentheses. It's also ugly as hell. And super difficult for future-me to understand.

This idea of doing one thing, then the next, then the next is often how we treat our data. We take a piece of data, sift it, transform it, summarize it, etc. And this step-by-step style of coding has a name - **functional programming**. It lies at the core of much of what we do in R.

So, is there a better way? Yes!  What if we could take the output of one function, and pass it to the next? This is known as piping. And it is implemented in R by the `magrittr` library.  (Pause. Get the joke. Laugh at it. Move on.) `magrittr` introduces a funny new operator `%>%` which says, take the output of the thing on the right hand side, and pass it to the next function as its first argument. 

As a simple example, let's sum one through ten using pipes.

```{r pipesum}
#If you haven't, install.packages("magrittr") first
library(magrittr)
1:10 %>% sum()
```

Notice that we supplied no argument to sum. Now, what if wanted to take the square root of the result?

```{r pipe_sum_sqrt}
1:10 %>% 
  sum() %>%
  sqrt()
```

Notice how I separated different steps on different lines? This is a more readable code. Future me can track what's going on, line by line, instead of having to read a looooong single line of code.

Let's try a few **exercises** with pipes.

a) Use pipes to sum the log of 100:200.

b) Use pipes to take the square root of the mean of 100 random uniform numbers.

c) Let's dive into the guts of R. Using the `mtcars` data frame, get it's summary and str that summary. What do you get back?


### 1.2 Plot

The last basic thing we'll need today is plotting, as we'll want to see the results of our simulations. Next week we'll learn `ggplot2` and get fancy. For the moment, I just want you to know a few things in baseplot.

First, here's how to make a histogram.

```{r hist}
#just some random numbers
vals <- runif(50, 5, 10)

#the histogram
hist(vals)
```

Easy, no?

Now, what if I wanted to plot x by y? A scatterplot. R uses an equation-like syntax for this, where I plot `y ~ x`. We'll use this again and again as we fit linear models. For now, here's an example scatterplot.

```{r scatterplot}
#first, a data frame
my_df <- data.frame(x=1:10, y=10:1)

#note how data is used
#as well as column names in the equation
plot(y ~ x, data=my_df)
```

Now, you can supply all sorts of arguments on plot - look at the help file. We're not really going to worry about those today.

Let's try some example **exercises**.

a) Using `mtcars`, plot mpg by cyl. What happens if you set pch to 19 or 5?

b) `boxplot()` works like plot. Use it for `mpg ~ vs`. Now try the `col` argument. Give it a vector of two colors, like `c("red", "blue")`.

## 2. dplyr

`dplyr` will change your life. It is the data munging tool you didn't know you needed until now. To learn our way around it, we'll use it with the `mtcars` data frame. `dplyr` is a collection of functions that manipulate data frames (or data tables, or tibbles, or other objects that work like data frames - more on those later). It's functions largely take the data frame object as the first argument, and then return data frames as the output, so pipes are a natural fit for `dplyr`.  Indeed, if you load `dplyr`, `magrittr` is loaded by default.

### 2.1 `mutate()`

Often, we want to make a column that uses information from other columns. For this, we have mutate. For example, what if we wanted to make a log transformed column of `mpg`? Note, I'm going to wrap the whole thing in head, so we only get the first 6 lines of the result

```{r mutate_mpg, message=FALSE}
library(dplyr)

mtcars2 <-  mutate(mtcars, log_mpg = log(mpg)) 

head(mtcars2)
```

OK, the first argument was the data frame. Then, we just specified a new column from whole cloth, but used one of the columns from mtcars to make it. How easy! It's even easier with pipes.

```{r mutate_mpg_pipe}
mtcars2 <- mtcars %>%
    mutate(log_mpg = log(mpg))

head(mtcars2)
```

### 2.2 `group_by()`
One of the ways we use `dplyr` the most is `group_by()`. This lets us group by different unique values of a column - think treatments or categories - and do something to those sub-data-frames.  For example, let's say we wanted to group by number of gears, and then use mutate to make a new column that's the average of the mpg values.

```{r group}
mtcars_group <- mtcars %>%
  group_by(gear) %>%
  mutate(avg_mpg = mean(mpg)) %>%
  ungroup()

head(mtcars_group)
```

Notice the `ungroup()` at the end. That's provided as some functions bork if you give them a grouped data frame. We want to remove that structure.

Now see that final column - each row with the same number of gears has the same avg_mpg value.

### 2.3 `summarize()`

Often, you don't want to have a data frame with summarized information repeated. You just want to reduce your data frame down to summarized information. In Excel, this is pivot tables, basically. For that, we use a combination of `group_by()` and `summarize()`  Let's do the same thing as the previous example, only let's add the standard deviation, and return just the summarized information.

```{r mtcars_summary}
mtcars %>%
  group_by(gear) %>%
  summarize(avg_mpg = mean(mpg),
            sd_mpg = sd(mpg)) %>% 
  ungroup()
```

Whoah!  Cool!

You try it, but group by gear and vs, and look at how that alters weight. Yes, you can group by multiple things in the same `group_by` statement - just separate them with a comma!

### 2.4 `select()`
Often when working with a data frame, you end up with some column you no longer want. Maybe it was intermediate to some calculation. Or maybe you just want only a few columns to save memory. Either way, select is a way to include or exclude columns. 

```{r select}
head(select(mtcars2, -mpg))
```

Uh oh! Where did mpg go? Gone!

Or

```{r select2}
mtcars3 <- mtcars %>%
  select(wt, am, gear)

head(mtcars3)
```

One can even use some more imprecise matches if need be.

```{r}
head(select(mtcars2, contains("mpg")))
```


### 2.5 `filter()`

We can also exclude by row values using filter. This function takes all manner of comparisons, and returns only those rows for which the comparison is true. For example, to get rid of 3 cylinder cars:

```{r filter1}
mtcars_filter <- mtcars %>%
  filter(cyl != 3)

head(mtcars_filter)
```

I'll often use this to remove rows with `NA` values for some column, such as 
```{r filter2, eval=FALSE}
mtcars %>%
  filter(!is.na(cyl))
```

**Exercises**  
1. Add some columns to `mtcars` to plot the log of `mpg` by the square root of `hp`.  
  
2. Get the average `hp` per `gear` and plot them against each other.  
  
3. Make a data fame for only 6 cylinder engines with only the `disp` and `carb` columns. Create a boxplot. of how carb influences `disp`.


## 3. Simulation and Sample Size



### 3.1 Random Numbers and `sample()`

There are a number of ways to get random numbers in R from a variety of distributions.  For our simulations, let's start with a sample of 40 individuals that's from a population with a mean value of 10, a SD of 3.

```{r samp}
set.seed(323)
samp <- rnorm(n = 40, mean = 10, sd = 3)
```

Note the `set.seed()` function. Random number generators on computers aren't REALLY random - they use some nugget of information that changes constantly to be pseudo-random. By setting that nugget - the seed - we make sure that all random numbers generated from this point on are the same.

Unsettling, no?

So, we can pull a random sample from a perfect population anytime we'd like. How do we get a bootstrap sample from a population?

Why, with the sample function. Let's sample the sample we've greated above, and draw five different elements.

```{r sample1}
sample(samp, size = 5)
```

This is *incredibly* useful in a wide variety of context. For example, let's say we wanted five rows of mtcars.

```{r sample_mtcars}
mtcars[sample(1:nrow(mtcars), size = 5), ]
```

Of course, for that kind of thing, `dplyr` has you covered with 

```{r sample_dplyr}
sample_n(mtcars, size = 5)
```

Or, you can even use a porportions - so, here's a random sampling of 10% of mtcars

```{r sample_dplyr_frac}
sample_frac(mtcars, 0.1)
```

But that is neither here nor there. 

One thing that all of these sample functions let you do is sample with or without replacement. If we're assuming a sample is representative of a population and we want to bootstrap resample it, we're going to need to sample **with replacement**. Fortunately, that's just one more argument.

```{r sample_replace}
sample(1:3, size = 15, replace = TRUE)
```

This will come in handy shortly!

### 3.2 Using `rowwise()` for simulation

The other tool we'll need to simulate sampling a population is some output. Let's go through this step by step. First, let's say we are going to have 10 samples per sample size, and try sample sizes from 3 to 50. We begin by creating a data frame with this information, as we're going to want to plot this information later.

```{r start_n}
#use rep to repeat the sample sizes 3 through 50 10 times
sampSim <- data.frame(samp_size = rep(3:50, times = 10))
```

We can view each row as one simulated replicate. Now, we're ready to go! What we want to do is, for each simulation, generate a sample population and take a mean. So, something like

```{r take_mean, eval=FALSE}
mean(rnorm(n = 10, mean = 10, sd = 3))
```

Or replace that rnorm bit with the `sample` function and vector of interest. We'll do it both ways.

So, we want to work this into a `dplyr` workflow.  Now, what I often do it write out, in comments, what I want to do. Then implement that in `dplyr`. For example.

```{r comment_dplyr}
# Take the data frame
# For each row
# Use samp_size to get the mean of a random normal population with mean 10 and sd 3
# Also draw a mean from the samp vector with n = samp_size, with replacement
```

Reasonable, no? You could have written this out. Now, let's operationalize this. We'll group by `sim_number`, as we're doing to do the same thing for each replicate simulation.
```{r comment_dplyr_nsim}
# Take the data frame
sampSim <- sampSim %>%
# For each individual simulation
  rowwise() %>%
# Use samp_size to get the mean of a random normal population with mean 10 and sd 3
  mutate(mean_pop = mean(rnorm(samp_size, mean = 10, sd = 3)),
         
# Also draw a mean from the samp vector with n = samp_size, with replacement
         mean_sample = mean(sample(samp, size = samp_size, replace=T))) %>%
# Cleanup
    ungroup()
```

#### 3.3.1 Faded Examples.
Let's try this out, and have you fill in what's missing in these faded examples.
```{r faded_sim, eval=FALSE}
#Some preperatory material
set.seed(42)
samp <- rnorm(100, 10, 3)
sampSim <- data.frame(samp_size = rep(3:50, times = 10))

#Mean simulations
sampSim %>%
  rowwise() %>%
  mutate(mean_pop = mean(rnorm(samp_size, mean = 10, sd = 3)),
         mean_sample = mean(sample(samp, size = samp_size, replace=T))) %>%
  ungroup()

#Now the faded examples! Fill in the ___

#Median simulations
sampSim %>%
  rowwise() %>%
  mutate(median_pop = ____(rnorm(samp_size, mean = 10, sd = 3)),
         median_sample = ____(sample(samp, size = samp_size, replace=T))) %>%
  ungroup()

#SD simulations
sampSim %>%
  ____() %>%
  ____(sd_pop = ____(rnorm(____, mean = 10, sd = 3)),
         sd_sample = ____(sample(samp, size = ____, ____)) %>%
  ungroup()
  
  
  
#IQR simulations
#function for interquartile range is IQR
sampSim %>%
  ____() %>%
  ____(IQE_pop = ____(____(____, mean = 10, sd = 3)),
         IQR_sample = ____(____(samp, size = ____, ____)) %>%
  ungroup()
```

### 3.3 Determining Optimal Sample Size with `plot` and `summarize`

Great, so we've done the simulation! How do we determine sample size?  The first way is a plot.

```{r plot_n}
plot(mean_pop ~ samp_size, data=sampSim)
```

We can eyeball this result and see a leveling out > 20 or so. OK, that's great, but...totally imprecise.

Better might be to see where the SD of the mean levels off. Let's pseudocode that out in comments to see what we might need to do.

```{r comment pseudo_sam}
# Take our sampSim data
# Group it by sample size
# Take the SD of the sample size
# ungroup the resulting data
# Sort it by SD
# Look at the top 5 entries.
```

A few new things here. First, we're grouping by sample size, not sim number. Second, we're summarizing. We are reducing our data down - boiling it to its essence. For this, `dplyr` has a function called - wait for it - `summarize()`. Second, we're going to have to do some arranging. With a `dplyr` function called - and this is a shocker - `arrange`. `sort` was already taken by the base package.  OK, let's walk through the resulting pipeline in two pieces. First, the summarization

```{r comment pseudo_sam_do}
# Take our sampSim data
sampSim_summary <- sampSim %>%
  # Group it by sample size
  group_by(samp_size) %>%
  # Take the SD of the sample size
  summarize(pop_mean_sd = sd(mean_pop))

```

Neat - this gives us a much reduced piece of data to work with. But - what about the arranging?
```{r psuedo_sam_do_2}
sampSim_summary <- sampSim_summary %>%
  # ungroup the resulting data
  ungroup() %>%
  # Sort it by SD
  arrange(pop_mean_sd)

sampSim_summary
```

Now, notice that I don't have to use `head` or chop of the top few rows (there's a dplyr function for that - `slice()`).  That's because dplyr creates a data frame-like object called a `tibble`.  We'll talk about this in more detail later. For now, tibbles work just like data frames, but they print out a little differently. If we REALLY wanted a data frame, we could just use `as.data.frame()`.

As for the result, we can see that, eh, 46 has the lowest SD, but things bounce around, with even some of the mid 20s sample sizes having nice properties. Now, to be sure, we should probably repeat this with 100 or even 1000 simulations per 

### 3.4 Exercises.

a) Look at the resulting simulations using the sample. Would you chose a different optimal sample size?

b) Repeat this whole process, but for the sample median. What is the optimal sample size? Feel free to reuse code or modify code in place (for the simulations).


## 4. Standard Errors
### 4.1 Basic Sample Properties

So, as we launch into things like standard errors, I wanted to pause and hit a few key functions we need to describe a sample. Let's use the `samp` vector from above as adata source.  First, mean, sd, and median.

```{r samp_properties}
mean(samp)

sd(samp)

median(samp)
```

This is pretty great. There are a ton of functions like these. Such as

```{r iqr}
IQR(samp)
```

Although there is no standard error of the mean or confidence interval function. There are functions for skew, kurtosis, and more in other packages. Last, for arbitrary percentiles, there's the 'quantile()' function.

```{r quantiles}
#quartiles by defeault
quantile(samp)

#or, get what you want
quantile(samp, probs = c(0.05, 0.1, 0.5, 0.9, 0.95))
```

You can get individual values by specifying the percentile you want with probs.

### 4.2 Standard Error of a Median via Simulation
Let's look at the standard error of a median from our samp vector. And let's do it via simulation. One way we can do this is to use the 'replicate()` function. This lets us replicate some statement over and over again. So, let's replicate taking the median of a sample with replacement of the samp vector. Then we can get the standard deviation of that.

```{r se_sim}
median_sims <- replicate(n = 100, median(sample(samp, size=10, replace=TRUE)))

sd(median_sims)
```

This is nice, but, man, that's a lot of nested statements. The other way we could do this is with `dplyr`. Again, we can start with a blank data frame with 100 simulations, waiting to take flight.

```{r se_dplyr_start}
median_sims <- data.frame(sims=1:100)
```

Now, we can take these, and for each simulation, draw a median from a sample of our `samp` vector.

```{r se_dplyr}
median_sims <- median_sims %>%
  rowwise() %>%
  mutate(median_sim = median(sample(samp, size=10, replace = TRUE))) %>%
  ungroup()

median_sims
```

Perfect! From this, we can get the standard error of our median quite simply.

```{r se_median}
sd(median_sims$median_sim)
```

Done!

#### 4.2.1 Faded Examples

To take this for a spin, let's try some faded examples.

```{r se_faded, eval=FALSE}
#some preperatory material
se_sims <- data.frame(sample_size=rep(5:50, 100))

median_se_sims <- se_sims %>%
  rowwise() %>%
  mutate(median_sim = median(sample(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(sample_size) %>%
  summarize(median_se = sd(median_sim))

plot(median_se ~ sample_size, data=median_se_sims, type="l")

#mean se sims
mean_se_sims <- se_sims %>%
  ___() %>%
  mutate(mean_sim = mean(___(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(___) %>%
  summarize(mean_se = sd(___))

plot(mean_se ~ sample_size, data=___, type="l")


#iqr se sims
iqr_se_sims <- se_sims %>%
  ___() %>%
  ___(iqr_sim = IQR(___(samp, size = sample_size, replace=TRUE))) %>%
  ungroup() %>%
  group_by(___) %>%
  summarize(iqr_se = sd(___))

plot(___ ~ ___, data=___, type="l")



#sd se sims
sd_se_sims <- se_sims %>%
  ___() %>%
  ___(sd_sim = sd(___(samp, size = ___, ___=___))) %>%
  ungroup() %>%
  ___(___) %>%
  ___(sd_se = sd(___))

plot(___ ~ ___, data=___, type="l")


```

<!--chapter:end:02_sim_samp.Rmd-->

---
title: "Reading Data & Libraries"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

There's a surprising amount that goes into the one simple act of reading in a data file to produce a data frame. Today we're going to explore that in all its complexity, and add in libraries to boot, as our data often requires external libraries to load.

### Let's start at the very beginning - directory structure!
First, as we enter our data, where should we save it?  Every project you have should have 
#### 1) It's own directory
#### 2) A standardized directory structure

I often use something like this:

```{r eval=FALSE}
My Project 
| 
|- Data 
|- R 
|- Notes 
|- Images 

```

You may build something more elaborate, but this is a good starting point for any project - be it a homework, research project, lab report, or anything.

Now, enter that data!

![](./images/03/excel_entry.jpg)

### Data entered! Can I just save it and move on?

Let's say you've entered your data. How do you save it so that R can read it? Well, what does an Excel file look like to R?

![It ain't pretty](./images/03/binary_data.jpg)

Oh no.  Oh no no no. That is not the way. Excel files are saved in a particular proprietary format that requires particular functionality to load. We'll get how to do that later. But this is an excellent object lesson. Programs change over time. Their save formats change over time. In 40 years, who is to say anything will be able to load that mess?  

Furthermore, what if you're bouncing between computers, and maybe bouncing between spreadsheet programs. Wouldn't it be better to have a standard human-readable saved data format?  

There are a lot of formats like these. We call them 'delimited'. A file can be tab delimited (tabs separate columns), space deliminted, semi-colon delimited, or, perhaps the most common, comma delimited. There are of course any number of arbitrary delimited formates, but, best to keep with the standards.  The above data in comma delimited format would look like so:

```{r, eval=FALSE}
Column_1,Column_2,Column_3,TheBiggestColumnNameEverThatIWantToWite,
4,8,Fred,5,
4,8,Fred,6,
4,8,Fred,4,
4,8,Fred,3,
4,8,Fred,7,
4,8,George,3,
4,8,Fred,5,
4,8,Fred,3,
4,9,Fred,7,
4,10,Nancy,3,
4,11,George,6,
```

Way more readable, no?  So, how do we save our file like this? Under the Save As option in Excel, there are a myriad of different file formats. Comma delimited is one of them.

![CSV creation](./images/03/save_as_csv.jpg)

Let's save this as `my_data.csv` (csv is a common file extension for comma separated files). Save it in your `data` directory for this project

### OK! Time to Load?

YES!  Fire up R, and save the script file you will be working on in your `R` directory.  Now, it's just hop-skip-and-a-jump to loading?

Well, almost. We have the `read.csv` function in R for reading CSVs. The first - and main - argument is `file`. But as we look at the description of the file argument, we see we can't just enter the name of the file. We need to say something about the directory:

"If it does not contain an absolute path, the file name is relative to the current working directory, getwd(). Tilde-expansion is performed where supported. "

What is this path stuff?  Working directory?  Huh?

You're all familiar with your computers file explorer or finder. You're used to navigating back and forth between folders or directories.  You're also used to the fact that the folders are nested within each other. For example both `R` and `Data` are here nested within `Project`.  There is a tree-like structure to all of the directories on your computer, from some toplevel, which is refered to as `/` all the way down to any other directory.

When you open R, based on how you've set it up, it assumes that it's got some directory open in front of it, just as if you opened a directory yourself. We call this the *working directory*.  Anything we do with respect to files - saving, loading, etc. - assumes we are in this directory. You can see your current directory with `getwd`.  So

```{r}
getwd()
```

Notice it starts at a top level and presents a hierarchy of directories all the way down to the one this file is being generated from.  

OK, great. So how do I change my working directory so I can tell R where to load the data from?  There are a few ways - the `setwd` function for one. This is good for a nice mature script that you always want to run the same way every time on one computer with the same directory structure. Often we'll want to be more dynamic than that. R Studio provides a nice option to set your working directory to wherever your script file is saved.

As you've saved your script file to the directory `R` within project, that will mean this is now your working directory.

![](./images/03/setwd.jpg)

### Now can I load things? A relative question.

Almost!  Now, you **could** use `getwd` and then change `R` to `data` for the directory name you use when loading your data. Or, you could be a little sneaky, and acknoledge that, when your working directory is `R` the `data` will always be one directory up, and then down in the `data` directory. Think about it in terms of walking up one branch of a tree to the trunk, and then going down the branch next door. 

This idea of **realtive paths** has a nice standard implementation.  One directory up is **../**  So, the full path to the data file from your current working directory is `../data/` - not so bad, no?  

Armed with this, we can finally load our data

```{r, eval=FALSE}
my_data <- read.csv("../data/my_data.csv")

head(my_data)
```

```{r, echo=FALSE}
my_data <- read.csv("./data/03/my_data.csv")

head(my_data)
```

### Factoring out our expectations

Did everything go as planned? `head` makes it look so, but let's dig further.

```{r}
str(my_data)
```

Oh now that's odd. What is this factor thing in Column 3?

```{r}
my_data$Column_3
```

A factor is a new class. We have had numerics, logicals, characters (which one would assume would be column 3) but factors are a slightly different beast. Factors are our controlled vocabularies made manifest. They are an object that can only take certain values. We can check those values with

```{r}
levels(my_data$Column_3)
```

Internally, factors are actually stored as an integer:

```{r}
as.numeric(my_data$Column_3)
```

But that integer is references to the levels before being shown as output.  Factors have a lot to recommend them - you cannot combine factors with different levels to keep your data safe, you can order them non-alphabetically for nicer plotting, and more.  

However, factors are also a nuissance. You cannot manipulate them the same way you can manipulate a string. As such, we often use the `stringsAsFactors` argument and set it to FALSE.

```{r, eval=FALSE}
my_data <- read.csv("../data/my_data.csv", 
                    stringsAsFactors=FALSE)

str(my_data)
```

```{r, echo=FALSE}
my_data <- read.csv("./data/03/my_data.csv", 
                    stringsAsFactors=FALSE)

str(my_data)
```

Ah, better.

### What other arguments might I want to think about in read.csv?

There's a lot to ponder here. There's a `skip` argument to skip lines at the head of a file in case there's metadata or header information there, `na.strings` let's you state whether there were any non-standard ways of specifying NAs, and more.

### Is there a better way to deal with csvs?

`read.csv()` can be a pain - remembering stringsAsFactors=FALSE, it doesn't handle dates well, and it can be really slow for large data files. And don't even get me started on how it destroys date formatting. It also inserts `.` anytime there is a space in column names which can be confusing.

But there is a better way! The `readr` library from the tidyverse makes loading data work...better, if you will.

```{r, echo=FALSE}
library(readr)
my_data <- read_csv("./data/03/my_data.csv")

my_data
```

Note that we also no longer have a data frame, but a tibble, which displays more easily.

### Back to Excel... and libraries
But what if I don't want to save a CSV? What if this is a project in motion, and saving a CSV will just be a huge PITA every time I want to load things right from Excel?

As R has no native Excel loading capacity we need to find a *package* that does it for us. The package `readxl` from Hadley Wickham. He's everywhere.

To install a library, you can either go through The Tools/Install Packages dropdown in R studio (this can help with package discovery) or just use 

```{r, eval=FALSE}
install.packages("readxl")
```

Some additional helper packaged might be installed as well that your package requires. That's OK.  Once you've installed a package, you load it with `library` - 

```{r}
library("readxl")
```

Readxl has a function called `read_excel` which is going to be our main function of interest here. Take a look at it's help file.  Note, it takes a few arguments - a path to the file (which includes the file name) the sheet (because your spreadsheet might not be the first one in the workbook), as well as a few others about specifying information about the columns. It does have an argument for missing value specification - but it's just `na` not `na.strings`. You also have a skip argument.  That's it.  Let's take it for a spin.

```{r, eval=FALSE}
my_data_excel <- read_excel("./data/my_data.xlsx")

str(my_data_excel)
```

```{r, echo=FALSE}
my_data_excel <- read_excel("./data/03/my_data.xlsx")


str(my_data_excel)
```

Note a few things - first off, our data now has multiple classes - data.frame, of course, but also tbl_df and tbl. We'll talk about these other classes later. For now, you can strip them off with

```{r}
my_data_excel <- as.data.frame(my_data_excel)

str(my_data_excel)
```

Better.

Notice second, that there are no factors. Heck, `read_excel` doesn't even provide the option for factors! If you really want one, sure, you can create it later. But Hadley doesn't want you to mess with factors unless you know you need to.

<!--chapter:end:03_read_data_libraries.Rmd-->

---
title: "Introduction to Graphics and Temperature Change"
author: "Biol607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

Good data visualizations can have a strong impact on how you think about a phenomena. Earlier in 2016, a single image made by Ed Hawkins brought home how much our global average temperature has changed in a way that even the famous hockey stick graph didn't. 

<center>![http://www.climatecentral.org/news/see-earths-temperature-spiral-toward-2c-20332](images/ggplot/hawkins_hadcrut.gif)\
from http://www.climatecentral.org/news/see-earths-temperature-spiral-toward-2c-20332</center>

There is a **lot** going on in this image. It's truly stunning. And a lot of it illustrates all of the best principles of data visualization - and it can be done in ggplot2! So, today, we're going to use this graph as our point of entry into exploring data visualization and ggplot2. Along the way we're going to explore many many aspects of `ggplot2` that are available to us.

### 1. Load the data

We'll begin by loading in the data using `readr`. We're doing this so that, later, we can do some ordering by month - but can determine that ordering ourselves. We'll also load `dplyr` as we'll use its functions in a few places today.

```{r load_data, messages=FALSE}
library(readr)

hadcrut_temp_anomoly <- read_csv("./data/hadcrut_temp_anomoly_1850_2015.csv")

hadcrut_temp_anomoly
```

Here we see a data table with year, month, temperature anomoly for that month, and a name for the month. The month name is convenient - but - if we want to use it as an ordered variable later, we're going to have problems. R by default orders alphabetically. 


#### 1.1 Factors and Forcats

To impose a different ordering schema on `month_name`, we'll need to turn it into a `factor` - a character vector that has a set of ordered levels. If we had used `read.csv()` to load the data, this would have already been done - but is generally bad practice to assume.

To create a factor, we can use `factor()`.  We can see the order of the levels with `levels()`.

```{r make_factor}
hadcrut_temp_anomoly$month_name <- factor(hadcrut_temp_anomoly$month_name)

levels(hadcrut_temp_anomoly$month_name)
```

Uh oh - alphabetical! Enter `forcats` - a wonderful library from the `tidyverse` designed to work with factors in just such a scenario. There are a number of useful functions such as `fct_recode()` for changing the names of different factor levels, `fct_relevel()` for specifying arbitrary factor level orders, `fct_rev()` for reversing level order, and more. We're going to use `fct_inorder` which specifies level order in the order we see the first appearance of each level. As the data is sorted by month, this should work out just like we need.

```{r forcats_relevel}
library(forcats)

hadcrut_temp_anomoly$month_name <- fct_inorder(hadcrut_temp_anomoly$month_name)

levels(hadcrut_temp_anomoly$month_name)
```

Notice now that `month_name`'s class changes when we look at `hadcrut_temp_anomoly`

```{r show_data}
hadcrut_temp_anomoly
```

**Exercise:** Make a new column that contains the same information as `month_name`. Now, try `fct_rev()`, `fct_relevel()`, and last, `fct_recode()` on it to futz with the factor levels. After each one, look at the output of `levels()` to see what you did. 

### 2. Your first ggplot

#### 2.1 Visualizing distributions 

All right! Let's try out `ggplot2`. `ggplot2` works simply by you specifying things in roughly the following order:  
1. A `ggplot2` object which links to a data set and has information about aesthetics.  
2. A `geom` or geometry which specifies how the data is to be seen.  
3. Other bells and whistles which we will get to.  
  
We add these pieces together with a `+` sign. So let's start with something simple, just visualizing a distribution. First, we have to create a ggplot. We'll link it to the `hadcrut_temp_anomoly` data, and map x to anomaly using the `aes` function, as we're going to look at the distribution of anomaly.

```{r first_ggplot}
library(ggplot2)

had_dens <- ggplot(data = hadcrut_temp_anomoly,
       mapping = aes(x = anomaly))

had_dens
```

Wait, what? Nothing happened! That actually makes sense, as we haven't told `ggplot2` anything about plotting! There's no `geom`. For starters, let's try the simple `geom_histogram()`

```{r hist}
had_dens +
  geom_histogram()
``` 

Nice! Aside from R yelling at us and giving telling us to use a different bin size, it's not bad!  
  
**Exercise:** Now you try `geom_density`. How's it look? If you want, try adding arguments like `fill`, `color`, and `alpha` to `geom_density()` if you want to get fancy.

#### 2.2 Visualizing multiple distributions 

To view multiple distributions, we'll need aesthetics that specify groupings. We'll look in a minute at how we put those on an axis themselves, but for now, let's introduce `group`.

```{r dens_group}
had_dens_group <- ggplot(data = hadcrut_temp_anomoly,
       mapping = aes(x = anomaly, group=month_name))
```

There's a lot we can try here. Let's revisit one old friend.

```{r hist_stack}
had_dens_group + 
  geom_density()
```

Well that shows something interesting, doesn't it! What if we don't overlap? We can try a few different 'position' arguments.

```{r dens_stack}
had_dens_group + 
  geom_density(position="stack")

had_dens_group + 
  geom_density(position=position_dodge(width=3))
```


**Exercise:** Now you try `geom_histogram`. How's it look? Bad, right? Try different colors, alphas, and fills to see if you can improve. Maybe a different position?


### 3. Two dimensions.

#### 3.1 Starting a plot
`ggplot2` contains a number of different types of geometries for displaying information in two dimensions However, all of them begin by giving ggplot2 some information, just as before. What data is going to be used? What elements of that data will map onto different aesthetic values and scales in the plot? We'll start by creating the base of our plot with the x-axis being month and the y-axis being temperature anomoly.

```{r ggplot_base}
had_plot_base <- ggplot(data = hadcrut_temp_anomoly,
                        mapping = aes(x = month_name, y = anomaly))
```

Great! We have the basics of our plot saved as an object. 

#### 3.2 Scatterplots

To take a basic plot and add a geometry choice to it, we use one of the family of `geom_`s in `ggplot2`. One of the most basic plot types we ever encounter is the scatterplot. y versus x. That's all. For that, we have `geom_point()`

```{r ggplot_point}
had_plot_base +
  geom_point()
```

This is great! But, well, a few things.  First, man, lots of points overlap. If only they were kinda transparent. Second, maybe they're too small? Each `geom` has a number of options for different visual elements - color, size, alpha, etc. So, for example, we can modify this plot a bit.

```{r ggplot_point_mod}
had_plot_base +
  geom_point(alpha = 0.5, size=3)
```

Note - all of these are relative to 1. Well, now we can see the distribution of points a bit better, given overlap. But....


#### 3.3 Jitter
Maybe we want to add a bit of random noise to the points, to better visualize what's going on here. For that, there's `geom_jitter`.

```{r jitter}
had_plot_base +
  geom_jitter()
```

Neat! You can see the density of each cluster for each month so much more clearly! Now, you may be wondering - hold on - I only want to add jitter to the x-axis, not the y-axis. For that, there's the width and height argument of how much jitter room there is to spare. So - let's say we want the points to vary in the x by $\pm$ 0.5, but 0 in the y. And let's throw in some alpha for good measure.

```{r jitter_0}
had_plot_base +
  geom_jitter(width=0.5, height=0, alpha=0.8)
```

#### 2.4 Exercise: Boxplot, Violins, and stacking.
This is all well and good, but, eyeballing is not the same as some solid information on the distribution of the data. Let's try some of the 2D ways of visualizing data.   

1. Try out the following geoms - `geom_boxplot()`, `geom_violin()`, `stat_summary()`. Which do you prefer?  
2. Try adding multiple geoms together. Does order matter?  
3. If you want to get saucy, install `ggridges`. You'll have to swap x and y in your mapping, but, try out `geom_density_ridges()`  

```{r ridges, echo=FALSE, message=FALSE}
library(ggridges)
ggplot(data = subset(hadcrut_temp_anomoly, hadcrut_temp_anomoly$year>1990),
                        mapping = aes(y =factor(year), x = anomaly)) + geom_density_ridges()
```



#### 3.5 Lines
OK - but these are timeseries! We need lines, no? There is, of course, a `geom_line` that shows lines connecting points, but does not show the points (yes, you can layer both `geom_line` and `geom_point`).

```{r line_bad}
had_plot_base +
  geom_line()
```

UGH - what happened? Welp, because month_name is a factor, all of the points within a month were connected. Oops! There is a way around it, and that is to specify what your groups are, rather than let ggplot define it for you.

One of the nice things about geoms is that we can add aesthetic elements from the data frame. Heck, if you want, you can add a geom with a whole new data set and new aesthetics, but for now, let's just redefine the group aesthetic.

```{r line}
had_plot_base +
  geom_line(mapping = aes(group=year))
```

Perfect! We are on our way!

**Exericise:** Try a line plot, using `group = month`. Feel free to play with other elements of the plot, such as alpha.

### 4. Adding information via aesthetic mapping
We've got our line plot working the way we want it to. But we have other information.

R provides a number of different scales that can all be tied to a variable inside the `aes` function. Most commonly, we'll use color, fill, alpha, size, and shape. There are other options, such as lty for line type, but those will be specific to the geom.

#### 4.1 Color

As color is the most common modification, let's concentrate on that. The lessons can be used broadly for other scales.

```{r color}
had_lines <- had_plot_base +
  geom_line(mapping = aes(group=year, color=year))

had_lines
```

A few things to note. First, year is treated continuously, so it's placed on a gradient. Second the default color gradient is from dark to light blue.

This is great, but what if we want different colors? If your colors are continuous, the first go-to to change them is `scale_color_continuous` - and we add scales just like we added geoms before.  Let's start with a traditional blue to red gradient.

```{r scale_color}
had_lines +
  scale_color_continuous(low = "blue", high = "red")
```

This works great. We could also have used `scale_color_gradient` exactly the same way to achieve the same results. If you want to know more of what colors are available in R, see [this post](http://research.stowers-institute.org/efg/R/Color/Chart/) or 

#### 4.2 More on Gradients

This is **much** better. But it's still hard to see the middle ranges of years  For that, we have the `scale_gradient_2` function, which takes an argument for what the midpoint color should be, but then **you** have to specify the value for that midpoint. Let's just go with the 1925.

```{r scale_color_grad2}
had_lines +
  scale_color_gradient2(low = "blue", mid = "yellow", high = "red",
                        midpoint = 1925)
```

Groovy! What if we wanted something more arbitrary - say, a 7 colors of the rainbow gradient? `scale_color_gradientn` has you covered.


```{r scale_color_gradn}
had_lines +
  scale_color_gradientn(colors=rainbow(7))
```

Note the `rainbow()` function. R comes with a few different color palatte functions (and see the `colors` helpfile for how to view all of the colors in R). For each palatte, we feed it a number of colors, and get a vector back. Using some code from the `rainbow` helpfile, here ar ethe default pallates.

```{r, echo=FALSE}

##------ Some palettes ------------
demo.pal <-
  function(n, border = if (n < 32) "light gray" else NA,
           main = paste("color palettes;  n=", n),
           ch.col = c("rainbow(n, start=.7, end=.1)", "heat.colors(n)",
                      "terrain.colors(n)", "topo.colors(n)",
                      "cm.colors(n)"))
{
    nt <- length(ch.col)
    i <- 1:n; j <- n / nt; d <- j/6; dy <- 2*d
    plot(i, i+d, type = "n", yaxt = "n", ylab = "", main = main)
    for (k in 1:nt) {
        rect(i-.5, (k-1)*j+ dy, i+.4, k*j,
             col = eval(parse(text = ch.col[k])), border = border)
        text(2*j,  k * j + dy/4, ch.col[k])
    }
}
n <- if(.Device == "postscript") 64 else 16
     # Since for screen, larger n may give color allocation problem
demo.pal(n)

```
Try one, and see what it does to you!

#### 4.3 More Organized Gradients

There are of course a ton of packages with other pallates our there. One of the most popular, because it's color selection is based on research looking at color blindness, and how we see sequential or diverging palattes of color, is `RColorBrewer`. You can view a lot more about it at http://colorbrewer2.org/ - for now, let's take a gander at what it provides.

```{r brew}
#install.packages("RColorBrewer")
library(RColorBrewer)

display.brewer.all(n=10, exact.n=FALSE)
```

That's a lot. Which of these do you think is best for seeing differences between years? Why? I admit, I'm often partial to `BrBG`

```{r brewer1}
had_lines +
  scale_color_gradientn(colors=brewer.pal(n = 7, 
                                          name = "BrBG"))
```

#### 4.4 More Color Packages
In point of fact, there are **many** different color palette packages out there. I'll just leave you with two more. The first is my *personal* favorite - the Wes Anderson package - https://github.com/karthik/wesanderson. Don't worry, there are install packages on the github page. 

The second is the viridis palatte. This is a pretty common yellow--blue-based palette (and the package - https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html - also comes with a 'magma' palette as well.) These two palettes are great as they show up well for people with color blindness. Heck - if you really want to have color-blind safe color schemes, you might also want to check out the [`dichromat`](https://cran.rstudio.com/web/packages/dichromat/index.html) package - it not only contains multiple colorblind safe palettes, but it also allows you to see what your color palette will look like under different forms of color blindness.

So, it's no accident that viridis is what was used for the original animation. Let's make it our own. We'll also add `guide = 'none'` to lose the colorbar at ths side.

```{r viridis}
library(viridis)
had_lines +
  scale_color_viridis(guide="none")
```

#### 4.5 Exercises

1. What happens to our histogram if we add a fill argument. Try `fill = month` and `fill = factor(year)` (note that without the factor, nothing happens!). You might also want to remove the guide with `+ guides(fill = "none")`.

2. Create a line plot of year on the x and anomoly on the y. Include month as a group. Try coloring by anomoly as well. First use the default palatte, and then try a blue-white-red.

3. Now color that line plot by month. Try at least two different palettes either of your own devising or from an additional package. Note, if you use `month_name`, you'll be mucking about with `scale_color_manual`. Or you can use RColorBrewer which handles discrete groups easily using `scale_color_brewer()`. 


### 5. Facets to see grouped information

Before we get to the circular animation hotness, I wanted to take a brief digression to show another way to visualize multiple dimensions. Figures often have multiple panels, each of which shows a different slice of the data. In `ggplot`, these are called facets. Before we jump whole hog into seeing this big timeseries, I want us to really look at each month and see if there are trends there. So, I want us to begin a slightly different plot - let's look at temperature anomoly over time, but with different lines showing different months, and the x axis is year.

```{r monthly_lines}
had_months <- ggplot(data = hadcrut_temp_anomoly,
       mapping = aes(x= year, y = anomaly, group = month_name)) +
  geom_line()
  
had_months
```

If you'd like, try adding a color and muck about with `scale_color_discrete` and its `values` argument. Categorical groupings are really a place also where `RColorBrewer` and `wesanderson` shine.

But, what if instead of separating by color - as these lines all overlap - we really wanted to see each line on its own in a panel.  There are two functions that can help us here. The first is `facet_wrap()`

```{r facet}
had_months +
  facet_wrap(~month_name)
```

The second is a function that allows us to facet with two variables - one with rows and one with columns.  We can't do that with this example, but, in essence, you would use a similar tilda notation as above - `facet_grid(rows ~ cols)`

#### 5.2 Faceting by continuous values
It's a little bit more difficult to facet our monthly plot. Maybe we want to split it up by decade. But, wait, year is continuous. What to do? R and `ggplot2` have a number of functions based off of the base function `cut()`. Let's try the first, that can make decadal plots.

```{r cut}
had_lines +
  facet_wrap(~cut_width(year, 10))
```

It's not pretty (that's a lot of decades!), but you can see the principle.


**Exercise:** 
1. Try out `cut_interval` and `cut_number`. What do they do?  
2. Make a plot where facets and colors reflect the same information.


### 6. Visualizing trends and summaries

As we move further into statistical visualization, `ggplot2` has a number of statistical summary plots. THe first, for grouped data, is `stat_summary`. This function will take grouped data and calculate means and other summary information. `fun.data` takes a function as an argument to calculate summary statistics. For example:

```{r mean_se}
had_plot_base +
  stat_summary(fun.data=mean_se)
```

For continuous data, we often look for trends. So going back to our faceted plot before, we can add a smoothed spline to each panel with:

```{r facet_trends, warnings=FALSE, messages=FALSE}
had_months +
  facet_wrap(~month_name) + 
  stat_smooth()
```

If instead we wanted a straight linear model fit or fit from another model, we can use the `method` argument.

```{r facet_trends_lm, warnings=FALSE, messages=FALSE}
had_months +
  facet_wrap(~month_name) + 
  stat_smooth(method="lm")
```

### 7. Making your plot theme your own

OK, getting back to our initial graph - the grey background, the font type, etc., might not really be doing it for you.
```{r echo=FALSE}
had_lines_color <- had_lines + scale_color_viridis()
had_lines_color
```

Ugh. Grey background. Weird white lines. Maybe we don't like the default.  Ggplot2 provides some alternatives wtih `theme` functions. Now, you can specify what you'd like to your heart's content, but, there are a few canned differnt themes that can be quite nice. The two I use most commonly are `theme_bw()` and `theme_void()`
```{r themes1}
had_lines_color + theme_bw()

had_lines_color + theme_void()
```

The `theme` function is highly dynamic - you can specify simple items, such as the `base_size` for basic font sizes, etc. Or you can customize to your hearts content, with angle of text on axes, different color schemes, etc. See `?theme` or the [theme vignette](http://docs.ggplot2.org/dev/vignettes/themes.html).


But it doesn't stop there. There are whole libraries of themes for you to try! And they are constantly being updated!  Want to make your figure look like it came from Excel, fivethirtyeight.com, or was made by Tufte himself?

```{r}
#install.packages("ggthemes")
library(ggthemes)

had_lines_color +
  theme_excel()

had_lines_color +
  theme_fivethirtyeight()

had_lines_color +
  theme_tufte(base_size=17)
```

See the `ggthemes` [package vignette](https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html) for more.

But for us, let's go with the solarized theme with the option `light=FALSE`, as it's close to the original.

```{r add_theme}
had_lines_color_theme <- had_lines_color +
  theme_solarized(light=FALSE)
```

**Exercise:** Take the year by anomoly plots and apply three different themes of your choice to them.

### 8. Plot annotation
OK, now for the fripperies and frills - annotating our plots with information! In the GIF above, we wad demarcations of certain critical threshold as well as a title. There were also years that changed as time went by, but we'll save that for a moment.

#### 8.1 Adding lines
First, there are two critical lines - 1.5 and 2.0 degrees C. To add a vertical or horizontal line to the plot, we use `geom_vline` or `geom_hline`. These accept a `xintercept` or `yintercept`, respectively, to indicate where they should cross a plot. So, to add to our current plot.

```{r add_lines}
had_lines_annotated <- had_lines_color_theme +
  geom_hline(yintercept=c(1.5, 2.0), color="red")

had_lines_annotated
```

### 8.2 Adding text annotations

We also want to have some annotations on the lines that say what each one is. For that, we could use `geom_text`, but, that can get messy with facets, etc. Better to use `annotate`. This function takes an x and y coordinate, a `geom` type, and then all of the arguments needed for that geom. Now, often we use this with just `geom_text`, but that gets tricky given that we want to overlay this on a line.

```{r bad_annotation}
had_lines_annotated +
  annotate(x=c(1,1), y=c(1.5,2), geom="text", label=c("1.5C", "2.0C"))
```

Instead, we want something that is a `label` - something that has a background. We need to specify the background color, which I'll put in in hex code, as I don't know it's name (looked this up in some information about solarized color palattes), and a color for the text - white.

```{r add_text}
had_lines_annotated <- had_lines_annotated+
        annotate(x=c(1,1), y=c(1.5,2), 
                 geom="label", 
                 label=c("1.5C", "2.0C"), 
                 fill="#002b36", 
                 color="white", label.size=0)  

had_lines_annotated
```

Gorgeous!

#### 8.3 Title

And last, titles are easy. There's a function for it! `ggtitle`

```{r add_title}
had_lines_annotated <- had_lines_annotated +
  ggtitle("Global Temperature Change (1850-2015)")

had_lines_annotated
```

### 9. Modifying axes

Of course, we've done this with simple cartesian axes that we haven't modified at all. We don't always want to do that.

#### 9.1 Axis limits

First, we want to have the lines extend all the way to the edges of the graph. So, to do that, we want the limits of the x-axis to be set to Jan and Dec. The `xlim()` function (and there is a `ylim()` function as well) should take care of that. If we had a continuous scale, we'd just feed a vector, with minimum and maximum - e.g. `c(1,12)`. 

But, here we want to feed the factor vector from which everything is derived. We've taken care of ordering, so, we no longer need `xlim()`. Instead, we want to use the `scale_x_discrete` function that lets us set properties of an x axis with discrete values. In our case, we want to alter how much additional space is placed around the first and last month using the `expand` argument. So let's cut that down to 0!

```{r xlim}
had_lines_annotated_final <- had_lines_annotated + 
  coord_cartesian(expand=FALSE)
  
had_lines_annotated_final
```

The labels get cut off - but they'll come back in a second!

#### 9.2 Axis modification

There are a number of ways to modify an axis. Using `scale_x_continuous()` provides access to a rich number of ways to modify the x axis (or replace with y, and you get the picture).

Often we're interested in log transforming our axes: 

```{r log, warning=FALSE}
had_lines_annotated_final + 
  scale_y_log10()
```

Here, this isn't a great idea, as we lose the negative numbers. But, you can begin to see the power of how the different axis scales can work. We can invoke an arbitrary scale using the `trans` argument of `scale_x_continuous()`, although there are many already buit in.

We want a circular plot, however, which implies polar coordinates. And there's an option for that, too! We'll also have to deal with that expansion issue again.

```{r polar}
had_lines_annotated_final <- had_lines_annotated_final +
  coord_polar()+ 
  scale_x_discrete(expand=c(0,0))
  
had_lines_annotated_final
```

Oh! Fancy! Almost there!

#### 9.3 Axis labeling
The whole month name, anomaly think on the axes is a little odd. Fortunately, we can replace the string labels on either axis. We could have put in other things, but for now, let's go blank.

```{r axis_lab}
had_lines_annotated_final <- had_lines_annotated_final +
  xlab("") +
  ylab("")

had_lines_annotated_final
```

For more great ggplot2 extensions, see https://www.ggplot2-exts.org/


### 10. Animation

Right now, we're in the midst of a great re-write of the package that actually performs animations with ggplot. The [gganimate](https://github.com/thomasp85/gganimate) package attempts to implement a nice consistent grammer of animation, much like a grammar for graphics. It's not on CRAN yet, so we'll have to install it and its companion, `transformr` with devtools.

```{r, eval=FALSE}
install.packages("devtools") #if you don't have it yet
devtools::install_github("thomasp85/farver")
devtools::install_github("thomasp85/tweenr")
devtools::install_github("thomasp85/transformr")
devtools::install_github("thomasp85/gganimate")
```

`gganimate` introduces a few new aesthetics for animations. The first is the `transition_*` series that defines how elements move between each other in a plot. For example, to generate the initial animation, we'd use the `transition_reveal()` which gradually adds pieces to the animation. It uses the first argument to say what is being added and the `along` argument to say in what order it should be added.

```{r animate, cache=TRUE}
library(gganimate)

had_lines_annotated_final  + 
   transition_reveal(year, along = year) 
```

What happens if you use `along=as.numeric(month_name)`? Note, we convert to a numeric as `gganimate` needs a number, and does not know how to order characters.

There are many many other elements to gganimate - it's a growing library - such as leaving trails behind points, and more - but that's a tutorial for another time (or one you should write!)

```{r trail, cache=TRUE}

had_points <- ggplot(data = hadcrut_temp_anomoly %>% filter(year < 1870),
                     aes(x = year, y = anomaly, color = month_name)) +
  geom_point() 

had_points_anim <- had_points +
  transition_time(year) 

had_points_anim +
  shadow_trail(0.05, max_frames=10) +
  exit_fade()
```

<!--chapter:end:04_ggplot_intro.Rmd-->

---
title: "Using Test Statistics"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size=17))
```

### 1. Put it to the Test
Today we're going to bein our exploration of how R handles performing statistical tests. Within R, there are many functions that enable one to do statistical testing. These functions typically force a user to be explicit about what they want R to do. In today's lab, we're going to first examine the simplicity of the $\chi^2$ test and begin to build a workflow for data analysis. We'll then dig into the t-test and more formally develop a data analysis workflow.

### 2. $\chi^2$
The $\chi^2$ test is a fairly straightforward test. It and its variations are ubiquitous in evaluating expectations. We also see it in likelihood ratio tests and evaluations of model fit across a variety of other techniques, all verifying whether or not our models adequately the data at hand.

#### 2.1 Day of Birth
In class we used a dataset looking at day of birth. Let's load that up and visualize it.

```{r load_birth}
#load some libriaries
library(readr)
library(ggplot2)

births <- read_csv("data/05/08e1DayOfBirth.csv")

births
```

Next, what's our expected value? If we assume each day should have an equal number of births, then...

```{r}
expected <- sum(births$`Number of births`)/nrow(births)

expected
```

Just seeing these numbers might be enough. We can also visulize things using 

```{r viz_chisq}
ggplot(data=births, mapping = aes(x=Day, y=`Number of births`)) +
  geom_point() +
  geom_hline(yintercept = expected)
```

But what abou the test. We don't have to worry about sample size. Our expectation is nothing fancy - an even distribution. So, we can use the `chisq.test()` function.

```{r chisq}
chisq.test(births$`Number of births`)
```

The output is fairly straightforward - we get our test statistic, DF, and p-value.

#### 2.2 Mass Extinctions and Non-Even Probabilities

We don't **have** to use an even probability distribution. For example, we might have an idea of how values should be distributed between cells. For example, let's look at the number of families going extinct in major extinction events over geological time.

```{r extinct}
extinct <- read_csv("data/05/08e6MassExtinctions.csv")

ggplot(extinct, mapping=aes(x=`Number of extinctions`, y=Frequency)) +
  geom_line()
```

So, few mass extinctions, but many where a few families went extinct. And 0 where 0 went extinct (hey, it wouldn't be an extinction event, otherwise).

One distribution that matches this kind of shape is negative binomial, describing the number of successes (families going extinct) until the first failure (extinction event ending). In our data, we 3.6 extinctions on average, and for fun, let's assume an overdispersion parameter of 3.

From this, we can generate a vector of expectations by first getting a vector of porportion of data that should be in each cell from the probability density distribution, and then multiplying it by the total number of observations.

```{r exp}
#get raw frequencies
freqs <- dnbinom(0:20, mu = 3.6, size = 3)

#make sure they sum to one
freqs <- freqs/sum(freqs)

extinct$expected <- freqs*sum(extinct$Frequency)
```

We can even plot the match/mismatch
```{r plot_exp}
ggplot(data=extinct, mapping=aes(x=expected, y=Frequency)) +
  geom_point()
```

Eh, not bad.

And then put it to the test, using the p argument to incorporate our new expected frequencies.

```{r chisq_ext}
chisq.test(x = extinct$Frequency,
           p = freqs)
```

Note the warning message. This indicates that there might be something fishy here - and, indeed, we know from looking at the data that many cells have <5 observations in them, indicating that we might want to rethink if this is the right approach.

#### 2.3 Faded Examples
So, to review, here's the births example, where we'll load, visualize, and test.

```{r birth_fade, eval=FALSE}
#Load
births <- read_csv("data/05/08e1DayOfBirth.csv")

#calculate expected values
expected <- sum(births$`Number of births`)/nrow(births)

#visualize
ggplot(data=births, mapping = aes(x=Day, y=`Number of births`)) +
  geom_point() +
  geom_hline(yintercept = expected)

#test
chisq.test(births$`Number of births`)
```

Let's try this for two other data sets.

First, days on which people buy Powerball tickets.
```{r powerball, eval=FALSE}
#load
powerball <- ____("data/05/08q02Powerball.csv")

#calculate expected values
expected_powerball <- sum(powerball$`Millions of tickets sold`)/nrow(powerball)

#visualize
ggplot(data=___, mapping = aes(x=Day, y=`Millions of tickets sold`)) +
  geom_point() +
  geom_hline(yintercept = ___)

##test
chisq.test(____$`Millions of tickets sold`)
```


Last, number of boys in 2-child families. We'd expect a distribution of 25:50:25 for 0,1, or 2 boys. So, an uneven frequency.


```{r boys, eval=FALSE}
#load
boys <- ____("data/05/08e5NumberOfBoys.csv")

#calculate expected values
freq_boys <- c(0.25, 0.5, 0.25)
boys$expected_boys <- freq_boys * sum(boys$Frequency)

#visualize
ggplot(data=___, mapping = ____(x=expected_boys, y=_____)) +
  geom_point()

##test
chisq.test(boys$_____,
           p = _______)
```


#### 2.4 Contingency Tables
Briefly, let's consider an extension of the $\chi^2$ test, the contingency table. The first difference between contingency table data and what we've already considered is that there are multiple columns with categories instead of just one.

```{r read_parasite, message=FALSE}
parasite <- read_csv("./data/09e3ParasiteBrainWarp.csv")

parasite
```

How can we visualize this? Well, we have two options. The first is as a stacked bar plot. This lets us see if, whatever we put on the x axis, is roughly evenly distributed. Let's look at it both ways using `patchwork` to put it into one figure.

```{r plot_cont_tab}
#devtools::install_github("thomasp85/patchwork") #if you don't have it.

library(patchwork)

a <- ggplot(parasite, 
       mapping = aes(fill=`infection status`, y = frequency, 
                     x=eaten)) +
  geom_col() 

b <- ggplot(parasite, 
       mapping = aes(x=`infection status`, y = frequency, 
                     fill=eaten)) +
  geom_col() 

a + b
```

Often times, we want to view the contingency table as is. For that, we can use `geom_raster` to make colored blocks - and we can even include the numbers as a form of redundant coding.

```{r plot_cont_tab_raster}
ggplot(parasite, 
       mapping = aes(x=`infection status`, y = eaten, 
                     fill=frequency, label = frequency)) +
  geom_raster() +
  scale_fill_gradient(low = "white", high = "orange") +
  geom_text()
```


Our data, on the other hand, doesn't look like the contingency table presented here. It needs a bit of reshaping for `chisq.test` to consider it as one. For that, we use the `xtabs` function - which, incidentally, will generalize into an array for an n x n contingency table.

```{r cont_tabl}
cont_tab <- xtabs(frequency ~ eaten + `infection status`, data = parasite)

cont_tab
```

Perfect - now we can just plug it into `chisq.test` and get our answer.

```{r cont_chisq}
chisq.test(cont_tab)
```

### 3. T-Tests

T-tests are among the most frequently used tests in data analysis. They're delightfully simple, and provide a robust example of how to examine the entire workflow of a data analysis. These are steps you'll take with any analysis you do in the future, no matter how complex the model! 

#### 3.1 One Sample T-Test

For a one sample t-test, we're merely testing whether or not a vector of observations are different from zero, our null hypothesis. This can be a sample of observed values, it can be a sample of differences between paired treatments, anything!

Let's look at the W&S data on blackbird immunoresponse before and after testosterone implants. So, first, load the data and visualize the change in immunoresponse.

```{r blackbird, message=FALSE}
blackbird <- read_csv("data/05/12e2BlackbirdTestosterone.csv")

ggplot(data = blackbird, mapping=aes(x=dif)) + 
  geom_density() +
  geom_vline(xintercept=0, lty=2)
```

So, right away we can see the problem with the distribution of the data.

Let's proceed and ignore it for the moment.

The `t.test()` function gives us a lot of options.
```{r t_test, eval=FALSE}
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)
```

We can feed it different alternatives (it defaults to two tails), specify a hypothesis we're testing against (it defaults to a null hypothesis with mu=0), and we can give it either just a sample of observations, or also a vector with groups. We can also tell it whether we're worried about equal variances, if we're giving it paired data, and more.

In this case, for a one-sample t-test, we just want to feed it our differences.

```{r t_blackbird}
t.test(blackbird$dif)
```

That's a fairly wonky output table, but we see all of the critical information - the value of t, our DF, our p-value, and both a mean estimate and confidence interval. Here we see a p-value of `r t.test(blackbird$dif)$p.value` that suggests we fail to reject the null.

If you find this output ugly, there's a wonderful package called `broom` that produces a standardized set of model output tidiers in a data frame format.

```{r broom}
library(broom)
tidy(t.test(blackbird$dif))
```

More on `broom` next week.

#### 3.2 Two Sample T-Test
For a two sample t-test, we can feed in a vector of groups, or, `t.test()` is the first function that will take a *formula* for statistical tests. Let's look at a data set examining the survival of juvenile chinook salmon (yum) in rivers with versus without brook trout.

First, we'll load and plot the data.

```{r chinook, message=FALSE}
chinook <- read_csv("data/05/12e4BrookTrout.csv")

ggplot(data=chinook, mapping=aes(x=`brook trout?`, y=`Mean chinook survival`)) +
  geom_boxplot() 
```

We have some unequal variances in the data here, which is of note. We also have two groups.  

As with `xtabs` above, R has a formula syntax that we can use to specify a relationship where we have a predictor and response. Broadly, it looks like:

$$response \sim predictor$$

We can use this syntax with our t.test function, as it also accepts formulae. So, here's our two-sample unpaired t-test with unequal variances:

```{r chinook_t}
chin_test <- t.test(`Mean chinook survival` ~ `brook trout?`, data = chinook,
       unequal.var = TRUE)

chin_test
```

Great, we have our means per group, the difference between them, and we see we're using a Welch's t-test for unequal variance. Here we'd again fail to reject the null hypothesis.

If you want prettier output:

```{r chinook_brook}
tidy(chin_test)
```

OK, not that much prettier, but you can put it in a table.

#### 3.3 Evaluating Residuals
So, we've ignored the assumption of normality up until now. Broadly, in any statistical model, error comes in as a residual value. So, often our data may not be normally distribted, but after accounting for predictors, we find that the *residuals* are. To test whether residuals are normal, we need to, well, create residuals!

For a t-test this is easy, as residuals are just means - either of a single column, or from groups.  We can thus use `dplyr` here. So for our one-sample t-test of blackbirds

```{r black_resid}
library(dplyr)

blackbird <- blackbird %>%
  mutate(resid = dif - mean(dif))
```

For our two-sample t-test, we use `group_by` for group means.
```{r chinook_resid}
chinook <- chinook %>%
  group_by( `brook trout?`) %>%
  mutate(mean_survival_resid = 
           `Mean chinook survival` - mean(`Mean chinook survival`)) %>%
  ungroup()
```

We can then evaluate for normality. Let's use the blackbird example. First, we'd look at the distribution of residuals.

```{r resid_hist}
ggplot(data = blackbird, mapping=aes(x=resid)) +
  geom_density()
```

Again, not looking good. But, who knows, this is a density estimated off of not many data points. Maybe we need something more accurate - like a qqplot. For a qqplot, we invoke two functions in base plot (if we want the fit line - `ggplot2` still doesn't do this, but give it time).

The functions are `qqnorm` and `qqline`. We use them sequentially.

```{r qq}
qqnorm(blackbird$resid)
qqline(blackbird$resid)
```

Now we can see that systematic behavior in the lower tail.

We may still want to *put it to the test* as it were, with a Shapiro Wilk's test. R provides a `shapiro.test()` functio for this.

```{r shapiro}
shapiro.test(blackbird$resid)
```

OK, so, what does that p-value mean? In this case, it means we would fail to reject the null hypothesis that this data comes from a normal distribution. So, we should actually be OK going forward! This is one case where we don't want to have a p value smaller than our alpha.

**Exercise** - Repeat this normality analysis for the chinook salmon!

#### 3.3 Plotting results

For a one-sample t-test, plotting a result - a mean and SE - might not be necessary. But for a two-sample test, it's highly informative! It should be the final step in any analysis in order to aid interpretation. Here, `ggplot2`'s stat_summary function is invaluable, as it defaults to plotting mean and standard errors.

```{r plot_salmon_means, message=FALSE}
salmon_means <- ggplot(data=chinook, 
                       mapping=aes(x=`brook trout?`, 
                                   y=`Mean chinook survival`)) +
  stat_summary(size=1.5)

salmon_means
```

Nice. If you want to see this relative to the data, you can still include it.

```{r plot_salmon_data, message=FALSE}
salmon_means+
  geom_jitter(color="red")
```

#### 3.4 Workflow and Faded Examples
As we've talked about, our general workflow for an analysis is

1) Build a Test
2) Evaluate Assumptions of Test
3) Evaluate Results
4) Visualize Results

If we've decided on a t-test, we've satisfied #1. So let's go through a few examples where we load up our data, evaluate assumptions, evaluate the results of our test, and visualize the results.

We'll start with the salmon example, all in one place.
```{r}
#Load and visualize data
chinook <- read_csv("data/05/12e4BrookTrout.csv")

ggplot(data=chinook, mapping=aes(x=`brook trout?`, y=`Mean chinook survival`)) +
  geom_boxplot() 

## test assumptions
chinook <- chinook %>%
  group_by( `brook trout?`) %>%
  mutate(resid = 
           `Mean chinook survival` - mean(`Mean chinook survival`)) %>%
  ungroup()

#qq
qqnorm(chinook$resid)
qqline(chinook$resid)

shapiro.test(chinook$resid)

#put it to the test!
t.test(`Mean chinook survival` ~ `brook trout?`, data = chinook,
       unequal.var = TRUE)
 
ggplot(data=chinook, 
                       mapping=aes(x=`brook trout?`, 
                                   y=`Mean chinook survival`)) +
  stat_summary(size=1.5)
```

OK, now that we have this, let's apply the same strategy to Cichlid habitat preferences that vary by genotypes.

```{r chichlid, eval=FALSE}
#Load and visualize data
cichlid <- read_csv("data/05/12q09Cichlids.csv")

ggplot(data=cichlid, mapping=aes(x=Genotype, y=preference)) +
  ____() 

## test assumptions
cichlid <- cichlid %>%
  group_by(____) %>%
  mutate(resid = 
          preference - mean(preference)) %>%
  ungroup()

#qq
qqnorm(cichlid$____)
qqline(cichlid$____)

shapiro.test(cichlid$____)

#put it to the test!
t.test(____ ~ ____, data = ____,
       unequal.var = TRUE)
 
ggplot(data=cichlid, mapping=aes(x=____,y=____)) +
  stat_summary(size=1.5)
```


And now for how monogamy influences testes size

```{r monogamy, eval=FALSE}
#Load and visualize data
monogomy <- ____("data/05/12q05MonogamousTestes.csv")

ggplot(data=____, mapping=aes(x=`Column 1` , y=`Testes area`)) +
  ____() 

## test assumptions
monogomy <- monogomy %>%
  group_by(____) %>%
  ____(resid = 
          `Testes area` - ____(____)) %>%
  ungroup()

#qq
____(____)
____(____)

shapiro.test(____)

#put it to the test!
t.test(____ ~ ____ , data = monogomy,
       unequal.var = ____)
 
ggplot(data=____, mapping=aes(x=____ ,y=____)) +
  ____(size=1.5)
```

### 4. T and Power

Trying to think about power analysis by simulation can be a daunting prospect. However, for any type of modeling endeavour, you can think about the various steps you'll need to execute in order to come up with a straightforward workflow.  You know that for any set of parameters you are monkeying with, you'll need to:  
  
1. Simulate a data set with those parameters.  
2. Fit a model to that simulated dataset and extract a p-value.  
3. Execute steps 1-2 some number of times.  
4. From those p-values, calculate power (1 - proportion of wrong p-values at a prespecified $\alpha$).  

For each of the above steps, we can write a nice simple function! And then use some `dplyr` magic to put the whole effort together. Let's walk through this, piece by piece.

#### 4.1 Simulating Data with a Function

The basic underlying model of a t-test is $y_{ij} ~ N(\mu_i, \sigma)$ where i us the group identity. We know there are only two means. So, we can whip up a simple function that returns a data frame with two groups, and data for each at some pre-specified sample size.

```{r fake_data}
make_t_data <- function(m1, m2, s, n){
  #make a data frame, repeating treatments n number of times
  #and use rnorm to get values
  data.frame(treatment = c(rep("A", n), rep("B", n)),
             value = rnorm(n*2, mean = c(rep(m1,n), rep(m2, n)), s))
}
```

To test if it works, you can record values for all of the parameters and see if the guts of the function work. We can also test it.

```{r test}
make_t_data(m1 = 1, m2 = 2, s = 1, n = 3)
```

#### 4.2 Fit a model and extract a p-value

OK, our next function needs to take a data set, and instead of all of the t-output, return the p-value of a t-test. If we look at the `t.test` helpfile, we can see that the output of any `t.test` is a list with an entry called `p.value`. So, we can write a simple extraction function, given the dataset from above.

```{r get_p_fun}
get_p_from_t_test <- function(sim_data){
  #run the t test on the data
  test <- t.test(value ~ treatment, data = sim_data)
  test$p.value
}
```

Let's test that out.

```{r}
get_p_from_t_test(make_t_data(m1 = 1, m2 = 2, s = 1, n = 3))
```

#### 4.3 Repeat some number of times

This is great! Now, we know we want some huge vector of p-values. Let's say we want 100 simulated p-values. For that, we've already introduced the `replicate()` function. Indeed, we can take the above statement, and show how it works with replicate.

```{r rep}
replicate(10,
          get_p_from_t_test(make_t_data(m1 = 1, m2 = 2, s = 1, n = 3)))
```

The nice thing about this approach is that, rather than generating a lot of data sets, and then testing each one indivudally - which would eat a lot of memory with a large number of simulations - we do things one at a time, so we're only limited by processor power (and could later multithread if we wanted).

We can also eyeball the above statement and see what the power is visually. What fraction are wrong? For a tiny sample size, and a SD that is the same as the difference between the means, we should see that our test has low power (i.e., most p values will be larger than some pre-specified low value of alpha assuming we should be rejecting our null). If that wasn't the case, you'd want to go back and see what went wrong.

Now, the above statement is, indeed, something we could wrap into a function quite nicely. A function for calculating power, which brings us to....

#### 4.4 Calculating Power from a vector of p-values.
OK, we have the parameters we need for simulations. We also need some number of simulations - let's call that `nsims` - and some pre-specified `alpha` in order to calculate power `1-(# of p > alpha)/nsims`. Let's code out a brief function just in comments, giving a default value for `nsims` and `alpha`.

```{r get_p_comments}
get_t_power <- function(m1, m2, s, n, nsims = 100, alpha = 0.07){
  #get a vector of p values
  
  #calculate the number of p values that are incorrect given
  #that we should be rejecting the null
  
  #return power
}
```

Put that way, we can fill the above function in with things we have already discussed. Remember, for booleans `TRUE = 1` and `FALSE = 0`

```{r get_p}
get_t_power <- function(m1, m2, s, n, nsims = 100, alpha = 0.07){
  #get a vector of p values
  p <- replicate(nsims,
          get_p_from_t_test(make_t_data(m1, m2, s, n)))
  
  #calculate the number of p values that are incorrect given
  #that we should be rejecting the null
  num_wrong <- sum(p > alpha)
  
  #return power
  1 - num_wrong/nsims
}
```

We can again test and make sure this produces sensible results.

```{r test_pow}
get_t_power(m1 = 1, m2 = 2, s = 1, n = 3, nsims = 100, alpha=0.07)
```

Yeah, that's not great power - nor should it be! We can test again, just to make sure, by putting in something that should have high power.

```{r test_pow_bigdiff}
get_t_power(m1 = 1, m2 = 5, s = 1, n = 3, nsims = 100, alpha=0.07)
```

Much better. Now... let's put the whole shebang together.

#### Power analysis!

Let's make a data frame where we look at a few effect sizes (difference between means), standard deviations, and sample sizes. We'll use the `tidyr` function `crossing()` which gets all possible combinations of parameter values, and then `dplyr::rowwise()` to iterate through our set of parameters, one by one. Note, this might take a little while.

```{r pow, message = FALSE, warning = FALSE, cache = TRUE}

#power!
library(tidyr)
library(dplyr)

pow_df <- crossing(diff = 1:5, s = 1:5, n = 5:10) %>%
  rowwise() %>%
  mutate(power = get_t_power(m1 = 0, m2 = diff, s = s, n = n, nsims=100, alpha=0.05)) %>%
  ungroup()

```

Now we can visualize this using ggplot2. For funsies, let's use the `beyonce` set of color palattes. YAS KWEEN!

```{r}
library(ggplot2)

#devtools::install_github("dill/beyonce")
library(beyonce)


ggplot(pow_df, aes(x=n, y = power, color = factor(diff))) +
  geom_point() +
  geom_line() +
  facet_wrap(~s) +
  scale_color_manual(values = beyonce_palette(79)) 
```

NICE!

**EXERCISE**: Try analyses that vary alpha, n, and the effect size. Or get crazy and vary those three *and* the SD.

<!--chapter:end:05_chisq_t.Rmd-->

---
title: "Hypothesis Testing, Simulation, and Power"
author: "Bill 607"
date: "September 29, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Probability Density Distributions
Every probability distribution in R - whether in the base package or in other packages - typically obeys a standard naming conventions. For some distribution, `dist`, then `ddist` is the function for it's density, `pdist` would give you p-values, `qdist` allows you to get the value of an observation that will produce a given p-value, and `rdist` uses the distribution to generate random numbers.

### 1.1 Normal and Continuous Densities
A normal distribution in R is defined by it's mean and standard deviation. To get a point density for a normal distribution, `dnorm()` accepts an obsered value, a `mean` argument, and a `sd` argument. The default mean and SD are for a standard normal distribution.

Let's say we wanted to get the density from an observation of 5 from a distribution with mean 8 and sd of 3.

```{r dnorm}
dnorm(5, mean = 8, sd = 3)
```

Woo! Number!

So, I've been plotting all kinds of distributions over the last few weeks. To do that, all I've needed was the density over a wide range of values. So, using this distribution, let's plot the normal curve from 0 to 16. For this, we'd give dnorm a vector instead of a single observation.

```{r dnorm_vec}
dens_frame <- data.frame(x = seq(0, 16, length.out = 200))

#note - you could do this with mutate! But I like $ 
dens_frame$density <- dnorm(dens_frame$x, mean = 8, sd = 3)
```

That produces a lot of numbers - but we can feed them easily ito ggplot2.

```{r plotnorm}
library(ggplot2)

norm_dens <- ggplot(dens_frame,
                    mapping=aes(x=x, y=density)) +
  geom_line()

norm_dens
```

Neat!

We've also already seen normal distributions in action to generate random variables.

```{r rand_norm}
rnorm(10, mean = 8, sd=3)
```

### 1.2 Poisson and Discrete Densities

What about discrete distributions? For the poisson distribution, we have the `dpois()` function for density, which just takes `lambda` as its argument to define the distribution. So, assuming a lambda of 10, what's the probability of an observation of 3?

```{r pois}
dpois(3, lambda=10)
```

Note, if you enter a continuous number, you get an error.

```{r pois2}
dpois(1.5, lambda=10)
```

Negative numbers just give a probability of 0, which is sensible. They are not possible.

Plotting is a wee bit funnier, as these distributions are discrete.

```{r dpois_vec}
pois_frame = data.frame(x = 0:20)

#note - you could do this with mutate! But I like $ 
pois_frame$density <- dpois(pois_frame$x, lambda = 10)
```

So how to plot? Line plots are... dishonest. We need to show the discrete nature of the distribution. So, we have `geom_bar` to come to our rescue. `stat="identity"` means just plot what value is supplied. And we can monkey with the bar width to express the discrete nature of the distribution.

```{r plot_pois}
ggplot(pois_frame) +
  aes(x=x, y=density) +
  geom_bar(width=0.1, stat="identity")

```


### 1.3 Exercises: Exploring Distributions
OK, let's try getting the density of one value and plotting the distribution of a few different distributions

1) Lognormal
```{r exercises, eval=FALSE}
# What's the density of 4
# from a standard lognormal?
dlnorm(4, meanlog=0, sdlog = 1)

#plot the density
vals <- seq(0,10, length.out=500)
dens <- dlnorm(vals, meanlog = 0, sdlog = 1)
ggplot(mapping = aes(x=vals, y=dens)) +
  geom_line() +
  geom_vline(xintercept = 4)
```

2) Gamma - Spruce budworms take 2 hours to eat a single leaf. You have given your budworm 5 leaves. What is the probability that you will wait 15 hours for it to finish eating? 
```{r gamma_ex, eval=FALSE}
dgamma(___, scale = 2, shape = 5)

#plot the density
vals <- seq(0,20, length.out = ___)
dens <- dgamma(___, scale = 2, shape = 5)

ggplot(mapping = aes(x=vals, y=dens)) +
  geom_line() +
  geom_vline(xintercept = ___)
```
3) Exponential - The distribution of time between events happening, with rate = 1/(average time to next event). If the mean time to mortality of a super-corgi is 20 years, what's the probability that a super-corgi will live to 30

```{r exp, eval=FALSE}
dexp(___, rate = ___)

#plot the density
vals <- ___(0,50, length.out=___)
dens <- ___(___, rate = ___)
ggplot(mapping = aes(x=___, y=___)) +
  geom_line() +
  geom_vline(___ = ___)
```

4) Binomial - You flip a coin 100 times, and think that you have 50:50 chance of heads or tails. What's the probability of obtaining 40 heads?

```{r bin, eval=FALSE}
dbinom(___, size = ___, prob = ___)

#plot the density
vals <- 0:100
dens <- ___(___, ___ = ___, ___=___)
ggplot(____ = ___(x=___, y=___)) +
  geom_bar(width=0.6, stat="identity") +
  ___(___ = ___)
```


## 2. Getting P-Values from Distributions

From getting densities it's a hop, skip, and a jump to getting p-values. With densitites, we were getting point probabilities. With p-values, we invoke the quantile function (which starts with `p`). Think about it. Quantiles are non-parametric p-values, if you will, stating the value that X% of a sample is less than or equal to.

### 2.1 1-Tailed P Values

Using that as a jumping off point, let us remember that quantiles are ordered from smallest value to largest value. They are inherently 1-tailed. Let's take an example of surveying coral reefs after a hurricane. We survey 500 reefs one year after a hurricane, and want to know if the hurricane has had an effect. We know that reefs get damaged from time to time by any number of things - not just storms. Our expectation based on past surveys is that there's a 30% chance of a reef being damaged. So, a 70% chance of it being undamaged. We survey the reefs, and find that 300 are undamaged. 

What is the probability of observing 300 or fewer undamaged reefs?

```{r binom_p}
pbinom(300, size = 500, prob = 0.7)
```

Excellent. So, good chance that this hurricane had an effect! What if we had wanted to do this with the damaged information instead?

```{r binom_p_dead}
pbinom(200, size = 500, prob = 0.3)
```

Oh that's weird. A really high p-value. Why?

Well, a quantile function looks at the *lower tail* of a distribution. If we're talking about number of damaged reefs, and want to know if the number is higher than usual, then we want to look at the upper tail. This is fairly straightforward with one additional argument. Or you can cleverly use some subtraction for many well behaved distributions.

```{r binom_p_2}
pbinom(200, size = 500, prob = 0.3, lower.tail=FALSE)

1 - pbinom(200, size = 500, prob = 0.3)
```


### 2.2 Two-Tailed Tests

What if instead of assuming that a hurricane would cause more damage, we weren't sure, one way or another. For example, maybe the hurricane stimulated so much growth, that it could have mitigated the propensity of a reef to be damaged!

Simply put, we take the p-value based on the appropriate tail, and double it.

```{r binom_p_two_tail}
2*pbinom(300, size = 500, prob = 0.7)
```

There are all sorts of tricks one can do to minimize coding here in different tests. For example, for a z-test, one can take the absolute value of the z-test, now that it's positive, and then use `lower.tail=FALSE` and multiply the whole shebang by 2. You'll see this with t-tests next weeek.

For example, let's assume a sample of 10 flies with an antenna length of 0.4mm. The population of fly antenna lengths has a mean of 0.3mm with a SD of 0.1SD.

```{r pnorm}
#Calculate SE based on the population
sigma_ybar = 0.1/sqrt(5)

#Z-Score
z <- (0.4 - 0.5)/sigma_ybar

#P-Value
2*pnorm(abs(z), lower.tail=FALSE)

```

## 3. P-Value via Simulation
What if you don't know your distirbution, or it's something weird? Let's go back to the tapir example from class

### 3.1 Creating a null distribution
So, we know that tapirs have a 90% chance, with even probability, of having a nose that is 5-10cm in length. And a 10% chance, with an even distribution of probability, of having noses from 10 to 15cm in length. Greater than that or less than that means that those tapirs have experienced the wrath of Papa Darwin.

First, how do we build a null distribution?

Simply put, start by making a vector of all of the possible values you might want.
```{r null_tapir}
nose_length <- c(seq(5,15, length.out=2000))
```

OK, we know that if `nose_length` $\le$ 10, there's a 90% chance of seeing one of those values. Otherwise, a 10% chance. So first we find the index of where nose length begins being over 10. 

`which` is a great function that 

```{r middle}
which(nose_length>10)[1]
```

1001 - cool, so, right in the middle. That means, the first 1000 have a high chance of being chose, and the second 1000 have a low chance of being chosen.

If you're in the first 1000, each individual size has a 0.9/1000 chance of being observed. If you're in the second 1000, each size has a 0.1/1000 chance of being observed. Let's turn that into a vector
```{r probs}
prob_observed =  c(rep(0.9/1000, 1000), rep(0.1/1000, 1000))
```

Now we can use `sample` to take replicate samples of a fictional population to create a distribution. Let's use a population size of 100000 - although you could go larger. We'll sample from the list of nose lengths, with replacement, but use the `probs` argument to specify the chance of being chosen using our `prob_observed` vector.

```{r sample_tapir}
samp <- sample(nose_length, 100000, replace=TRUE,
               prob = prob_observed)
```

### 3.2 Getting a p-value
OK, you have a null population. How do you get a p-value from that population sample? Let's say, for example, we have a tapir with a nose length of 14.5. Is it a tapir, or a *faux* tapir?

Given that p-values are the probability of observing a value or something greater than it, this translates to the fraction of individuals in a population that is equal to or greater than some value. So, what fraction of the observations in your simulated population are equal to or greater than 14.5?

We can assess this a number of ways. Using our ```which``` function above, we can look at the length of the output vector.

Or, in any comparison that produces boolean values, `TRUE` = 1 and `FALSE` = 0. So we could sum up a comparison.

```{r sample_p}
length(which(samp >= 14.5))/length(samp)

sum(samp >= 14.5)/length(samp)
```

We can of course make this a two-tailed comparison by doubling our simulated p-value.

## 4. Power Via Simulation

To evaluate how power works via simulation, let's walk through an example with a z-test. Let's assume an average population-wide resting heart rate of 80 beats per minute with a standard deviation of 6 BPM.

A given drug speeds people's heart rates up on average by 5 BPM. What sample size do we need to achieve a power of 0.8?

### 4.1 Creating the simulation

To begin with, we need to create a simulated data frame of observations at different sample sizes. We've done this before two weeks ago using dplyr, so, making a data frame with, let's say 500, replicates of each sample size between 1-10 shouldn't be too onerous. We'll draw our random samples from a normal distribution with mean of 85 BPM and a SD of 6 BPM.

```{r sim_data}
library(dplyr)

#first make a data frame with sample sizes
sim_data <- data.frame(samps = rep(1:10, 500)) %>%
  
  #a trick - group by 1:n() means you don't need sim numbers!
  group_by(1:n()) %>%
  
  #get a sample mean
  mutate(sample = mean(rnorm(samps, 90, 6))) %>%
  
  #clean up
  ungroup()
```

### 4.2 Applying the Test
OK, now you have the data....so, let's do a z-test! We can define a variable for the population standard error of the mean, and then use that to calculate z, and then calculate p.

```{r z_sim}

sim_data <- sim_data %>%
  mutate(se_y = 6/sqrt(samps)) %>%
  mutate(z = (sample-80)/se_y) %>%
  mutate(p = 2*pnorm(abs(z), lower.tail=FALSE))
```

Cool! We have p values! We can even plot this by sample size

```{r p-plot}
ggplot(data = sim_data, mapping = aes(x=samps, y=p)) +
  geom_jitter(alpha=0.4)
```

### 4.3 Calculating Power
Now - power! We know that our type II error rate, beta, is the fraction of p values that are greater than our alpha (here 0.05). Great! Power is then 1 - beta.

Note, I'm going to use `n()` within a group here rather than trying to remember how big my number or replicate simulations for each sample size was. Protect yourself! Practice lazy coding!

```{r power}
sim_power <- sim_data %>%
  group_by(samps) %>%
  summarise(power = 1-sum(p>0.05)/n()) %>% 
  ungroup()
```

And we can plot that result with our critical threshold for power.

```{r powerplot}
ggplot(data=sim_power, mapping = aes(x=samps, y=power)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept=0.8, lty=2)
```

Wow - pretty good!

### 4.3 Exercise
OK, let's take the above example, and now, assume that the drug only increased the BPM by 2, the population has a SD of 7, and our alpha is 0.01.  What is our power?


### 4.3 Multiple Alphas

Let's say we wanted to look at multiple alphas. Well, we'd need another column for alpha! And we want all possible combinations of alpha and the `sim_data`. For that, we can use a function in the `tidyr` library that does that sort of all possible combinations of values in two objects called `crossing()`.

```{r alphas}
library(tidyr)

sim_tradeoff <- sim_data %>%
  
  #more alphas!
  crossing(alpha = 10^(-1:-10)) %>%
  
  #now calculate power for each alpha
  group_by(samps, alpha) %>%
  summarise(power = 1-sum(p>alpha)/500) %>%
  ungroup()
```

We can then plot this with `ggplot`

```{r plot_tradeoff}
ggplot(sim_tradeoff) +
  aes(x=samps, y=power, color=factor(alpha)) +
  geom_point() + geom_line() +
  xlab("Sample Size") +
  
  #just a little note on greek letters and re-titling
  #colorbars
  scale_color_discrete(guide = guide_legend(title=expression(alpha)))
```


### 4.3 Exercise (and homework): Many SDs

1) Let's start from scratch. Assume a mean effect of 5 again. But now, make a simulated data frame that not only looks at multiple sample sizes, but also multiple SD values. You're going to want `crossing` with your intitial data frame of sample sizes and a vector of sd values from 3 to 10 (just iterate by 1).

2) OK, now that you've done that, calculate the results from z-tests. Plot p by sample size, using `facet_wrap` for different SD values.

3) Now plot power, but use color for different SD values. Include our threshold power of 0.8.

4) Last, use `crossing` again to explore changing alphas from 0.01 to 0.1. Plot power curves with different alphas as different colors, and use faceting to look at different SDs. 

5) What do you learn about how alpha and SD affect power? 

6) How do you think that changing the effect size would affect power? You can just answer this without coding out anything. Based on what we've learned so far - what do you think?

<!--chapter:end:05_hypothesis_power.Rmd-->

---
title: "Linear Regression"
author: "Bill 607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

Believe it or not, despite all of the complexity under the hood, fitting a linear model in R with least squares is quite simple with a straightfoward workflow.

1. Load the data
2. Visualize the data - just to detect problems and perform a cursory test of assumptions!
3. Fit the model.
4. Use the fit model to test assumptions
5. Evaluate the model
6. Visualize the fit model

Let's go through each step with an example of seals. Are older seals larger?

### 0. Load and visualize the data

## Are Older Seals Bigger?
```{r}
library(dplyr)
library(ggplot2)

seals <- read.csv("./data/17e8ShrinkingSeals Trites 1996.csv")

seal_base <- ggplot(seals, aes(x=age.days, y=length.cm)) +
  geom_point() +
  theme_grey(base_size=14)

seal_base
```

Neat data set, no?

Now, looking at this from the get-go, we can see it's likely nonlinear. Maybe even non-normal! Let's ignore that for now, as it will make the results great fodder for our diagnostics!

### 1. Fitting a model

OK, so, you have the data. And in your model, you want to see how age is a predictor of length. How do you fit it?

Nothing could be simpler. R has a function called `lm()` which stands for linear model. It works just like base plot, with the `y ~ x` syntax. And we create a fit model object.

```{r lm}
seal_lm <- lm(length.cm ~ age.days, data=seals)
```

That's it.

Now, if we want to just peak at the fit, before going forward, we can use `coef()` which is pretty standard across all R fit model functions.

```{r coef}
coef(seal_lm)
```

But that's getting ahead of ourselves...

### 2. Evaluating Assumptions

R also provides a 1-stop shop for evaluating functions. Fit model objects can typically be plotted. Now, it uses base plot, so, we'll use the `par` function to setup a 2x2 plotting area.

```{r assumptions}
par(mfrow = c(2,2)) #2 rows, 2 columns

plot(seal_lm)

par(mfrow=c(1,1)) #reset back to 1 panel
```

Whoah - that's a lot! And, there's no figure with Cook's D or a histogram of residuals.

OK, breathe.

`plot.lm()` actualyl generates even more plots than shown here. You can specify what plot you want with the `which` argument, but will need to look at `?plot.lm` to know just what to look at.

I have five plots I really like to look at - four of which `plot.lm()` will generate. Those four are the fitted versus residuals:

```{r assumptions_fit_resid}
plot(seal_lm, which=1)
```

Note the curvature of the line? Troubling, or a high n?

A QQ plot of the residuals
```{r qq}
plot(seal_lm, which=2)
```
Not bad!

The Cook's D values
```{r cooks}
plot(seal_lm, which=4)
```
All quite small!

And last, leverage
```{r leverage}
plot(seal_lm, which=5)
```

I also like to look at the histogram of residuals.There is a function called `residuals` that will work on nearly any fit model object in R. So we can just...

```{r resid_hist}
hist(residuals(seal_lm))
```

Note, there's also a library called modelr which can add the appropriate residuals to your data frame using a dplyr-like syntax. 

```{r modelr_resd}
library(modelr)

seals <- seals %>%
  add_residuals(seal_lm)

head(seals)
```

Check out that new column. You can now plot your predictor versus residuals, which should show no trend, which you can use a spline with stat_smooth to see.

```{r obs_resid}
qplot(age.days, resid, data=seals) +
  stat_smooth()
```

And you can also add fitted values and look at fitted versus residual.

```{r modelr_fit}
seals <- seals %>%
  add_predictions(seal_lm)

qplot(pred, resid, data=seals) +
  stat_smooth(method="lm")
```

### 3. Putting it to the test
OK, ok, everything looks fine. Now, how do we test our model.

First, F-tests! R has a base method called `anova` which - well, it's for looking at analysis of partitioning variance, but really will take on a wide variety of forms as we go forward. For now, it will produce F tables for us

```{r anova}
anova(seal_lm)
```

Boom! P values! And they are low. Simple, no?

For more information - t tests, R<sup2>2</sup>, and more, we can use `summary()` - again, a function that is a go-to for nearly any fit model.

```{r summary}
summary(seal_lm)
```

This is a lot of information to drink in - function call, distribution of residuals, coefficient t-tests, and multiple pieces of information about total fit.

We may want to get this information in a more condensed form for use in other contexts - particularly to compare against other models.  For that, there's a wonderful packages called `broom` that sweeps up your model into easy digestable pieces.

First, the coefficient table - let's make it pretty.
```{r broom}
library(broom)

tidy(seal_lm)
```

Nice.

We can also do this for the F-test.

```{r broom_anova}
tidy(anova(seal_lm))
```

If we want to get information about fit, there's `glance()`

```{r glance}
glance(seal_lm)
```

### 4. Visualization

Lovely! Now, how do we visualize the fit and fit prediction error?

In `ggplot2` we can use the smoother, `stat_smooth` in conjunction with `method = "lm"` to get the job done.

```{r show_data}
seal_fit_plot <- ggplot(data=seals) +
  aes(x=age.days, y=length.cm) +
  geom_point() +
  stat_smooth(method="lm")

seal_fit_plot
```

Note - you cannot see the fit interval because our SE is so small with such a large N.

What about prediction intervals? That's a bit harder, and we need to dig into the specific prediction function for `lm`s, `predict()`.  This function is very useful as you can use it to generate new predicted values, and it will also give you error as well. Now, `modelr::add_prediction` will give you values, but no error yet.  YET.

So how does predict work? We begin with a new data frame that has columns matching our predictors in the model. Then add a new column. As the data frame is not the first argument, we can't use pipes here. Booo.

We then cbind the predictors and predicted values - along with the interval we've created. We'll also do some renaming, simply to make it easier to use this new data in our ggplot coming up. Note, there are also arguments to get intervals other than the 95%.

```{r predict_ci}
predFrame <- data.frame(age.days = min(seals$age.days):max(seals$age.days))
predVals <- predict(seal_lm, newdata=predFrame, interval="prediction")

predFrame <- cbind(predFrame, predVals) %>%
  rename(length.cm = fit)

head(predFrame)
```

We can then add this to our plot using the `geom_ribbon` which takes a `ymin` and `ymax` argument to generate a ribbon - like the fit standard error ribbon.

```{r predict_plot}
seal_fit_plot +
  stat_smooth(method="lm", color="blue") +
  geom_ribbon(data=predFrame, mapping=aes(x=age.days, ymin=lwr, ymax=upr),
              fill="grey", alpha=0.6) +
  theme_bw()
```

Now we can better see the prediction 95% interval - and that we do have some points that fall outside of it.

### 5. Faded Examples.

#### A Fat Model
Fist, the relationship between how lean you are and how quickly you lose fat. Implement this to get a sense ot the general workflow for analysis

```{r, eval=FALSE}
library(ggplot2)
fat <- read.csv("./data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv")

#initial visualization to determine if lm is appropriate
fat_plot <- ggplot(data=fat, aes(x=leanness, y=lossrate)) + 
  geom_point()
fat_plot

fat_mod <- lm(lossrate ~ leanness, data=fat)

#assumptions
plot(fat_mod, which=1)
plot(fat_mod, which=2)

#f-tests of model
anova(fat_mod)

#t-tests of parameters
summary(fat_mod)

#plot with line
fat_plot + 
  stat_smooth(method=lm, formula=y~x)
```

#### An Itchy Followup  
For your first faded example, let's look at the relationship between DEET and mosquito bites.

```{r eval=FALSE}
deet <- read.csv("./data/17q24DEETMosquiteBites.csv")

deet_plot <- ggplot(data=___, aes(x=dose, y=bites)) + 
  geom_point()

deet_plot

deet_mod <- lm(bites ~ dose, data=deet)

#assumptions
plot(___, which=1)
plot(___, which=2)

#f-tests of model
anova(___)

#t-tests of parameters
summary(___)

#plot with line
deet_plot + 
  stat_smooth(method=lm, formula=y~x)
```
### 6. Log-Transformation for Nonlinearity

One of the most common reasons for a linear model to not fit is a nonlinearity in the data generating process.  Often in nature we encounter exponential processes with a log-normal error structure. This is common in count data. Now, really, it's often a poisson distribted variable, but, to a first approximation, log-transformation can often help fix what ails your model and give you reasonable estimates for our tests. We'll talk later in the course about why this isn't the best idea, and why you should start with a nonlinear/non-normal model to begin with.

Let's practice the workflow of how we handle this log transformation.

#### Was that relationship linear?  
We might suspect that the relationship was nonlinear. Let's see how a simple log transform works here. Note the modifications to model fitting and `stat_smooth`.

```{r eval=FALSE}

deet_mod_log <- lm(log(bites) ~ dose, data=deet)

#assumptions
plot(___, which=1)
plot(___, which=2)

#f-tests of model
anova(___)

#t-tests of parameters
summary(___)

#plot with line
deet_plot + 
  scale_y_continuous(trans="log") +
  stat_smooth(method=lm, formula=y~x)
```

#### Long-Lived species and Home Ranges
Do longer lived species also have larger home ranges? Let's test this!
```{r eval=FALSE}

zoo <- read.csv("./data/17q02ZooMortality Clubb and Mason 2003 replica.csv")

zoo_plot <- ggplot(data=___, aes(x=mortality, y=homerange)) + 
  ___()

___

zoo_mod <- lm(___, data=___)

#assumptions
plot(___, which=1)
plot(___, which=2)

#f-tests of model
anova(___)

#t-tests of parameters
summary(___)

#plot with line
zoo_plot + 
  stat_smooth(method=___, formula=___)
```


### Nonlinear home-ranges

That definitely wasn't linear. Look at that outlier! Let's log our y and see how things change.

```{r eval=FALSE}

zoo_mod_log <- lm(log(___) ~ ___, ___=___)

#assumptions
___(___)
___(___)

#f-tests of model
___(___)

#t-tests of parameters
___(___)

#plot with line
zoo_plot + 
  scale_y_continuous(trans="___")+
  ___(method=___, formula=___)

```



<!--chapter:end:06_lm.Rmd-->

---
title: "Power for Linear Regression"
author: "Bill 607"
date: "October 18, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
seals <- read.csv("./data/06/17e8ShrinkingSeals Trites 1996.csv")
seal_lm <- lm(length.cm ~ age.days, data=seals)

```

### Power

OK, we can fit and evluate linear regression models, but what about their power?  Let's look at our [seal fit](https://biol607.github.io/lab/06_lm.html) again - maybe we want to know what would have happened with a lower sample size?

Note, I'm going to use a knitr function, `kable` to neaten up the table for markdown. Try it in your own homeworks!

```{r seal_coef}
knitr::kable(tidy(seal_lm))
```

What about the residual SD?

```{r seal_SD}
knitr::kable(glance(seal_lm)[,1:3])
```

All right - there are our target values.  Now, we can change a lof ot things. The effect size (slope), range of values, sigma, and more. But let's try sample size.


###### 1 Make a Table of Parameters, some of which vary

The first step of a power analysis is to setup a data frame with all of the different possibilities that you might want to assess for power. For linear regression, we have the following parameters, all of which might vary:

1. Slope
2. Intercept
3. Residual variance
4. Sample Size
5. Range of X values

To do a power analysis via simulation for a linear regression, we begin by building our simulation data frame with the parameters and information that varies. In this case, sample size.

```{r simPop}
library(dplyr)

set.seed(607)

simPopN <- data.frame(slope = 0.00237, 
                      intercept=115.767,
                      sigma = 5.6805,
                      n=10:100) 
```

Note, if we wanted to vary more than one parameter, we'd first create a data frame where only one parameter varied, then add a second that varried using crossing in tidyr, like so:

```{r cross, eval=FALSE}
library(tidyr)

simPopN <- data.frame(slope = 0.00237, 
                      intercept=115.767,
                      sigma = 2:6) %>%
  crossing(n=10:100) 
```

###### 2 Expand to have a number of rows for each simulated sample with each parameter combination

OK, once we have everything in place, including sample size, we need to expand this out to  have some number of samples for each n. For that, we can use the function in `tidyr` (same library as crossings), `expand()`.

```{r add_samp_size}
library(tidyr)

simPopN <- simPopN %>%
  group_by(slope, intercept, sigma, n) %>%
  expand(reps = 1:n) %>%
  ungroup()
```

###### 3 Expand to create repeated simulated data sets for each combination of parameters

Now, if we want to simulate each of these, say, 100 times, we need to assign unique sim numbers, so for each n and sim number we have a unique data set. We can use crossing to replicate each combination of variables above some number of times. Note - 100 is really low, but it doesn't eat your processor! Use low numbers of simulation for development, then crank them up for final analysis.

```{r increase_sims}
simPopN <- simPopN  %>%
  crossing(sim = 1:100)
```


###### 3 Simulate the data

Great - almost ready to go! Now we just need to add in fitted values. Fortunately, as `rnorm()` works with vectors, we can just use a mutate here. We'll also need to simulate random draws of ages, but that's just another random number.

```{r add_length}
simPopN <- simPopN %>%
  mutate(age.days = runif(n(), 1000, 8500)) %>%
  mutate(length.cm = rnorm(n(), intercept + slope*age.days, sigma))
```

Yatzee! Ready to run!

###### 4 Fit models and extract coefficients

First, we now need to generate a lot of fit models. Dplyr doesn't take too kindly to including fit things, so, we can use two powerful functions here - first, `nest` and `unnest()` allow us to collapse grouped data down into little pieces and re-expand it.

```{r simNest}
fits <- simPopN %>%
    group_by(slope, intercept, sigma, n, sim) %>%
    nest()

fits
```

Second, the `map` function in the purrr library allows us to iterate over different levels or grouped data frames, and perform some function. In this case, we'll fit a model, get it's coefficients using `broom`. This is a new weird set of functions. What's odd about map is that the first argument is a column. But that argument, for the rest of the arguments of the function, is now called `.` and we also use the `~` notation. What `~` does is says that, from this point forward, `.` refers to the first argument given to `map`. 

In essence, what map does is iterate over each element of a list given to it. Once we nest, the data column is now a list, with each element of the list it's own unique data frame. So, map works with lists to apply a function to each element.  The output of the model fitting is another list - now called mod. A list of models. We then iterate, using `map` over that list to generate another list of data frames - this time of coefficients.

```{r simFit, cache=TRUE}
library(purrr)
library(broom)

fits <- fits %>%
    mutate(mod = map(data, ~lm(length.cm ~ age.days, data=.))) %>%
    mutate(coefs = map(mod, ~tidy(.)))

fits  
```

Last, we cleanup - we `unnest`, which takes list-columns from above, and expands them out as into full data frames that get slotted back into our original data frame. Nice trick, no?

We'll also filter for just the slope coefficient.

```{r simUnnest}
fits <- fits %>%
  unnest(coefs) %>%
  ungroup() %>%
  filter(term == "age.days")

fits
```

###### 5 Calculate your Power

Notice that we do indeed have p-values, so we can use these fits to get power for each sample size. We can now do our normal process - in this case grouping by sample size - to get power. And then we can plot the result!

```{r pow}
pow <- fits %>%
    group_by(n) %>%
    summarise(power = 1-sum(p.value>0.05)/n()) %>%
    ungroup() 


qplot(n, power,  data=pow, geom=c("point", "line")) +
  theme_bw(base_size=17) +
  geom_hline(yintercept=0.8, lty=2)
```

###### 6. Examples

Let's take a look at the whole workflow, this time trying a bunch of standard deviation values.

```{r examp_1, eval=FALSE}
##setup parameters
simSD <- data.frame(slope = 0.00237, 
                      intercept=115.767,
                      sigma = seq(2:6, lengthout=5),
                      n=100) %>%
  group_by(slope, intercept, sigma, n) %>%

  #add sample sizes
  expand(reps = 1:n) %>%
  
  #add sims
  crossing(sim = 1:100) %>%
  
  #add fake data
  mutate(age.days = runif(n(), 1000, 8500)) %>%
  mutate(length.cm = rnorm(n(), intercept + slope*age.days, sigma))


##make your fits
fitsSD <- simSD %>%
    group_by(slope, intercept, sigma, n, sim) %>%
    nest()%>%
  
  #mapping
  mutate(mod = map(data, ~lm(length.cm ~ age.days, data=.))) %>%
  mutate(coefs = map(mod, ~tidy(.)))%>%
  
  #unnest and filter
  unnest(coefs) %>%
  ungroup() %>%
  filter(term == "age.days")

##calculate and plot power
powSD <- fitsSD %>% group_by(n) %>%
    summarise(power = 1-sum(p.value>0.05)/n()) %>%
    ungroup() 


qplot(n, power,  data=powSD, geom=c("point", "line")) +
  theme_bw(base_size=17) +
  geom_hline(yintercept=0.8, lty=2)
```

<!--chapter:end:06a_power_analysis.Rmd-->

---
title: "Power for Linear Regression"
author: "Bill 607"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(kableExtra)
seals <- read.csv("./data/17e8ShrinkingSeals Trites 1996.csv")
seal_lm <- lm(length.cm ~ age.days, data=seals)

```

## Power

OK, we can fit and evluate linear regression models, but what about their power?  Let's look at our [seal fit](https://biol607.github.io/lab/06_lm.html) again - maybe we want to know what would have happened with a lower sample size?

Note, I'm going to use a knitr function, `kable` to neaten up the table for markdown. Try it in your own homeworks!

```{r seal_coef}
knitr::kable(tidy(seal_lm), "html") %>% kable_styling(bootstrap_options = "striped")
```

What about the residual SD?

```{r seal_SD}
knitr::kable(glance(seal_lm)[,1:3], "html")%>% kable_styling()
```

All right - there are our target values.  Now, we can change a lof ot things. The effect size (slope), range of values, sigma, and more. But let's try sample size.

Now, the steps of a power analysis as discussed previously are

1. Simulate a data set with relevant parameters.  
2. Fit a model to that simulated dataset and extract a p-value of interest.  
3. Execute steps 1-2 some number of times.  
4. From those p-values, calculate power (1 - proportion of wrong p-values at a prespecified $\alpha$).  

## 1. Simulate a data set with relevant parameters

The first step of a power analysis is to write a function that returns a data frame based on the different possibilities that you might want to assess for power. For linear regression, we have the following parameters, all of which might vary:

1. Slope  
2. Intercept  
3. Residual variance  
4. Sample Size  
5. Range of X values (minimum and maximum)  


That's a lot! But, as we're talking about a straight linear model, it's not a huge deal to write a function to create a data frame.  We can break this down into a skeleton of a function that utilized the parameters above.

```{r skel_data}
make_linear_data <- function(slope, intercept, resid_sd, n, xmin, xmax){
  
  #first, get values of x
  #we will assume a uniform distribution
  
  #second, with those values of x, calculate values of y due only to x
  
  #now, add variation for true simulated values of y
  
  #return a data frame
  
}
```

Broken into steps, it's not so daunting. We want to

1. Draw a bunch of random numbers for x  
2. Do a simple bit of arithmetic to get values of y
3. Add some noise to y
4. Wrap the whole shebang in a data frame and return it.

So, let's fill out that skeleton based on the steps above

```{r make_data}
make_linear_data <- function(slope, intercept, resid_sd, n, xmin, xmax){
  
  #first, get values of x
  #we will assume a uniform distribution
  x <- runif(n, xmin, xmax)
  
  #second, with those values of x, calculate values of y due only to x
  y <- intercept + slope * x
  
  #now, add variation for true simulated values of y
  y <- rnorm(n, mean = y, sd = resid_sd)
  
  #return a data frame
  return(data.frame(x=x, y=y))
}
```

That is our model of how the world works!

## 2. Fit a model and extract p-value of interest

OK, next we need a function that takes that input data frame, and then returns one, and only one, p value. Now, what p value do you want? Well, it depends on your goal. It could be the p value for the slope coefficient. It could be the p value for the intercept. It could be for the f-test for the model. This later is often a good one to examine (it's typically what we mean when we're looking at overall model power), but, this is just to say as we get to fancier models with multiple predictors, there are a lot of different things you could examine for power.  
\
For now, we want a function that will give us a p-value from an F test. Let's write out the skeleton

```{r p_skel}
get_lm_p_value <- function(sim_data){
  #fit a model with y~x from the data
  
  #get the F table
  
  #extract the p-value
  
}
```

Three simple steps. The one that seems tricky is the p-value extraction, but, if we look at the output from `anova()`

```{r anova}
a <- anova(seal_lm)

class(a)
```

We can see it's a data frame! Heck,

```{r anova_tab}
names(a)

a$`Pr(>F)`[1] 
```

gives us that p value!

So....

```{r p_fun}
get_lm_p_value <- function(sim_data){
  #fit a model with y~x from the data
  mod <- lm(y ~ x, data = sim_data)
  
  #get the F table
  f_tab <- anova(mod)
  
  #extract the p-value
  return(f_tab$`Pr(>F)`[1])
}

```

is our function. Let's test it!

```{r fun_test}
#expect small
get_lm_p_value(make_linear_data(slope = 1, intercept = 0, resid_sd = 2,
                                n = 10, xmin = 0, xmax = 10))

#expect large

get_lm_p_value(make_linear_data(slope = 0, intercept = 0, resid_sd = 2,
                                n = 10, xmin = 0, xmax = 10))

```

Bueno!

## 3. Get a LOT of p-values

OK, so, now we just need to write a function that takes ALL of the arguments of `make_linear_data()`, but also the number of simulations! We then just use that as a wrapper to call the relevant use of `replicate()`. I'm just going to launch right in, rather than write a skeleton

```{r get_lotsa_p}
get_many_lm_p_values <- function(nsim, slope, intercept, resid_sd,
                                 n, xmin, xmax){
  
  replicate(nsim, get_lm_p_value(make_linear_data(slope = slope, intercept = intercept, 
                                                  resid_sd = resid_sd, n = n, xmin = xmin,
                                                  xmax = xmax)))

}

#test!
get_many_lm_p_values(10, slope = 1, intercept = 0, resid_sd = 2,
                                n = 10, xmin = 0, xmax = 10)
```

## 4. Get the power!

OK, last function - get the power based on all of those p-values. The only other argument we need is our $\alpha$, and then it's off to the races with a fairly standard power calculation.  Again, no skeleton needed here - it's just get the p-values, figure out how many are greater than our alpha, despite that not being the case, and then return 1-fraction wrong. I'll also set a default $\alpha$ of 0.08. Because why not!

```{r get_lm_power}
get_lm_power <- function(nsim, slope, intercept, resid_sd,
                                 n, xmin, xmax, alpha = 0.08){
  
  #get p values
  p <- get_many_lm_p_values(nsim = nsim, slope = slope, intercept = intercept, 
                       resid_sd = resid_sd, n = n, xmin = xmin,
                       xmax = xmax)
  
  #how many are "wrong"
  wrong_p <- sum(p>alpha)
  
  #return the power
  return(1-wrong_p/nsim)
}  


#test with good power
get_lm_power(nsim = 10, slope = 1, intercept = 0, 
             resid_sd = 2, n = 10, xmin = 0, xmax = 10,
             alpha = 0.08)


#test with low power
get_lm_power(nsim = 10, slope = 0.1, intercept = 0, 
             resid_sd = 2, n = 10, xmin = 0, xmax = 10,
             alpha = 0.08)

```


## 5. A power analysis
Now, let's see the power (heehee) of this fully operational set of functions! Let's say, for example, you wanted to look at that slope of 1, and see how changing both sample size and residual SD change power. OK! Let's start by setting up a data frame using `tidyr::crossing()`.

```{r sim_data}
library(tidyr)

#setup that tibble!
pow_frame <- crossing(nsim = 100,
                      slope = 1,
                      intercept = 0,
                      resid_sd = 1:10,
                      n = 4:10,
                      xmin = 0, 
                      xmax = 10)

pow_frame

```

Great! We can use dplyr to do the rest!

```{r dplyr_power, cache=TRUE}
pow_frame <- pow_frame %>%
  rowwise() %>%
  mutate(power = get_lm_power(nsim = nsim, slope = slope, intercept = intercept, 
                       resid_sd = resid_sd, n = n, xmin = xmin,
                       xmax = xmax)) %>%
  ungroup()
```

And `ggplot2` to show us what we've learned!

```{r plot_power}
ggplot(pow_frame,
       aes(x = resid_sd, y = power, color = factor(n))) +
  geom_line() + 
  geom_point()
```

Neat! A higher sample size means more power, but, more SD means lower power!


## 6. Exercises

Try the following:  
A. Look at how changing the range of values of x affects power and how that interacts with changing the resid_sd
B. Does the intercept choice ever change power?  
C. Using your p-value function as an exaple, write a new function to get the power for the t-test for the intercept. Does sample size and residual SD change power for the intercept? Is there a relationship between power of the F-test and power of t-test for the intercept?

<!--chapter:end:06b_power_analysis.Rmd-->

---
title: "Likelihood!"
author: "Biol 607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(MASS)
```

#### 1. Likelihood of a Single Data Point

At its core, with likelihood we are searching parameter space and attempting to maximize our likelihood of a model given the data. Let's begin with a simple one parameter model of how the world works and a single data point. Sure, it's trivial, but it demonstrates the core properties of likelihood.

Let's assume we are sampling a field of flowers. As this is count data, we have assumed a Poisson distribution. We throw our plot down, and find 10 flowers. What is the MLE of $\lambda$, the mean (and variance) of our POisson distribution?

##### 1.1 The Likelihood Function

As we've assumed our data is Poisson distributed, our **likelihood function* is that of a Poisson distribution. Now, we could write out the formula for the probability of a data point given a Poisson distribution (note L(H|D = p(D|H))), but, hey, these are just the probability density functions of each distribution!  So, in this case, 'dpois(x, lambda)`.

Now, how do we search parameter space? We've got a Poisson distribution and an observation of 10. A reasonable set of values could be from 0 to 20. Why not!

At this point, we can simply vectorize our search for a maximum likelihood.

```{r mle_1}
dpois(x = 10, lambda = 0:20)
```

Well, great. We can squint at that output and find our Maximum Likelihood. But that's a pain in the butt. We can instead use R intelligently to grab estimates for us.

```{r mle_2}
lik_pois_10 <- dpois(x = 10, lambda = 0:20)

#maximum likelihood
max(lik_pois_10)
```

Yay! We have a likelihood. But...what's the estimate? Now, we can use `which` to figure this all out to get the index of the value of the vector of lambdas. But this gets...clunky. Here's an example, just for your delictation.

```{r mle_3}
#Index of the MLE
idx_mle <- which(lik_pois_10 == max(lik_pois_10))

idx_mle

#OK, what's the MLE?
c(0:20)[idx_mle]
```

And, of course, it would have been better practice to have a vector of lambda values to work on, etc., but, woof. This gets ugly and code-y fast. 

##### 1.2 MLE with `dplyr`

Why not keep all of our information in one nice tidy place and use `dplyr` to get easy answers? We can make a data frame with a set of lambda values, calculate the likelihood, and then filter down to the MLE for our answer.

```{r mle_dplyr}
library(dplyr)

pois_10 <- data.frame(lambda_vals = 0:20) %>%
  mutate(likelihood = dpois(x = 10, lambda = lambda_vals)) 

#Get the MLE
pois_10 %>%
  filter(likelihood == max(likelihood))
```

We can even use this to make a nice plot.
```{r mle_plot}
ggplot(pois_10, aes(x=lambda_vals, y=likelihood)) +
  geom_point()
```

##### 1.3 Log-Likelihood
So, notice that we have a *lot* of values close to 0? Lots of room for rounding error and weird peaks. And while this distribtion  was nicely shaped, due to these issues, the log-likelihood often looks much nicer (and leads to good test statistic performance).

How do we add that? We simply add the `log=TRUE` argument to any probability density function.

```{r mle_ll}
pois_10 <- pois_10 %>%
  mutate(log_likelihood = dpois(x = 10, lambda = lambda_vals, log=TRUE))



ggplot(pois_10, aes(x=lambda_vals, y=log_likelihood)) +
  geom_point()
```

Note, I'm not logging after the fact. That's because we'll get different estimates if we log within the function versus after a really really small value is spit out.

##### 1.4 A Real Likelihood "Function"

This is all well and good, but, as we move into more complex models, and more data, we're not going to be able to include a simple `dpois()` statement or otherwise in our mutate. Instead, we need to think of the model as a whole, and write out a likelihood generating **function**. This will have other advantages, too, as we do things like profile, etc.  The general struction of a likelihood function should be something like as follows:

```{r lfun}
my_likelihood_function <- function(observations, predictors, parameters){
  
  #a data generating process that makes
  #fitted values of your response variable
  
  #a likelihood function that compares observations 
  #to predicted values
  
}
```


For example, in the case of our Poisson data, we can write something like so:

```{r pois_lik}

pois_likelihood <- function(y, lambda){
  # Data Generating Process
  y_hat <- lambda
  
  #likelihood function
  sum(dpois(y, lambda = y_hat, log = TRUE))
  
  
}

```

Why include the sum? Well, in this case, it makes the function scalable to look at many data points, as we will see below! If we were looking at the straight likelihood, we'd use `prod()` instead. But, as we've talked about, go with `dat log-likelihood!

#### 2. Likelihood of a Data Set

#### 2.1 Integrating Likelihood over Many Data Points 

Here's the beauty of a data set. The only two differences between the workflow for 1 point and many is first, that you use either `prod()` (for likelihood) or `sum()` (for log-likelihood) to get the total value. Second, as the density functions don't take kindly to a vector of data and a vector of parameters, we'll use `rowwise()` to iterate over rows, but `ungroup()` after for other operations. As we've written our likelihood functoin, as well, we're really in good shape to just nail this.

Let's try this for a random dataset that we generate from a Poisson distribution with a lambda of 10.

Note the tiny tiny changes in the calculation of the likelihood and log-likelihood.

```{r mle_data}
set.seed(607)
pois_data <- rpois(10, lambda=10)

pois_mle <- data.frame(lambda_vals = 0:20) %>%
  rowwise() %>%
  mutate(log_likelihood = pois_likelihood(y = pois_data, lambda = lambda_vals)) %>%
  ungroup()

#Plot the surface
qplot(lambda_vals, log_likelihood, data=pois_mle)

#Get the MLE
pois_mle %>%
  filter(log_likelihood == max(log_likelihood))
```


#### 2.2 Confidence Intervals 

OK, given our assumption that the CI of each parameter is 1.92 away from the Maximum Log-Likelhood. Again, pretty straightfoward to get with filtering.

```{r mle_ci}
pois_mle %>%
  filter(log_likelihood > (max(log_likelihood) - 1.92) )
```

So, our CI is between 8 and 10.

#### 2.3 Examples

1. You have run 5 trials of flipping 20 coins. The number of heads in each trial is: 11, 10,  8,  9,  7. What's the maximum likelihood estimate of the probability getting heads? Use `dbinom()` here for the binomial distribution.


#### 3. Likelihood with Two Parameters

##### 3.1 Two parameters and crossing!

The great thing about multiple parameters is that, heck, it's no different than one parameter. Let's use the seals example from last week and look at estimating the mean and SD of the seal age distribution. To start with, what's the range of values we should choose to test? Let's look at the data!

```{r seals}
seals <- read.csv("data/17e8ShrinkingSeals Trites 1996.csv")

hist(seals$age.days)
```

Eyeballing this, I'd say the mean is between 3720 and 3740 and the SD is...oh, between 1280 and 1310.  
  

OK, we're going to need a likelihood function for this - one that takes a mean and a SD. Let's do this!

```{r norm_lik}
norm_likelihood <- function(obs, mean_est, sd_est){
  
  #data generating process
  est <- mean_est
  
  #log likelihood
  sum(dnorm(obs, mean = est, sd = sd_est, log = TRUE))
  
}
```



Now, we can use `crossing` in `tidyr` to generate a set of parameters to test, and then literally nothing is different from the workflow before. Let's just look at the Log Likelihood.

```{r seal_ll, cache=TRUE}
seal_dist <- crossing(m = seq(3720, 3740, by = 0.1), 
                      s=seq(1280, 1310, by = 0.1)) %>%
  rowwise() %>%
  mutate(log_lik = norm_likelihood(obs = seals$age.days, mean_est = m, sd_est = s)) %>% 
  ungroup()
```

Notice that took a bit That's because we searched `r nrow(seal_dist)` combinations of values. And we didn't even do that with high precision!  And I gave you a really narrow band of possible values! If anything can convince you that we need an algorithmic solution rather than something brute force - I hope this does!

What's the MLE?

```{r seal_mle}
seal_dist %>%
  filter(log_lik == max(log_lik))
```

And can we plot a super-sexy contour plot of the surface?

```{r seal_contour}
ggplot(seal_dist, aes(x=m, y=s, z=log_lik)) +
  geom_contour(aes(color = ..level..))
```

Note that odd color thing enables you to see the contours. Yeah, I don't like it, either.

##### 3.2 Profile Likelihoods for CIs

So, how do we get profile likelihoods from this mess? 

Simply put, for each variable we want the profile of, we look at the MLE estimate at each value of the other parameter. So, group by the **other** parameter and filter out the MLE.  So for the mean - 

```{r mean_prof}
seal_sd_profile <- seal_dist %>%
  group_by(s) %>%
  filter(log_lik == max(log_lik)) %>%
  ungroup()

qplot(s, log_lik, data=seal_sd_profile, geom = "line")
```

OK, nice profile.

So, what about the CI? Unfortunately, even after filtering, we have a lot of values. We want to just look at the first and last. Weirdly, `filter` lets us use `row_number` as something to work on. So, let's take advantage of that here!

##See the CI
```{r prof_ci}
seal_sd_profile %>%
  filter(log_lik > max(log_lik) - 1.92) %>%
    filter(row_number()==1 | row_number()==n())
```

So, now we see the CI of our SD is from 1280 to 1310 - at least from this grid.

##### 3.3 Exercises

1. Using the seal data, get the mean and SD of the seal lengths. Actually use `mean` and `sd` to derive reasonable parameter ranges likelihood surface so you don't take forever.  
\
2. What is the profile estimate of the CIs of the mean?


#### 4. Linear Regression and Likelihood: `glm`

So, you want to fit a model with multiple parameters using Likelihood? We've seen that brute force is not the way. We need algorithms to search parameter space. R has multiple optimizer functions that seek to fund the minimum value of a function (likelihood or otherwise). Many require you to write your own likelihood function in order to fit, and this are **incredibly** flexible. But, for many workaday needs, we will use the function `glm()` which stands for Generalized Linear Model (more on that at the end of the course) and uses the iteratively reweighted least squares algorithm.

##### 4.1 Seal Regression with `glm`

The great thing about `glm()` is that it is incredibly similar to `lm()` only - now you have to specify what distribution you're drawing from, in order to get the likelihood correct. You should also specify the shape of the curve, as many different curves are possible now.  Let's take a look.

```{r bbmle_seals}
seals <- read.csv("./data/17e8ShrinkingSeals Trites 1996.csv")

seal_mle <- glm(length.cm ~ age.days,
                family = gaussian(link = "identity"),
                data = seals)
  
```

This looks pretty similar! Only, notice now we have this `family` argument for a distribution. We're going to go all `gaussian` all the time for now. We also specify what's called a `link` function. Basically, how do we translate linear predictors into the shape of a curve. For now, we can just use `"identity"` which means a 1:1 translation between our linear predictors and the shape of the curve.

##### 4.2 Testing Assumptions

So now that we have this fit object, how do we test our assumptions. Let's remember what they are:

1. No relationship between fitted and residual values
2. Residuals follow normal distribution
3. The surface is peaked approximately symetrically

Let's break these down one by one. There is no nice `plot()` method here, so we'll have to do it by hand.

First, fitted and residual. We can extract both of these from the model object and plot them using `predict()` and `residuals()`. Note, using `predict()` with no new data merely get the fitted values.

```{r fit_res}
fitted <- predict(seal_mle)
res <- residuals(seal_mle)

qplot(fitted, res)
```

If we wanted to add a spline, we could have used `stat_smooth()` with no method argument.

Second, qq! Well, we have our residuals, so we can use `qqnorm()` and ``qqline()` as we did with t-tests. Or plot a histogram.

```{r qq}

qqnorm(res)
qqline(res)

hist(res)
```

Again, looks good!


##### 4.3 The Profile

Now, what about looking at the surface? We don't have a nice way of visualizing a 4-D surface, so, instead we look at the profile likelihoods of each parameter. For that, we use the `profile` function which gets a profile of any function that can be accessed with the `logLik()` function. Note, this might take some time.

```{r profile_mle2}
library(MASS)
plot(profile(seal_mle))
```

Well that's not what we expect. What is going on here? In essence, `profile.glm()` takes the profile of a parameter, subtracts the optimized deviance (thus centering it at 0), and takes a signed square root. This produces a straight line from which it's much easier to see deviations. Which is great in terms of diagnostics!

It's not great if you want to see CIs and such. For that, we can use the `profileModel` library. Let's take a look.

```{r profileModel}
library(profileModel)

prof <- profileModel(seal_mle,
                     objective = "ordinaryDeviance")
```

Here we see that we can request how we want the deviance transformed. In this case, not at all. Let's see how this looks.

```{r plot_prof}
plot(prof)
```

Much nicer. To see the sampling grid used:

```{r prof_grid}
plot(prof, print.grid.points = TRUE)
```

We can also look at the CIs here naturally.

```{r prof_ci_profmod}

prof <- profileModel(seal_mle,
                     objective = "ordinaryDeviance",
                     quantile = qchisq(0.95, 1))

plot(prof)
```


##### 4.4 Coefficients and CIs

OK, now that we've done this and feel comfortable with our fit, what are our coefficients?

```{r mle2_summary}
summary(seal_mle)

#or

broom::tidy(seal_mle)
```

First, let's see how that compares to an `lm()` fit.

```{r lm}
broom::tidy(lm(length.cm ~ age.days, data=seals))
```

Not bad. Pretty close. Note that `glm()` uses something called a `dispersion parameter` which is a general name for additional factors related to spread in the data. For a Gaussian model, it's just the residual variance.

Note, the SEs are based on the assumption that the profiles are well behaved. If we want to get into more detail and change that assumption, we use `confint()`.

```{r confint_mle}
#based off of profile
confint(seal_mle)
```


##### 4.5 Model Comparison
So, how do we compare models? Unfortunately, there's not an automated way. This is likelihood, so you have to specify your null or alternate models. You have to think. In this case, a null model would only have an intercept - a mean - term.

```{r null_mod}

seal_mle_null <- glm(length.cm ~ 1,
                family = gaussian(link = "identity"),
                data = seals)
```

We can now compare these models using a LRT. Note that we use the `anova()` function, and now have to specify what kind of test we want

```{r mod_compare}
anova(seal_mle_null, seal_mle, test = "LRT")
```

Yes! They are different!  

##### 4.6 Plotting a fit glm

To plot, we can use the same strategy as before, only, now we need to specify two things to `stat_smooth()`. First, our method is now `glm`, but second, we need to tell it about the family and link we're using. `stat_smooth` takes a list of arguments that are passed to the method. So, let's try it out.

```{r seal_plot_glm}
ggplot(seals, 
       mapping = aes(x = age.days, y = length.cm)) +
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = gaussian(link="identity")))
```

Works!

##### 4.7 Faded Examples
Let's revisit the examples from our lm lab the other day.

#### A Fat Model
Fist, the relationship between how lean you are and how quickly you lose fat. Implement this to get a sense ot the general workflow for analysis

```{r, eval=FALSE}
fat <- read.csv("../data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv")

#initial visualization to determine if lm is appropriate
fat_plot <- ggplot(data=fat, aes(x=leanness, y=lossrate)) + 
  geom_point()
fat_plot

#fit that model
fat_mod <- glm(lossrate ~ leanness, 
               family = gaussian(link = "identity"), 
               data=fat)
  
  
#assumptions
fat_fit <- predict(fat_mod)
fat_res <- residuals(fat_mod)

qplot(fat_fit, fat_res)

qqnorm(fat_res)
qqline(fat_res)

plot(profile(fat_mod))

#LRT test of model
fat_mod_null <- glm(lossrate ~ 1, 
               family = gaussian(link = "identity"), 
               data=fat)
  
anova(fat_mod_null, fat_mod, test = "LRT")

#t-tests of parameters
summary(fat_mod)
```

#### An Itchy Followup  
For your first faded example, let's look at the relationship between DEET and mosquito bites.

```{r eval=FALSE}
deet <- read.csv("../data/17q24DEETMosquiteBites.csv")

deet_plot <- ggplot(data=___, aes(x=dose, y=bites)) + 
  geom_point()

deet_plot

#fit that model
deet_mod <-  glm(bites ~ dose, 
                   family = gaussian(link = "______")
                   data=___)
  

#assumptions
deet_fit <- predict(____)
deet_res <- residuals(____)

qplot(deet_fit, deet_res)

qqnorm(_____)
qqline(_____)

plot(profile(____))

#f-tests of model
deet_mod_null <- glm(bites ~ ___, 
                   family = gaussian(link = "______")
                   data=___)

anova(___, _____, test = "LRT")

#t-tests of parameters
summary(___)

```

#### Long-Lived species and Home Ranges
Do longer lived species also have larger home ranges? Let's test this!
```{r eval=FALSE}

zoo <- read.csv("../data/17q02ZooMortality Clubb and Mason 2003 replica.csv")

zoo_plot <- ggplot(data=___, aes(x=mortality, y=homerange)) + 
  ___()

#fit that model
zoo_mod <- glm(___~___,
              family = ____(____ = _____),
                data = ____)

#assumptions
zoo_fit <- ____(____)
zoo_res <- ____(____)

qplot(____, ____)

qqnorm(____)
qqline(____)

plot(_____(____))

#LRT-tests of model
zoo_mod_null <- <- glm(___~___,
              family = ____(____ = _____),
                data = ____)

anova(___, _____, ________)

#z-tests of parameters
_____(___)
```


For this last one, I want you to also also try something. Try changing `link = "identity"` to `link = "log"`. What do you see? How does this compare to the log transformed model? Try also to `ggplot` the results with the two different links!

<!--chapter:end:07_likelihood.Rmd-->

---
title: "Linear Models with Bayes"
author: "Biol 607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

For this lab, see the etherpad at  https://etherpad.wikimedia.org/p/607-bayes 

## 1. Fitting a Single Parameter Model with Bayes

Last week, we fit parameters with likelihood. To give you a hands-on feel for Bayesian data analysis let's do the same thing with Bayes. We're going to fit a single parameter - lambda from a poisson distribution - with Bayes instead of likelihood (although as you'll see likelihood is a part of it!).

Let's say you start with this data:

```{r pois_data}
pois_data <- c(6, 6, 13, 7, 11, 10, 9, 7, 7, 12)
```

Now, how do you get the Bayesian estimate and credible intervals for this?

Now, in Bayesian data analysis, according to Bayes theorem

$$p(\lambda | data) = \frac{p(data | \lambda)p(\lambda)}{p(data)}$$

To operationalize this, we can see three things we need to either provide or calculate

1) The likelihood of each choice of lambda.

2) The prior probability of each choice of lambda.

3) The marginal distribution - i.e., the sum of the product of p(D|H)p(H), as this is a discrete distribution.

What's great is that we can fold 1 and 2 into a function! We already wrote a log likelihood function for this very scenario last week, and now all we need to do is include a prior. In essence, any function for Baysian grid sampling includes  
1) A Data Generating Process  
2) A Set of Priors  
3) A Likelihood  

Then we either multiple the likelihood and prior - or sum the log likelihood and log prior.

```{r bfun}
bayes_pois <- function(y, lambda_est){
  #our DGP
  lambda <- lambda_est
  
  #Our Prior
  prior <- dunif(lambda_est, 6, 12)
  
  #Our Likelihood
  lik <-prod(dpois(y, lambda))
  
  #The numerator of our posterior
  return(lik * prior)
  
  
}
```

Great, so, what range of lambdas should we test? Well, since our range of data is from 6-12, we can reasonably assume lambda must be between 6 and 12. Could be outside of that, but it's a reasonable suggestion. We can then sample using our function and dplyr.

```{r lik, warning=FALSE}
library(dplyr)
library(tidyr)

bayes_analysis <- data.frame(lambda = 6:13) %>%
  rowwise() %>%
  mutate(posterior_numerator = bayes_pois(pois_data, lambda)) %>%
  ungroup()
```


OK, now let's get our posterior! To do that, we just divide the numerator of our posterior by the marginal probability - which is just the sum of that numerator!

```{r posterior}
bayes_analysis <- bayes_analysis %>%
  mutate(posterior = posterior_numerator/sum(posterior_numerator))

```

Let's take a look!

```{r plot_post}
library(ggplot2)
ggplot(data=bayes_analysis, mapping = aes(x = lambda, y = posterior)) +
  geom_bar(stat="identity")
```

Or we can look at a table
```{r tab, echo=FALSE}
knitr::kable(bayes_analysis)
```

From this table, we can see that the 90%CI is wide - ranges from 6-13. That's because we have a weak prior and not much data. Now, what if we'd had a stronger prior? Maybe a normal distribution centered on 10 with a SD of 1

```{r prior2}

bayes_pois_strong_prior <- function(y, lambda_est){
  #our DGP
  lambda <- lambda_est
  
  #Our Prior
  prior <- dnorm(lambda_est, 10,1)
  
  #Our Likelihood
  lik <-prod(dpois(y, lambda))
  
  #The numerator of our posterior
  return(lik * prior)
  
  
}

bayes_analysis <- bayes_analysis %>%
  rowwise() %>%
  mutate(posterior_numerator_strong = bayes_pois_strong_prior(pois_data, lambda)) %>%
  ungroup() %>%
  mutate( posterior_strong = posterior_numerator_strong / sum(posterior_numerator_strong))

ggplot(data=bayes_analysis) +
  geom_area(alpha=0.5, fill="red", mapping=aes(x=lambda, y=posterior)) +
  geom_area(alpha=0.5, fill="blue", mapping=aes(x=lambda, y=posterior_strong)) +
  ggtitle("Red = Flat Priot, Blue = Informative Prior")

knitr::kable(bayes_analysis %>% select(lambda, posterior, posterior_strong))

```

A noticable difference. The 90% CI is now from 8-11, and the 80% is even narrower.

What's super near about this is that you can simulate samples from your posterior density. Say, draw 100 sampled lambdas, then, for each lambda, draw a sample of 10 random numbers (as in our initial distribution). We can then see how these posterior predictive distributions compare to the original.

```{r ppd}
nsims <- 10

posterior_sims  <- data.frame(sampled_lambda = sample(6:13, size = nsims, 
                                     replace=TRUE, 
                                     prob = bayes_analysis$posterior), 
                              sim = 1:nsims) %>%
  group_by(sim) %>%
  nest() %>%
  mutate(predicted_values = purrr::map(data, ~rpois(10, .$sampled_lambda))) %>%
  unnest(predicted_values) %>%
  ungroup()

ggplot() +
    geom_density(data=posterior_sims, mapping=aes(group = sim, x=predicted_values), color="black") +
  geom_density(mapping=aes(x=pois_data), fill="lightblue", color = "red", alpha = 0.7) +
  theme_bw()
```

## 2. Fitting a Line Using Bayesian Techniques

Today we're going to go through fitting and evaluating a linear regression fit using Bayesian techiniques. For that, we're going to use the `rstanarm` library which uses [STAN](http://mc-stan.org) to perform the MCMC simulations.

We'll use the seal linear regression as an example.

```{r load_seals}
library(brms)

seals <- read.csv("data/17e8ShrinkingSeals Trites 1996.csv")

head(seals)
```

Note that when you loaded `brms` it gave you some warnings bout wanting to use more cores. This is great - MCMC is one of those places where using all of your computer's cores (most these days have at least two) can **really** speed things along. And the parallelization is done for you!

```{r cores}
options(mc.cores = parallel::detectCores())
```

The basic steps of fitting a linear regression using Bayesian techniques (presuming you've already settled on a linear data generating process and a normal error generating process) are as follows.

**1. Fit the model  
2. Assess convergence of chains  
3. Evaluate posterior distributions  
4. Check for model misspecification (fit v. residual, qq plot)  
5. Evaluate simulated residual distributions  
6. Evaluate simulated fit versus observed values  
7. Compare posterior predictive simulations to observed values  
8. Visualize fit and uncertainty  
**

### 2.1 Defining Your Model

To begin, let's define our model. By default, though, it sets relatively flat priors for slopes and an intercept prior ~ N(0,10)). No intercept is set for the SD, as the SD results from your choice of slope and intercept.

To specify that we're using a gaussian error, simply set the `family` argument to `gaussian()` - evertyhing else is handled for you.

```{r fit_seals, cache=TRUE}
set.seed(607)

seal_lm_bayes <- brm(length.cm ~ age.days,
                         data = seals,
                         family=gaussian(), file = "./brms_fits/seal_lm_bayes")
```

Note the output - you can see you're doing something! And you get a sense of speed. And you can see the multiple cores going!


### 2.2 Assessing MCMC Diagnotics

Before we diagnose whether we have a good model or not, we want to make sure that our MCMC chains have converged properly so that we can feel confident in our ability to assess the model. Now, `rstanarm` usually runs until you have reached convergence, as the models it works with are pretty straightforward. But, good to check.

We're going to check a few diagnostics:

| **Diagnostic** | **Fix** |
|---------------------------|-----------------------------|
| Did your chains converge? | More iterations, check model|
| Are your posteriors well-behaved? | Longer burning, more interations, check model & priors |
| Are samples in your chains uncorrelated? | Change your thinning interval |

So, first, did your model converge? The easiest way to see this is to plot the trace of your four chains. We can use `plot' to see both how well our model converged and the distribution of each parameter.

```{r converge}
plot(seal_lm_bayes)
```

Note, there's a `par` argument, so, you can look at just one chain if you want.

```{r one_par}
plot(seal_lm_bayes, par = "b_Intercept")
```

This looks pretty good, and those posteriors look pretty normal!

You can also look at just the chains by turning our fit model into a posterior data frame.

```{r post_df}
seal_posterior <- posterior_samples(seal_lm_bayes, add_chain = T)

head(seal_posterior)
```

Then, you can make whatever plot you want! The `bayesplot` function `mcmc_trace()` works great here.

```{r plot_df}
library(bayesplot)
mcmc_trace(seal_posterior)+
  scale_color_manual(values = c("red", "blue", "orange", "yellow"))
```

You can assess convergence by examining the Rhat values in `rhat(seal_lm_bayes)`. These values are something called the Gelman-Rubin statistic, and it should be at or very close to 1. You can also do fun things to visualize them with bayesplot

```{r rhat}
rhat(seal_lm_bayes)

mcmc_rhat(rhat(seal_lm_bayes))
```

Note that the function itself tells you what the intervals are. You can set them yourself, for example for the 67% interval and showing the full width of the distribution using

Last, we want to look at autocorrelation within our chains. We want values in the thinned chain to be uncorelated with each other. So, while they'd have an autocorrelation that might be high at a distance of 1, this should drop to near zero very quickly. If not, we need a different thinning interval.

```{r autcorrelation}
mcmc_acf(seal_posterior)
```

What do you do if you have funky errors?
1) Check your model for errors/bad assumptions. Just visualize the data!
2) Check your priors to see if they are of and try something different. Maybe a uniform prior was a bad choice, and a flat normal is a better idea. (I have made this error)
3) Try different algorithms found in `?stan_glm` but make sure you read the documentation to know what you are doing.
4) Dig deeper into the docs and muck with some of the guts of how the algoright works. Common fixes include the following, but there are so many more
      - `iter` to up from 2000 to more iterations
      - `warmpup` to change the burnin period
      - `thin` to step up the thinning
      - `adapt_delta` to change the acceptance probability of your MCMC chains.
      
Also, as a final note, using `shinystan` you can create a nice interactive environment to do posterior checks.

### 2.3 Assessing Model Diagnostics

So, your MCMC is ok. What now? Well, we have our usual suite of model diagnostics - but they're just a wee bit different given that we have simulated chains we're working with.  So, here's what we're going to look at:

|      **Diagnostic**       |           **Probable Error**           |
|---------------------------|-----------------------------|
| Fitted v. residual        | Check linearity, error  |
| QQ Plot       | Check error distribution  |
| Simulated residual histograms       | Check error distribution  |
| Simulated Fit v. Observed       | Check linearity  |
| Reproduction of Sample Properties       | Respecify Model  |
| Outlier Analysis      | Re-screen data  |

#### 2.3.1 Classical Point Model Diagnostics

Let's start by looking at point diagnostics. Note, because we're dealing with MCMC output here, we have to average over our posterior to get the responses we want. So, we can start by looking at the fitted versus residual values.

```{r fitres}
seal_fit <- fitted(seal_lm_bayes) %>% as.data.frame()
res <- residuals(seal_lm_bayes) %>% as.data.frame()

plot(seal_fit$Estimate, res$Estimate)
```


Good. Now the QQ plot.

```{r qq}
qqnorm(res$Estimate)
qqline(res$Estimate)
```


This looks excellent

#### 2.3.1 Model Diagnostics with Simulations

This is well and good, but, we know that we're working with chains here. So, while these plots might be good for point estimates, we reall want to look at either replicate simiulated outputs or the same plots as above, but with mean values from MCMC draws of our posterior predictions. For example, here's a qq plot with simulated residuals from 4000 simulations per each data point. Note that the only funky thing here, besides generating the prediction simulation matrix via `posterior_predict`, is the transposition of matrices we have to do to get things to align for the subtraction

```{r qq_sim, cache=TRUE}
pred_vals <- posterior_predict(seal_lm_bayes)
resid_vals <- t( t(pred_vals) - seals$length.cm)

qqnorm(colMeans(resid_vals))
qqline(colMeans(resid_vals))
```

Not too different. And, hey, now we can look at the average of the posterior prediction against the residual.

```{r qq_fit_res_sim}
plot(colMeans(pred_vals), colMeans(resid_vals))
```

`brms` has a few of these types of diagnostics builtin. They look at simulated draws, as those give you a better idea of how your model is behaving. They use the function`pp_check()` whose options you can see from `?"PPC-overview"`

For residuals, the first is to look at the histogram of residuals from several simulated runs.

```{r res_hist, cache=TRUE}
pp_check(seal_lm_bayes, "error_hist", bins=10)
```

We can also look at the relationship between average fitted and observed values

```{r fit_scatter, cache=TRUE}
pp_check(seal_lm_bayes, "scatter")
```

Note, you can add the `nreps` argument to look at individual simulation runs. But, in general there should be a nice cloud around this line. And it should follow a roughly 1:1 relationship.

Another check is to see if we have reproduced the properties of our observed response variable. The "test" check gets at that. You can one or more properties of the response variable. The default is to look at the mean, but it's often useful to look at both the mean and SD

```{r properties, cache=TRUE}
pp_check(seal_lm_bayes, "stat")
pp_check(seal_lm_bayes, "stat", stat = "sd")
pp_check(seal_lm_bayes, "stat_2d", stat=c("mean", "sd"))
```

Last, we can look at whether simulations of the posterior lineup with the actual distribution of the posterior. If there are wild swings, we know there is a problem.

```{r check, cache=TRUE}
pp_check(seal_lm_bayes, "dens")

pp_check(seal_lm_bayes, "dens_overlay")

```

As with all diagnostics, failures indicate a need to consider respecifying a model and/or poor assumptions in the error generating process.

Last, we can look for outliers using `loo` - leave one out. We use a particular form of this function as there are a few.

```{r loo, cache=TRUE, eval=FALSE}
plot(loo(seal_lm_bayes), label_points = TRUE, cex=1.1)
```

Points with a score >0.5 are a bit worrisoome, and scores >1.0 have a large deal of leverage and should be examined.

### 2.4 Assessing Coefficients and Credible Intervals

OK, *phew* we have gotten through the mdoel checking stage. Now, what does our model tell us?

```{r mod_summary}
#adding extra digits as some of these are quite small
summary(seal_lm_bayes, digits=5)
```

Wow. No p-values, no nothing. We have, in the first block, the mean, SD (assuming a gaussian distribution of posteriors), and some of the quantiles of the parameters. 


We can also visualize coefficients. Here we'll use `tidybayes` which has some really cool plotting tools.

```{r coef_plot}
#devtools::install_github("mjskay/tidybayes")
library(tidybayes)


seal_lm_bayes %>%
  gather_draws(b_Intercept, b_age.days, sigma) %>%
  ggplot(aes(x = .value, y = .variable)) +
  geom_halfeyeh( .width = c(0.8, 0.95)) +
  ylab("") +
  ggtitle("Posterior Medians with 80% and 95% Credible Intervals")
```


Woof - things are so different, we can't get a good look in there. Let's try looking at just one parameter.

```{r coef_plot_slope}
#devtools::install_github("mjskay/tidybayes")
library(tidybayes)


seal_lm_bayes %>%
  gather_draws(b_age.days) %>%
  ggplot(aes(x = .value, y = .variable)) +
  geom_halfeyeh( .width = c(0.8, 0.95)) +
  ylab("") +
  ggtitle("Posterior Medians with 80% and 95% Credible Intervals")
```

That's an interesting look! We might also want to see how things differ by chain. To do that (and we can incorporate this for looking at multiple parameters), let's try `ggridges`.

```{r ggridges}
library(ggridges)

seal_lm_bayes %>%
  gather_draws(b_age.days) %>%
  ggplot(aes(x = .value, y = .chain, fill = factor(.chain))) +
  geom_density_ridges(alpha = 0.5)
```

That looks pretty good, and we can see how similar our posteriors are to one another. FYI, I think `ggridges` and `geom_halfeyeh` are two pretty awesome ways of looking at Bayesian Posteriors. But that just might be me.

If we want to know more about the posteriors, we have to begin to explore the probability densities from the chains themselves. To get the chains, we convert the object into a data frame.

```{r as_df}
seal_chains <- as.data.frame(seal_lm_bayes)

head(seal_chains)
```

We can now do really interesting things, like, say, as what is the weight of the age coefficient that is less than 0? To get this, we need to know the number of entries in the chain that are <0, and then divide that total by the total length of the chains.

```{r}
sum(seal_chains$b_age.days<0)/nrow(seal_chains)
```

Oh, that's 0.  Let's test something more interesting. How much of the PPD of the slope is between  0.00229 and  0.00235?


```{r}
sum(seal_chains$b_age.days>0.00229 & 
      seal_chains$b_age.days<0.00235) / nrow(seal_chains)
```

28.9% - nice chunk.  We can also look at some other properties of that chain:
```{r chain_properties}
mean(seal_chains$b_age.days)
median(seal_chains$b_age.days)
```

To get the Highest Posteriod Density Credible Intervals (often called the HPD intervals)

```{r hpd}
posterior_interval(seal_lm_bayes)
```

Yeah, zero was never in the picture.

### 2.5 Visualizing Model Fit and Uncertainty

This is all well and good, but, how does our model fit? Cn we actually see how well the model fits the data, and how well the data generating process fits the data relative to the overall uncertainty.

#### 2.5.1 Basic Visualization
To visualize, we have coefficient estimates. We can use good olde `ggplot2` along with the `geom_abline()` function to overlay a fit onto our data.

```{r show_fit}
library(ggplot2)

#the data
seal_plot <- ggplot(data = seals, 
                    mapping=aes(x = age.days, y = length.cm)) +
  geom_point(size=2)

#add the fit
seal_plot + 
  geom_abline(intercept = fixef(seal_lm_bayes)[1], slope = fixef(seal_lm_bayes)[2],
              color="red")
```

#### 2.5.2 Credible Limits of Fit

This is great, but what if we want to see the CI of the fit? Rather than use an area plot, we can actually use the output of the chains to visualize uncertainty. `seal_chains` contains simulated slopes and intercepts. Let's use that.

```{r show_uncertainty}
seal_plot +
  geom_abline(intercept = seal_chains[,1], slope = seal_chains[,2], color="grey", alpha=0.6) +
  geom_abline(intercept = fixef(seal_lm_bayes)[1], slope = fixef(seal_lm_bayes)[2], color="red")
```

We can see the tightness of the fit, and that we have high confidence in the output of our model.

#### 2.5.3 Prediction Uncertainty
So how to we visualize uncertainty given our large SD of our fit? We can add additional simulated values from `posterior_predict` at upper and lower values of our x-axis, and put lines through them.

```{r prediction_intervals}
seal_predict <- posterior_predict(seal_lm_bayes, newdata=data.frame(age.days=c(1000, 8500)))
```

This produces a 4000 x 2 matrix, each row is one simulation, each column is for one of the new values.
```{r full_uncertainty}

seal_predict <- as.data.frame(seal_predict)
seal_predict$x <- 1000
seal_predict$xend <- 8500

#The full viz
seal_plot +
  geom_segment(data = seal_predict, 
               mapping=aes(x=x, xend=xend, y=V1, yend=V2), 
               color="lightblue", alpha=0.1)+
  geom_abline(intercept = seal_chains[,1], slope = seal_chains[,2], color="darkgrey", alpha=0.6) +
  geom_abline(intercept = fixef(seal_lm_bayes)[1], slope = fixef(seal_lm_bayes)[2], color="red")
```

We can now see how much of the range of the data is specified by both our data and error generating process. There's still some data that falls outside of the range, although that's not surprising given our large sample size.

### 2.6 Futzing With Priors

What if you wanted to try different priors, and assess the influence of your choice? First, let's see how our current prior relates to our posterior.

```{r priors, cache=TRUE}
prior_summary(seal_lm_bayes)
```

Eh, not much, most likely. Let' see if we had a different prior on the slope. Maybe a strong prior of a slope of 110. A very strong prior. Our `brms` uses lists to create priors

```{r strong_prior, cache=TRUE}
seal_lm_bayes_prior <- brm(length.cm ~ age.days,
                         data = seals,
                         family=gaussian(),
                   prior = c(prior(normal(110, 5), class = Intercept),
                             prior(normal(1, 1), class = b),
                             prior(uniform(3, 10), class = sigma)),
                   file = "brms_fits/seal_lm_bayes_prior")
```

Note that this was faster due to the tighter priors.

## 3. Faded Examples of Linear Models


#### A Fat Model
Fist, the relationship between how lean you are and how quickly you lose fat. Implement this to get a sense ot the general workflow for analysis

```{r, eval=FALSE}
fat <- read.csv("./data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv")

#initial visualization to determine if lm is appropriate
fat_plot <- ggplot(data=fat, aes(x=leanness, y=lossrate)) + 
  geom_point()
fat_plot

#fit the model!
fat_mod <- brm(lossrate ~ leanness,
                data = fat, 
                family=gaussian())
  
# Inspect chains and posteriors
plot(fat_mod)

#Inspect rhat
mcmc_rhat(rhat(fat_mod))

#Inspect Autocorrelation
mcmc_acf(as.data.frame(fat_mod))


#model assumptions
fat_fit <- predict(fat_mod) %>% as_tibble
fat_res <- residuals(fat_mod)%>% as_tibble

qplot(fat_res$Estimate, fat_fit$Estimate)

#fit
pp_check(fat_mod, type="scatter")

#normality
qqnorm(fat_res$Estimate)
qqline(fat_res$Estimate)
pp_check(fat_mod, type="error_hist", bins=8)

##match to posterior
pp_check(fat_mod, type="stat_2d", test=c("mean", "sd"))
pp_check(fat_mod)

#coefficients
summary(fat_mod, digits=5)

#confidence intervals
posterior_interval(fat_mod)

#visualize
fat_chains <- as.data.frame(fat_mod)

fat_plot +
  geom_abline(intercept=fat_chains[,1], slope = fat_chains[,2], alpha=0.1, color="lightgrey") +
  geom_abline(intercept=fixef(fat_mod)[1], slope = fixef(fat_mod)[2], color="red") +
  geom_point()
```

#### An Itchy Followup  
For your first faded example, let's look at the relationship between DEET and mosquito bites.

```{r eval=FALSE}
deet <- read.csv("./data/17q24DEETMosquiteBites.csv")

deet_plot <- ggplot(data=___, aes(x=dose, y=bites)) + 
  geom_point()

deet_plot

#fit the model!
deet_mod <- brm(___ ~ dose,
                data = ____, 
                family=gaussian())
# Inspect chains and posteriors
plot(deet_mod)

#Inspect rhat
mcmc_rhat(rhat(deet_mod))

#Inspect Autocorrelation
mcmc_acf(as.data.frame(deet_mod))


#model assumptions
deet_fit <- predict(____) %>% as_tibble
deet_res <- residuals(____)%>% as_tibble

qplot(____$Estimate, ____$Estimate)

#fit
pp_check(____, type="____")

#normality
qqnorm(____$Estimate)
qqline(____$Estimate)
pp_check(____, type="error_hist", bins=8)

##match to posterior
pp_check(____, type="stat_2d", test=c("mean", "sd"))
pp_check(____)

#coefficients
summary(___, digits=5)

#confidence intervals
posterior_interval(___)

#visualize
deet_chains <- as.data.frame(___)

deet_plot +
  geom_abline(intercept=deet_chains[,1], slope = deet_chains[,2], alpha=0.1, color="lightgrey") +
  geom_abline(intercept=fixef(___)[1], slope = fixef(___)[2], color="red") +
  geom_point()

```

#### Long-Lived species and Home Ranges
Do longer lived species also have larger home ranges? Let's test this!
```{r eval=FALSE}

zoo <- read.csv("../data/17q02ZooMortality Clubb and Mason 2003 replica.csv")

zoo_plot <- ggplot(data=___, aes(x=mortality, y=homerange)) + 
  ___()

___


#fit the model!
zoo_mod <- brm(___ ~ ___,
                data = ____, 
                family=___,
                file = "zoo_mod.Rds")

#model assumptions
deet_fit <- predict(____) %>% as_tibble
deet_res <- residuals(____)%>% as_tibble

qplot(____$____, ____$____)

#fit
pp_check(____, type="____")

#normality
qqnorm(____$Estimate)
qqline(____$Estimate)
pp_check(____, type="____", bins=____)

##match to posterior
pp_check(____, type="stat_2d", test=c("____", "____"))
pp_check(____)

#coefficients
summary(___, digits=5)

#confidence intervals
___(___)

#visualize
zoo_chains <- as.data.frame(___)

zoot_plot +
  ___(___=___[,1], ___ = ___[,2], alpha=0.1, color="lightgrey") +
  ___(___=fixef(___)[1], ___ = ____(___)[2], color="red") +
  geom_point()
```

<!--chapter:end:08_bayes.Rmd-->

---
title: "ANOVA"
author: "Biol 607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

For this lab, see the etherpad at  https://etherpad.wikimedia.org/p/607-anova-2018

## 1.One-Way ANOVA Model
We'll start today with the dataset `15e1KneesWhoSayNight.csv` about an experiment to help resolve jetlag by having people shine lights at different parts of themselves to try and shift their internal clocks.

```{r knees}
library(tidyverse)

knees <- read_csv("./data/10/15e1KneesWhoSayNight.csv")
```

We can see the outcomes with `ggplot2`

```{r knees_plot}
library(ggplot2)
ggplot(knees, mapping=aes(x=treatment, y=shift)) +
  stat_summary(color="red", size=1.3) +
    geom_point(alpha=0.7) +
  theme_bw(base_size=17)
```

### 1.1 LM, AOV, and Factors
As the underlying model of ANOVA is a linear one, we fit ANOVAs using `lm()` just as with linear regression.

```{r intro_knees}
knees <- read.csv("./data/10/15e1KneesWhoSayNight.csv")

knees_lm <- lm(shift ~ treatment, data=knees)
```

Now, there are two things to notice here. One, note that treatment is a either a character or factor. If it is not, because we are using `lm()`, it will be fit like a linear regression. So, beware!

There **is** an ANOVA-specific model fitting function - `aov`.

```{r aov}
knees_aov <- aov(shift ~ treatment, data=knees)
```

It's ok, I guess, and works with a few functions that `lm()` objects do not. But, in general, I find it too limiting. You can't see coefficients, etc. Boooooring.

### 1.2 Assumption Evaluation

Because this is an lm, we can check our assumptions as before - with one new one.  First, some oldies but goodies.

```{r assumptions}
#The whole par thing lets me make a multi-panel plot
par(mfrow=c(2,2))
plot(knees_lm, which=c(1,2,5))
par(mfrow=c(1,1))
```

Now, the residuals v. fitted lets us see how the residuals are distributed by treatment, but I often find it insufficient, as spacing on the x-axis can get odd. I could roll my own plot of resudials versus treatment, but, there's a **wonderful** package called `car` - which is from the book *Companion to Applied Regression* by John Fox. I recommend it highly! It has a function in it called `residualPlots()` which is useful here.

```{r residualPlots, warning=FALSE}
library(car)
residualPlots(knees_lm)
```

Note how it both does fitted v. residuals but also a boxplot by treatment. Handy, no?

### 1.3 F-Tests

OK, so, let's see the ANOVA table! With the function....`anova()`!

```{r anova}
anova(knees_lm)
```

Now....this is a type I sums of squares test. Which is fine for a 1-way ANOVA. If you want to start getting into the practice of using type II, `car` provides a function `Anova()` - note the capital A - which defaults to type II and I use instead. In fact, I use it all the time, as it handles a wide set of different models.

```{r Anova}
Anova(knees_lm)
```

Here it matters not a whit as you get the same table.


### 1.4 Post-hoc Tests

So, there are a lot of things we can do with a fit model

#### 1.4.0 Summary Output

```{r anova_summar}
summary(knees_lm)
```

First, notice that we get the same information as a linear regression - including $R^2$ and overall model F-test. THis is great. We also get coefficients, but, what do they mean?

Well, they are the treatment contrasts. Not super useful. R fits a model where treatment 1 is the intercept, and then we look at deviations from that initial treatment as your other coefficients. It's efficient, but, hard to make sense of. To not get an intercept term, you need to refit the model without the intercept. You can fit a whole new model with `-1` in the model formulation. Or, as I like to do to ensure I don't frak anything up, you can `update()` your model. Just use `.` to signify *what was there before*.

```{r update_summary}
knees_lm_no_int <- update(knees_lm, formula = . ~ . -1)

summary(knees_lm_no_int)
```

OK - that makes more sense. For a 1-way ANOVA, we can also see treatment means using the `emmeans` package - much more on that next week (and later below for contrasts).

```{r emmeans, message=FALSE}
library(emmeans)
library(multcompView)

emmeans(knees_lm, ~treatment)
```

I also like this because it outputs CIs.

We see means and if they are different from 0. But....what about post-hoc tests

#### 1.4.1 A Priori Contrasts

If you have a priori contrasts, you can use the `constrat` library to test them. You give contrast an a list and a b list. Then we get all comparisons of a v. b, in order. It's not great syntactically, but, it lets you do some pretty creative things.

```{r contrasts, message=FALSE}
contrast::contrast(knees_lm, 
         a = list(treatment = "control"), 
         b = list(treatment = "eyes"))
```

#### 1.4.2 Tukey's HSD
Meh. 9 times out of 10 we want to do something more like a Tukey Test. There is a `TukeyHSD` function that works on `aov` objects, but, if you're doing anything with an `lm`, it borks on you. Instead, let's use `emmeans`. It is wonderful as it's designed to work with ANOVA and ANCOVA models with complicated structures such that, for post-hocs, it adjusts to the mean or median level of all other factors. Very handy. 

```{r tukey_emmeans}
knees_em <- emmeans(knees_lm, ~treatment)

contrast(knees_em,
        method = "tukey")
```

We don't need to worry about many of the fancier things that emmeans does for the moment - those will become more useful with other models. But, we can look at this test a few different ways. First, we can visualize it

```{r plot_tukey}
plot(contrast(knees_em,
        method = "tukey")) +
  geom_vline(xintercept = 0, color = "red", lty=2)
```

We can also, using our tukey method of adjustment, get "groups" - i.e., see which groups are statistically the same versus different.

```{r groups}
library(multcompView)
cld(knees_em, adjust="tukey")
```

This can be very useful in plotting. For example, we can use that output as a data frame for a `ggplot` in a few different ways.

```{r plot_groups}
cld(knees_em, adjust="tukey") %>%
  ggplot(aes(x = treatment, y = emmean, 
             ymin = lower.CL, ymax = upper.CL,
             color = factor(.group))) +
  geom_pointrange() 


cld(knees_em, adjust="tukey") %>%
  mutate(.group = letters[as.numeric(.group)]) %>%
  ggplot(aes(x = treatment, y = emmean, 
             ymin = lower.CL, ymax = upper.CL)) +
  geom_pointrange() +
  geom_text(mapping = aes(label = .group), y = rep(1, 3)) +
  ylim(c(-2.5, 2))


knees_expanded <- left_join(knees, cld(knees_em, adjust="tukey"))
ggplot(knees_expanded,
       aes(x = treatment, y = shift, color = .group)) + 
  geom_point()
```

#### 1.4.2 Dunnet's Test

We can similarly use this to look at a Dunnett's test, which compares against the control
```{r bunnett_emmeans}
contrast(knees_em,
        method = "dunnett")
```

Note, if the "control" had not been the first treatment, you can either re-order the factor using `forcats` or just specify which of the levels is the control. For example, eyes is the second treatment. Let's make it our new reference.

```{r bunnett_emmeans_2}
contrast(knees_em,
        method = "dunnett", ref=2)
```

You can even plot these results
```{r plot_contrast}
plot(contrast(knees_em,
        method = "dunnett", ref=2)) +
  geom_vline(xintercept = 0, color = "red", lty=2)
```



#### 1.4.2 Bonferroni Correction and FDR

Let's say you wanted to do all pairwise tests, but, compare using a Bonferroni correction or FDR. Or none! No problem! There's an `adjust` argument

```{r tukey_emmeans_other_adjust}
contrast(knees_em,
        method = "tukey", adjust="bonferroni")


contrast(knees_em,
        method = "tukey", adjust="fdr")

contrast(knees_em,
        method = "tukey", adjust="none")
```



### 1.5 Faded Examples
Let's try three ANOVAs!
First - do landscape characteristics affect the number of generations plant species can exist before local extinction?

```{r plants, eval=FALSE}
plants <- read.csv("./data/10/15q01PlantPopulationPersistence.csv")

#Visualize
qplot(treatment, generations, data=plants, geom="boxplot")

#fit
plant_lm <- lm(generations ~ treatment, data=plants)

#assumptions
plot(plant_lm, which=c(1,2,4,5))

#ANOVA
anova(plant_lm)

#Tukey's HSD
contrast(emmeans(plant_lm, ~treatment), method = "tukey")
```

Second, how do different host types affect nematode longevity?


```{r nemetods, eval=FALSE}
worms <- read.csv("./data/10/15q19NematodeLifespan.csv")

#Visualize
qplot(treatment, lifespan, data=____, geom="____")

#fit
worm_lm <- lm(______ ~ ______, data=worms)

#assumptions
plot(______, which=c(1,2,4,5))

#ANOVA
anova(______)

#Tukey's HSD
contrast(emmeans(______, ~________), method = "tukey")
```

And last, how about how number of genotypes affect eelgrass productivity. Note, THERE IS A TRAP HERE. Look at your dataset before you do ANYTHING.

```{r eelgrass, eval=FALSE}
eelgrass <- read.csv("./data/10/15q05EelgrassGenotypes.csv")

#Visualize
________(treatment.genotypes, shoots, data=____, geom="____")

#fit
eelgrass_lm <- __(______ ~ ______, data=________)

#assumptions
________(______, which=c(1,2,4,5))

#ANOVA
________(______)

#Tukey's HSD
contrast(________(______, ~________), method = "________")
```

## 2. ANODEV (ANOVA + Likelihood)
If you had done this using likelihood, you could have done all of this with a LR Chisq also using `Anova()` from the `car` package.

```{r glm_anova}
knees_glm <- glm(shift ~ treatment, data=knees,
                 family=gaussian())

Anova(knees_glm)
```

Further, `emmeans` works here as well.

```{r glm_emmeans}
knees_glm_em <- emmeans(knees_glm, ~treatment)

cld(knees_glm_em)
```

And we can look at posthocs as usual

```{r glm_contrast}
contrast(knees_glm_em, method = "dunnett")
```

## 3. BANOVA

### 3.1 Fit and Summary
Yes, we can fit this using Bayesian methods as well. Here I'll use default priors (flat on the means).

```{r banova, message=FALSE, results = "hide"}
library(brms)
library(tidybayes)

knees_banova <- brm(shift ~ treatment,
                         data = knees,
                         family=gaussian(),
                     file = "knees_banova.rds")
```

We can actually examine this in a few different ways. First, there's a default method for looking at ANOVA-like models in `brms` using `margina_effects`

```{r marginal}
marginal_effects(knees_banova)
```

Neat, right?  We can also use `emmeans` as before.

```{r banova_emmeans}
knees_b_em <- emmeans(knees_banova, ~treatment)
knees_b_em
```

Note, now we're lookign at posteriod intervals (and you define that interval!)


### 3.2 BANOVA and Variance Partioning

An analogue to F-tests, although philosophically different, is to look at how the finite population variance - i.e. the variation of just your dataset rather than the entire super-population - is explained by different elements of the model. We'll explore this more next week, but, for now, let's get a feel for it. We want to visualize the variability in the model due to each source - i.e. treatment and residual in this case. Then, let's look at the posterior of this variance component, both in terms of raw numbers, and percentages.  

`emmeans` helps us out here again as it provides ready access to the treatment levels.

```{r banova_var_trt}
sd_treatment <- gather_emmeans_draws(knees_b_em) %>%
  group_by(.draw) %>%
  summarize(sd_treatment = sd(.value))
```

Extracting residuals is a bit trickier, as `tidybayes` does not yet have a nice residual extractor. I wrote some code for that - https://gist.github.com/jebyrnes/c28d1f5523be392e4666da2f06110c10 - and submitted an issue, but it might be a while until they get it.  

For now, we can use the `residuals` function with `summary=FALSE` which returns a giant matrix. We can manipulate the matrix a bit and summarise it to get the sd of residuals for each draw of the coefficients. Then, we can make a nice big tibble for plotting. I admit, this is a bit of a PITA, but, it will also help you get into the guts of BANOVA.

```{r banova_var_resid}
sd_residuals <- residuals(knees_banova, summary=FALSE) %>%
  t() %>%
  as.data.frame() %>%
  summarise_all(sd) %>%
  as.numeric
  
sd_groups <- tibble(type = c(rep("treatment", 4000),
                             rep("residual", 4000)),
                    value = c(sd_treatment$sd_treatment, sd_residuals))
```

Now we can plot this and look at it in a variety of ways.

```{r banova_view}
ggplot(sd_groups, 
       aes(x = value, y = type)) +
  geom_halfeyeh()
```

We can also make a table using a tidier from broom. `broom` has a wonderful function for summarizing MCMC results called `tidyMCMC` - but we need each posterior to have it's own column, so, we can make a new tibble.

```{r tab_bayes}
sd_bycol <- tibble(treatment_sd = sd_treatment$sd_treatment,
                   residuals_sd = sd_residuals)

broom::tidyMCMC(sd_bycol, conf.int = TRUE, conf.method = "HPDinterval")
```

Last, we can also look at this so that we can get % of variance by transforming `sd_bycol` to percentages.

```{r percent_banova}
sd_percent_bycol <- sd_bycol/rowSums(sd_bycol) * 100

broom::tidyMCMC(sd_percent_bycol, estimate.method = "median",
         conf.int = TRUE, conf.method = "HPDinterval")
```
### 3.3 BANOVA post-hocs

We can use `emmeans` for contrasts, too. Here's a tukey test.

```{r tukey}
contrast(knees_b_em, method = "tukey")
```


We can visualize this using `tidybayes::gather_emmeans_draws`` to see the results of the contrast.

```{r vis_bayes_cont}
contrast(knees_b_em, method = "tukey") %>%
  gather_emmeans_draws() %>%
  ggplot(aes(x = .value, y = contrast)) +
  geom_halfeyeh() +
  geom_vline(xintercept = 0, color = "red", lty = 2)
```


Or maybe something that matches earlier visualizations
```{r vis_bayes_2}
contrast(knees_b_em, method = "tukey") %>%
  gather_emmeans_draws() %>%
  ggplot(aes(y = .value, x = contrast)) +
  geom_jitter(alpha = 0.05)+
  geom_hline(yintercept = 0, color = "red", lty = 2)
```


As you can see, `gather_emmeans_draws` lets us to a lot with categorical variables in a BANOVA context very easily. We can even use it to generate some interesting and useful visualizations of the means themselves with some additional geoms.

```{r vis_bayes_3}
gather_emmeans_draws(knees_b_em) %>%
  ggplot(aes(x = treatment, y = .value)) +
  stat_lineribbon(alpha = 0.25, fill = "gray25") +
  stat_pointinterval() 

gather_emmeans_draws(knees_b_em) %>%
  ggplot(aes(x = treatment, y = .value)) +
  geom_line(aes(group = .draw), alpha = 0.01) +
  stat_pointinterval(color = "darkred") 
  
```

We can even add this back to the original data.
```{r vis_bayes_4}

ggplot(knees,
       aes(x = treatment, y = shift)) +
  geom_point() +
  stat_pointinterval(data = gather_emmeans_draws(knees_b_em), 
                     mapping = aes(y = .value), color = "red", alpha = 0.4)

```
  
  
## 4. Final Exercise

Take two of the examples from the faded examples section. Execute one analysis using likelihood. Execute the other using Bayesian methods. Be sure to do all of the usual checks!

<!--chapter:end:10_anova.Rmd-->

---
title: "Multiway and Factorial ANOVA"
author: "Biol 607"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

For this lab, see the etherpad at  https://etherpad.wikimedia.org/p/607-anova-2018

Let's start with some libraries from last time!
```{r libraries}
library(car)
library(tidyverse)
library(ggplot2)
library(emmeans)
```

## 1. Two-Way ANOVA
We'll work with the zooplankton depredation dataset for two-way ANOVA. This is a blocked experiment, so, each treatment is in each block just once.

```{r zoop}
zooplankton <- read.csv("./data/18e2ZooplanktonDepredation.csv")

qplot(treatment, zooplankton, data=zooplankton, geom="boxplot")
qplot(block, zooplankton, data=zooplankton, geom="boxplot")
```

Oh. That's odd. What is up with block? AH HA! It's continuous. We need to make it discrete to work with it.

```{r zoop_factor}
zooplankton$block  <- factor(zooplankton$block)
qplot(block, zooplankton, data=zooplankton, geom="boxplot")
```

There we go. Always check! 

### 1.1 Fit and Assumption Evaluation
Fit is quite easy. We just add one more factor to an lm model!

```{r zoop_fit}
zooplankton_lm <- lm(zooplankton ~ treatment + block,
                     data = zooplankton)
```

We then evaluate residuals almost as usual...

```{r zoop_assume}
par(mfrow=c(2,2))
plot(zooplankton_lm, which=c(1,2,5))
par(mfrow=c(1,1))
```

We want to look more deeply by treatment and block. For which we use `car`'s `residualPlots()`

```{r zoop_car}
residualPlots(zooplankton_lm)
```

Notice that this pops out a Tukey test, and we are looking...GOOD!

### 1.2 Type II Sums of Squares
Given that we now have multiple factors, in case of unbalance, we should use type II sums of squares.

```{r Anova_zoop}
Anova(zooplankton_lm)
```

### 1.3 Coefficients

If we want to look at coefficients, we have to make means contrasts.

```{r means}
summary(update(zooplankton_lm, . ~ . -1))
```

This is still a little odd, though, as our treatments are evaluated in block 1. To truly get just the treatment effect, we need to look at the estimated marginal means - the `emmeans`!  The big thing about `emmeans` is that it creates a reference grid based on the blocks. It then calculates the treatment effect averaged over all blocks, rather than just in block 1.

```{r emmeans}
zoop_em <- emmeans(zooplankton_lm, ~treatment)

zoop_em
```

### 1.4 Post-Hocs

Here, `emmeans` gets interesting.

```{r ref}
contrast(zoop_em, method = "tukey")
```

Note the message that we've averaged over the levels of block. You can do any sort of posthoc as you wanted before. And, you could have done the same workflow for block as well.  

### 1.5 Faded Examples

Given then similarity with 1-way ANOVA, let's just jump right into two examples, noting a key difference or two here and there.

To start with, let's look at gene expression by different types of bees.

```{r bees_1, eval=FALSE}
bees <- read.csv("./data/18q07BeeGeneExpression.csv")

#Visualize
________(type, Expression, data=____, geom="____")

#fit
bee_lm <- __(______ ~ ______ + _____, data=________)

#assumptions
________(______, which=c(1,2,4,5))

residualPlots(bee_lm)

#ANOVA
________(______)

#Tukey's HSD
contrast(________(______, ____, method = "________")
```

Wow, not that different, save adding one more term and the residualPlots.

OK, one more.... repeating an experiment in the intertidal?
```{r echo=FALSE}
intertidal <- read.csv("./data/18e3IntertidalAlgae.csv")
```

```{r intertidal_1, eval=FALSE}
intertidal <- read.csv("./data/18e3IntertidalAlgae.csv")

#Visualize
________(herbivores, sqrtarea, data=____, geom="____")

#fit
intertidal_lm <- __(______ ~ ______ + _____, data=________)

#assumptions
________(______, which=c(1,2,4,5))

residualPlots(______)

#ANOVA
________(______)

#Tukey's HSD
contrast(________(______, ____, method = "________")
```

Did that last one pass the test of non-additivity?


## 2. Factorial ANOVA
Going with that last intertidal example, if you really looked, it was a factorial design, with multiple treatments and conditions.

```{r plot_mice}
qplot(herbivores, sqrtarea, data=intertidal, fill=height, geom="boxplot")
```

### 2.1 Fit and Assumption Evaluation
We fit factorial models using one of two different notations - both expand to the same thing

```{r int_fact}
intertidal_lm <- lm(sqrtarea ~ herbivores + height + herbivores:height, data=intertidal)

intertidal_lm <- lm(sqrtarea ~ herbivores*height, data=intertidal)
```

Both mean the same thing as `:` is the interaction. `*` just means, expand all the interactions.

But, after that's done...all of the assumption tests are the same. Try them out.

### 2.2 Type II and III Sums of Squares
Now, we can choose type II or III SS once we have >n=1 for simple effects. Let's see the difference. Both are from `Anova()` from the car package.

```{r Anova_compare}
Anova(intertidal_lm)

Anova(intertidal_lm, method="III")
```

### 2.3 Post-Hocs
Post-hocs are a bit funnier. But not by much. As we have an interaction, let's look at the simple effects. There are a few ways we can do this

```{r tukey_simple}
emmeans(intertidal_lm, ~ herbivores + height)

emmeans(intertidal_lm, ~ herbivores | height)

emmeans(intertidal_lm, ~ height | herbivores)
```

Notice how each presents the information in a different way. The numbers are not different, they just show you information in different ways. The contrasts each reference grid implies *do* make s difference, though, in how p-value corrections for FWER is handled. Consider the first case.

```{r tuk_simp}
contrast(emmeans(intertidal_lm, ~ herbivores + height), method = "tukey")
```

OK, cool. Every comparison is made. But what if we don't want to do that? What if we just want to see if the herbivore effect differs by height?

```{r tuk_2}
contrast(emmeans(intertidal_lm, ~ herbivores |height), method = "tukey")
```

Notice that, because we're only doing two contrasts, the correction is not as extreme. This method of contrasts might be more what you are interested in given your question. We can also see how this works visuall.

```{r plot_tuk_fact}
cont <- contrast(emmeans(intertidal_lm, ~ herbivores |height), method = "tukey")

plot(cont) +
  geom_vline(xintercept = 0, color = "red", lty = 2)
```

You can then use `CLD` or your own annotations to complete the visualization

### 2.4 A Kelpy example

Let's just jump right in with an example, as you should have all of this well in your bones by now. This was from a kelp, predator-diversity experiment I ran ages ago. Note, some things that you want to be factors might be loaded as 
```{r echo=FALSE}
kelp <- read.csv("./data/kelp_pred_div_byrnesetal2006.csv")
```

```{r kelp_1, eval=FALSE}
kelp <- read.csv("./data/kelp_pred_div_byrnesetal2006.csv")

## Check and correct for non-factors
____________
_________

#Visualize
________(Treatment, Porp_Change, data=____, geom="____", fill=Trial)

#fit
kelp_lm <- __(______ ~ ______ * _____, data=________)

#assumptions
________(______, which=c(1,2,4,5))

residualPlots(_________)

#ANOVA
________(______)

#Tukey's HSD for simple effects
contrast(________(______, ____, method = "________")
```

#### 3.3.1 The Cost of Tukey
So, the kelp example is an interesting one, as this standard workflow is *not* what I wanted when I ran this experiment. I was not interested in a Tukey test of all possible treatments. Run it with no adjustement - what do you see?

```{r no_adjust, eval=FALSE}
#Pariwise Comparison without P-Value adjustment - The LSD test
contrast(________(______, ____, method = "________", adjust = "_______")
```


Instead, I was interested in asking whether predator diversity - having a mixture versus only one species of predator - led to less kelp loss than any of the other treatments.  There are a few ways to assess the answer to that question.

First, a Dunnet's test with the Predator Mixture as the control.  Try that out. Note, the default "control" is Dungeness crabs, so, you might want to revisit that.

```{r kelp_dunnet, eval=FALSE}
#Dunnet's Test
contrast(________(______, ____, method = "________", ref = _____)
```

What did you learn? 

#### 3.3.2 Replicated Regression

So.... this was actually a replicated regression design. There are a few ways to deal with this. Note the column `Predator_Diversity`


Try this whole thing as a regression. What do you see?

Make a new column that is `Predator_Diversity` as a factor. Refit the factorial ANOVA with this as your treatment. NOW try a Tukey test. What do you see?

#### 3.3.3 A Priori Contrast F tests

OK, one more way to look at this. What we're actually asking in comparing monocultures and polycultures is, do we explain more variation with a monoculture v. poyculture split than if not?

```{r contrast_anova}
kelp_contr <- lm(Change_g ~ C(factor(Predator_Diversity), c(0,1,-1))*Trial, data=kelp)

Anova(kelp_contr)
```

Now we see that, yes, we explain variation when we partition things into monoculture versus polyculture than when we do not.

Setting up a priori ways of partitioning your sums of squares (that must be orthogonal) is a powerful way to test grouping hypotheses and worth keeping in your back pocket for future explorations.

<!--chapter:end:11_anova.Rmd-->

---
title: "General Linear Models"
author: "Biol 607"
date: "December 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. ANCOVA
Combining categorical and continuous variables is not that different from multiway ANOVA. To start with, let's look at the neanderthal data.
```{r neand}
neand <- read.csv("./data/11/18q09NeanderthalBrainSize.csv")
head(neand)
```

We can clearly see both the categorical variable we're interested in, and the covariate.

To get a preliminary sense of what's going on here, do some exploratory visualization with `ggplot2` why doncha!

```{r plot_neand}
library(ggplot2)

qplot(lnmass, lnbrain, color=species, data=neand) +
  stat_smooth(method="lm")
```

Now, the CIs are going to be off as this wasn't all tested in the same model, but you begin to get a sense of whether things are parallel or not, and whether this covariate is important.

What other plots might you produce?

As this is a general linear model, good olde `lm()` is still there for us.
```{r neand_lm}
neand_lm <- lm(lnbrain ~ species + lnmass, data=neand)
```

### 1.1 Testing Assumptions of ANCOVA

In addition to the ususal tests, we need to make sure that the slopes really are parallel. We do that by fitting a model with an interaction and testing it (which, well, if there *was* and interaction, might that be interesting).

First, the usual

```{r neand_tests}
par(mfrow=c(2,2))
plot(neand_lm, which=c(1,2,5))
par(mfrow=c(1,1))

#And now look at residuals by group/predictors
library(car)
residualPlots(neand_lm, tests=FALSE)
```

To test the parallel presumption

```{r neand_parallel}
neand_int <- lm(lnbrain ~ species*lnmass, data=neand)

Anova(neand_int)
```

### 1.2 Assessing results

So, first we have our F-tests.

```{r f_neand}
Anova(neand_lm)
```

Both the treatment and covariate matter.

Second, we might want to compare covariate adjusted groups and/or look at covariate adjusted means.

```{r neand_lsmeans}
library(lsmeans)
adj_means <- lsmeans(neand_lm, specs="species")

#adjusted means
adj_means

#comparisons
contrast(adj_means, method="tukey", adjust="none")
```

If you have an interaction, this method is no longer valid. Instead, you'll need to monkey with your posthocs (if you even want to use them - often we don't) to look at tests at different levels of the covariate.

### 1.3 Visualization

Visualization is funny, as you want to make parallel lines and also get the CIs right. Rather than rely on `ggplot2` to do this natively, we need to futz around a bit with generating predictions

```{r neand_predictions}
neand_predict <- predict(neand_lm, interval="confidence")

head(neand_predict)
```

So, here we have fit values, lower confidence interval, and upper confidence intervals. As we have not fed `predict()` any new data, these values line up with our `neand` data frame, so we can cbind it all together, and then use these values to make a prediction plot.

```{r neand_plot_predict}
neand <- cbind(neand, neand_predict)

ggplot(data=neand) +
  geom_point(mapping=aes(x=lnmass, y=lnbrain, color=species)) +
  geom_line(mapping = aes(x = lnmass, y=fit, color=species)) + 
  geom_ribbon(data=neand, aes(x = lnmass, 
                              ymin=lwr, 
                              ymax=upr,
                              group = species), 
              fill="lightgrey", 
              alpha=0.5) +
  theme_bw()
```

And there we have nice parallel lines with model predicted confidence intervals!

### 1.4 Examples
I've provided two data sets:  
1) `18e4MoleRatLayabouts.csv` looking at how caste and mass affect the energy mole rates expend  
\
2) `18q11ExploitedLarvalFish.csv` looking at how status of a marine area - protected or not - influences the CV around age of maturity of a number of different fish (so, age is a predictor)

Using the following workflow, analyze these data sets.

```{r sampe_workflow}
# Load the data

# Perform a preliminary visualization

# Fit an ANCOVA model

# Test Asssumptions and modeify model if needed

# Evaluate results

# Post-hocs if you can

# Visualize results
```



## 2. Multiple Linear Regression
Multiple linear regression is conceptially very similar to ANCOVA. Let's use the keeley fire severity plant richness data to see it in action.

```{r keeley}
keeley <- read.csv("data/11/Keeley_rawdata_select4.csv")

head(keeley)
```

For our purposes, we'll focus on fire severity and plant cover as predictors.

### 2.1 Visualizing
I'm not going to lie, visualizing multiple continuous variables is as much of an art as a science. One can use colors and sizes of points, or slice up the data into chunks and facet that. Here are a few examples.

```{r plot_keeley}
qplot(cover, rich, color=firesev, data=keeley) +
  scale_color_gradient(low="yellow", high="red") +
  theme_bw()

qplot(cover, rich, color=firesev, data=keeley) +
  scale_color_gradient(low="yellow", high="red") +
  theme_bw() +
  facet_wrap(~cut_number(firesev, 4))
```

Note the new faceting otion. Cool, no?

What other plots can you make?

### 2.2 Fit and Evaluate Assumptions
Fitting is straightforward for an additive MLR model. It's just a linear model!

```{r k_fit}
keeley_mlr <- lm(rich ~ firesev + cover, data=keeley)
```

As for assumptions, we have the usual

```{r k_assume}
par(mfrow=c(2,2))
plot(keeley_mlr, which=c(1,2,5))
par(mfrow=c(1,1))
```

But now we also need to think about how the residuals relate to each predictor. Fortunately, there's still `residualPlots`.

```{r k_resid}
residualPlots(keeley_mlr, test=FALSE)
```

Odd bow shape - but not too bad. Maybe there's an interaction? Maybe we want to log transform something? Who knows!

We also want to look at multicollinearity. There are two steps for that. First, the `vif`

```{r vif}
vif(keeley_mlr)
```

Not bad. We might also want to look at the correlation matrix. Dplyr can help us here as we want to select down to just relevant columns.

```{r k_cor}
library(dplyr)

keeley %>%
  select(firesev, cover) %>%
  cor()
```


### 2.3 Evaluation

We evaluate the same way as usual

```{r k_eval}
Anova(keeley_mlr)
```

And then the coefficients and R<sup>2</sup>

```{r k_coef}
summary(keeley_mlr)
```

Not amazing fit, but, the coefficients are clearly different from 0.

### 2.3 Visualization

This is where things get sticky. We have two main approaches. First, visualizing with component + residual plots

```{r cr_k}
crPlots(keeley_mlr, smooth=FALSE)
```

But the values on the y axis are....odd. We get a sense of what's going on and the scatter after accounting for our predictor of interest, but we might want to look at, say, evaluation of a variable at the mean of the other.

For that, we have `visreg`

```{r k_visreg}
library(visreg)
par(mfrow=c(1,2))
visreg(keeley_mlr)
par(mfrow=c(1,1))
```

Now the axes make far more sense, and we have a sense of the relationship.

We can actually whip this up on our own using `crossing`, the median of each value, and predict.

```{r k_pred}
k_med_firesev <- data.frame(firesev = median(keeley$firesev),
                                 cover = seq(0,1.5, length.out = 100))
k_med_firesev <- cbind(k_med_firesev, predict(keeley_mlr, 
                                              newdata = k_med_firesev,
                                              interval="confidence"))
  
ggplot() +
  geom_point(data=keeley, mapping = aes(x=cover, y = rich)) +
  geom_line(data = k_med_firesev, mapping=aes(x=cover, y=fit), color="blue") +
  geom_ribbon(data = k_med_firesev, mapping = aes(x=cover, 
                                                  ymin = lwr,
                                                  ymax = upr),
              fill="lightgrey", alpha=0.5)
```

### 2.4 Examples
OK, here are two datasets to work with:\

1) `planktonSummary.csv` showing plankton from Lake Biakal (thanks, Stephanie Hampton). Evluate how Chlorophyll (`CHLFa`) is affected by other predictors.  
\
2) `SwaddleWestNile2002NCEAS_shortnames.csv` is about the prevalence of West Nile virus in Birds around Sacramento county in California. What predicts human WNV?\

Using the following workflow, analyze these data sets.

```{r sampe_workflow_mlr}
# Load the data

# Perform a preliminary visualization. Play with this and choose two predictors

# Fit a MLR model

# Test Asssumptions and modeify model if needed

# Evaluate results

# Visualize results
```

## 3. Interaction Effects in MLR


## 4. Information Criteria

<!--chapter:end:11_glm_aic.Rmd-->

---
title: "bayes"
author: "Andrew JH"
date: "November 12, 2018"
output: html_document
---

# Introduction to Stan probabilistic language

Stan is a probabilistic programming language for statistical inference written
in C++ that can be accessed through several interfaces (e.g., R, Python,
Matlab, etc.). We will focus on the package `rstan` that integrates Stan
to R. In Stan, we first define how the structure of the data looks like, the parameters we want
to estimate, and then the priors  and likelihood. Stan uses a variant of
Hamiltonian Monte Carlo (HMC), called No-U-Turn sampler (NUTS), which is much
more efficient than most handcrafted samplers, and also than the traditional Gibbs
sampler used in other probabilistic languages such as (Win)BUGS [@lunn2000winbugs] and JAGS [@plummer2016jags].

<!-- Hoffman, M. D. and Gelman, A. (2011). The no-U-turn sampler: Adaptively setting
path lengths in Hamiltonian Monte Carlo. arXiv, 1111.4246. xi, 25, 118, 171, 316,
391, 394
Hoffman, M. D. and Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting
Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research,
15:1593–1623. xii, 25, 118, 171, 316, 389, 390, 391, 394
 -->

## A Stan program

A Stan program is usually saved as a `.stan` file and accessed through R (or
other interfaces) and it is organized into a sequence of optional and
obligatory blocks, which must be written in order.


```
functions {
// This is an optional block used for functions that can be used in other blocks.
}
data {
// Obligatory block that specifies the required data for the model.
}
transformed data {
// Optional block if we want to manipulate the data.
}
parameters {
// Obligatory block that specifies the model's parameters.
}
transformed parameters {
// Optional block if we want to manipulate the parameters (re-parametrize the model).
}
model {
// Obligatory block that specifies the model's likelihood and priors.
}
generated quantities {
// Optional block if we want to manipulate the output of our model.
}
```

A big difference compared to R is that every variable used needs to be
declared first with its type (real, integer, vector, matrix, etc.).  There are
more types, but we'll start using the following ones. Another difference from R is that there must be a semi-colon (`;`) at the end of each line.

* If a variable `mu` is going to contain a real number, either positive or negative, we define it like this:

`real mu;`

* If a variable `X` is going to contain a real number that is bounded between two numbers (or by only one), we can add `lower` and/or `upper` to the declaration. Suppose `X` is some type of measurement that can only be between 0 and 1000. (We can add lower and/or upper to any type.)

<!-- see Lena's notes to make it clearer -->

`real<lower = 0, upper = 1000>  X;`

* If `N` is going to contain integers, such as the number of observations (which should be $>0$):

`int<lower = 0>  N;`

* If `Y` is a going to contain multiple real values, we define it as a vector, and we need to define the number of elements that it will contain. This number can be a variable that was defined earlier (such as `N`, the number of observations, in our example). We can optionally specify a lower boundary for all the values inside the vector. (In Stan, the values inside a vector are always *real*.)

`vector<lower = 0> [N] Y;`



## A first example \label{sec:first}

As a first example, we will look at on of the analytical example presented before. Recall that we wanted to study whether comprehension of non-canonical sentences in individuals with aphasia is at chance level. The data showed 46 correct responses out of 100. Save this as `firstmodel.stan` (don't run it in R).

```{r firstmodel_stan_code, tidy = TRUE, comment="", echo=FALSE}
cat(readLines("firstmodel.stan"), sep = "\n")
```

Use the following code to call the model from R. We first load the package `rstan`, and then we tell it that we want to save the models we compiled (this will speed up things), and that we want to run the chains in parallel using all the computer cores. (You may want to use all the cores but one, especially if you plan to use your computer while fitting models.) Then we need to save the data as a `list`, before fitting the model with it. In this simple case, the data is just the number of correct answers, and the total number of observations. We'll skip the fake data simulation for now. But see Exercise \ref{ex:first-fake}.


```{r, message=F, warning=F}
library(rstan)
# Save compiled models:
rstan_options(auto_write = TRUE)
# Parallelize the chains using all the cores:
options(mc.cores = parallel::detectCores())
# options(mc.cores = parallel::detectCores() - 1) # If you want to have an extra core free

# Create a list:
qresp_data <-  list(c = 46,
                    N = 100)
# Fit the model with the default values of number of chains and iterations
# chains = 4,    iter = 2000
fit <- stan(file = 'firstmodel.stan', data = qresp_data)
```


We can see a summary of the posterior by "printing" the model's fit.

```{r}
print(fit, pars=c("theta"))
```

The summary displayed by `print`  includes means,  standard  deviations  (`sd`),  quantiles,  Monte
Carlo standard errors (`se_mean`), split Rhats, and effective sample sizes (`n_eff`).  The summaries
are computed after removing the warmup  and merging together all chains. Notice that the `se_mean` is unrelated to the `se` of an estimate in the parallel frequentist model.  We will see later how to communicate the posterior distribution.


And we should also inspect the chains, see the next section:

```{r, fig.height=2}
traceplot(fit, pars=c("theta"))
```


## MCMC diagnostics: Convergence problems and Stan warnings



Before inferring anything from a model, we need to assess convergence. The most important checks or MCMC diagnostics are the following:


* The chains should look like a straight "fat hairy caterpillar": the chains should
  bounce around the same values and with the same variance.
* The potential scale reduction factors, $\hat{R}$s, of the parameters should
  be close to one (as a rule of thumb less than $1.1$).  This indicates that
  the chains have mixed and they are traversing the same distribution.
  $\hat{R}$s are printed in `rstan` summary in the column `Rhat` [see
  section 11.4 of @GelmanEtAl2014].
* The effective sample size, $n_{eff}$ should be large enough. The effective
  sample size is an estimate of the number of independent draws from the
  posterior distribution. Since the samples are not independent, $n_{eff}$
  will generally be smaller than the total number of samples, $N$.  How large
  $n_{eff}$ should be depends on the summary statistics that we want to use.
  But as a rule of thumb, $n_{eff}/N > 0.1$.
* There are no (important) warnings, such as, divergent transitions, Bayesian
  fraction of missing information (BFMI) that was too low, etc. These warning may
  indicate that the sampler is not adequately exploring  the parameter space.
  See also http://mc-stan.org/misc/warnings.html

For useful graphical checks see https://cran.r-project.org/web/packages/bayesplot/vignettes/MCMC-diagnostics.html

These issues **should not be ignored**! See the Appendix \ref{sec:Appendix} for some troubleshooting ideas to solve them.


## A small experiment \label{sec:small}

Now we will focus on a more interesting example. The file \verb|1.dat| contains data of a subject (actually, me) pressing the space bar without reading in a self-paced reading experiment.

### Preprocessing of the data

```{r, reading_noreading}
noreading_data <- read.table(header = F,"data/1.dat",encoding="latin1")
noreading_data <- noreading_data[c("V2","V3","V5","V6","V8")]
colnames(noreading_data) <- c("type","item","wordn","word","RT")
tail(noreading_data)
summary(noreading_data$RT)
class(noreading_data)
```

We can't use the `data.frame`, and we need to transform it to a `list`.

```{r, noreading_list}
# We save the RTs of each row, and also the total number of observations N.

noreading_list <-  list(Y = noreading_data$RT,
                    N = length(noreading_data$RT))
```

### Probability model


Let's model the data with the following assumptions:

- There is a true underlying time, $\mu$, that the participant needs to press the space-bar.
- There is some noise in this process.
- The noise is normally distributed.

This means that the likelihood for each observation $i$ will be:

\begin{equation}
\begin{aligned}
y_i \sim Normal(\mu, \sigma)
\end{aligned}
\end{equation}

where $i =1 \ldots N$

And we are going to use the following priors:
\begin{equation}
\begin{aligned}
\mu &\sim Normal(0, 2000) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0
\end{aligned}
\end{equation}

The prior for $\mu$ is encoding the following information: The model expects
that the underlying time could be both positive and negative, and given that
the scale of the prior (in this case the standard deviation of the normal
distribution) is 2000, we are $\approx 68\%$ certain that the true value would
be between 2000 ms and -2000 ms and $\approx 95\%$ certain that it would be
between -4000 ms and 4000 ms (two standard deviations away from zero). But we
obviously know that the time can't be negative! So we have more prior
information than what we are using for informing the model. We'll discuss this
later. Regarding the prior for $\sigma$: It must be positive, and we are
$\approx 68\%$ certain that the true value would be between 0 ms and 500 ms
and $\approx 95\%$ certain that it would be between 0 ms and 1000 ms.

This can be translated to Stan in the following way:



```{r no_reading_1_stan_code, tidy = TRUE, comment="", echo=FALSE}
cat(readLines("noreading_1.stan"), sep = "\n")
```

Save it as ```no_reading_1.stan```.

### Running the Stan model

<!-- This is the same probabilistic model that we implemented with our handcrafted
Metropolis sampler in 1.2.1.2.
 -->
 Before we fit it to our real data, let's verify
what happens with fake data.

```{r, fake_data}
set.seed(123)
N <- 500
true_mu <- 400
true_sigma <- 125
RT <- rnorm(N, true_mu, true_sigma)

RT <- round(RT)
fake_list <- list(Y=RT,N=N)

```


```{r fit_fake, message=FALSE, warning=FALSE, results="hide"}
fit_fake <- stan(file = 'noreading_1.stan',  data = fake_list)

```

<!-- We see that Stan is much more efficient than our homemade sampler, it converges with fewer iterations: -->

We see that the model converges:

```{r traceplot_fit_fake, fig.height=2}
traceplot(fit_fake)

```

<!-- And we get approximately the same posteriors: -->
And we get the following posterior distributions:

```{r}
print(fit_fake, probs =c(.025,.5,.975), pars=c("mu","sigma"))
```


The successful recovery of the parameters means that the model is behaving as expected, now we can see what happens with real data:

```{r, message=FALSE, warning=FALSE, results="hide"}
fit_noreading <- stan(file = 'noreading_1.stan',
    data = noreading_list)
```

```{r }

print(fit_noreading, probs =c(.025,.5,.975), pars=c("mu","sigma"))

```

```{r, fig.height=2}
traceplot(fit_noreading)
```




### Summarizing the posterior

The package `bayesplot` provides very convenient plot functions (based on `ggplot2`). See the vignettes in
https://cran.r-project.org/web/packages/bayesplot/index.html

```{r noreading_plot, fig.height=2, message=F}
library(bayesplot)
# We need to first extract the chains:
post_noreading <- as.array(fit_noreading)
dimnames(post_noreading)
# And then we can plot them:
mcmc_hist(post_noreading, pars = c("mu", "sigma"))
```




\begin{nbox}{The posterior distribution of $\mu$ is not the distribution of RTs!}
We are assuming that there's a true underlying time it takes to press the space bar, $\mu$, and there is normally distributed noise that generates the different RTs. This is encoded in our likelihood by assuming that RTs are distributed with an unknown true mean $\mu$ (and an unknown standard deviation $\sigma$). The objective of the Bayesian model is to learn about the possible values of $\mu$, or in other words, to get a distribution that encodes what we know about the true mean of the distribution of RTs (and also another distribution that encodes what we know about true standard deviation, $\sigma$, of the distribution of RTs.)
\end{nbox}

*Given our data, what can we learn from the posterior distribution of $\mu$?*

Our model allows us to have answers to questions such as:

**What is the probability that the underlying value of the mindless press of
the space bar would be over, say 170 ms?**

**Answer**:

```{r, message=FALSE, warning=FALSE, tidy=F}

mu_samples <- post_noreading[, , "mu"]
# Another possibility would be
# mu_samples <- rstan::extract(fit_noreading)$mu

# Proportion of samples over 170
mean(mu_samples > 170)

```

**Which range of values contains a specified amount of probability?**


This type of interval is also known as a *credible interval*.

A credible interval demarcates the range within which we can be certain with a certain probability that the "true value" of a parameter lies given the data and the model.
This is very different from the frequentist confidence interval! See for example, @HoekstraEtAl2014 and @MoreyEtAl2015.

The percentile interval is a type of credible interval (the most common one), where we assign equal probability mass to each tail. We generally report 95% credible intervals. But we can extract any interval, a 73% interval, for example, leaves `r (1.00-.73)/2*100`% of the probability mass on each tail, and we can calculate it like this:

```{r, message=FALSE, warning=FALSE}
print(fit_noreading, probs =c((1.00-.73)/2, (1.00+.73)/2), pars=c("mu"))
```

### Influence of priors and sensitivity analysis

Everything was normally distributed in our example (or truncated normal), but the fact that we assumed that RTs were normally distributed is completely unrelated to our (truncated) normally distributed priors. Let's try with uniform priors without low or high boundaries, these are improper distributions^[They don't integrate to 1.] that assign the same probability density to every outcome. In general, this is a bad idea for two reasons: (i) it is computationally expensive (the sampler has a huge parameter space), and (ii) it is providing information that we know it's not accurate (every value is equally likely). But in our very simple example these priors will work.


\begin{equation}
\begin{aligned}
\mu &\sim Uniform(-\infty, \infty) \\
\sigma &\sim Uniform(0, \infty)
\end{aligned}
\end{equation}


This can be translated to Stan in the following way. If no priors are specified, uniform priors are assumed. I'm saving it as `noreading_2.stan`.

<!-- cat(readLines("noreading_1.stan"), sep = "\n")
 -->

```{r no_reading_2_stan_code, tidy = TRUE, comment="", echo=FALSE}
cat(readLines("noreading_2.stan"), sep = "\n")
```

```{r, message=FALSE, warning=FALSE, results="hide"}
fit_noreading_2 <- stan(file = 'noreading_2.stan',
    data = noreading_list)
```

The output of the model will be more or less the same.

```{r }

print(fit_noreading_2, probs =c(.025,.5,.975), pars=c("mu","sigma"))

```

```{r, fig.height=2}
traceplot(fit_noreading_2)
```

```{r fit_noreading_2plot, fig.height=2, message=F}
post_noreading_2 <- as.array(fit_noreading_2)
mcmc_hist(post_noreading_2, pars = c("mu", "sigma"))

```


In general we don't want our priors to have too much influence on our
posterior. This is unless we have *very* good reasons for having informative
priors, such as a very small sample and a lot of prior information; an example
would be if we have data from an impaired population, which makes it hard to
increase our sample size. In general, however, we only use priors to get
reasonable posterior distributions. We center the priors in 0 and we let the
data alone to "decide" whether an effect is positive or negative. This type of
priors are called *weakly regularizing priors*. Notice that a uniform prior is
not a weakly regularizing prior, it assumes that every value is equally
likely, zero is as likely as infinity. If you're unsure whether the priors you
chose have too strong an effect on the posterior distribution, you can do a
*sensitivity analysis*: You try different priors and verify that the posterior
doesn't change drastically [for a published  example, see
@VasishthetalPLoSOne2013]. See also Exercise \ref{ex:firstpriors}.




## A slightly more interesting analysis of the small experiment \label{sec:moreinter}

More realistically, we might have run the small experiment to know whether I (the
participant) tended to speedup (practice effect) or slowdown (fatigue effect) while I was
pressing the space bar. And maybe how strong  this effect was.


### Preprocessing the data

We need to have data about the number of times the space bar was
pressed for each observation, and add it to our list. It's a good idea to
center the number of presses (a covariate) to have a
clearer interpretation of the intercept. In general, centering predictors is
always a good idea, for interpretability and for computational reasons.

```{r, reading_noreading_sb}
# We create the new column in the data frame
noreading_data$presses <- 1:nrow(noreading_data)
# We center the column
noreading_data$c_presses <- noreading_data$presses - mean(noreading_data$presses)
# We add it to the list that goes into rstan
noreading_list$presses <-  noreading_data$c_presses
```

### Probability model


Our model changes, because we have a new parameter.

\begin{equation}
RT_i \sim Normal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


And we are going to use the following priors:

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 2000) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0 \\
\beta &\sim Normal(0, 500)
\end{aligned}
\end{equation}


We are basically fitting a linear model, $\alpha$ represents the intercept (namely, the grand mean of the RTs), and $\beta$ represents the slope. Which information are the priors encoding? Does it make sense?

We'll write this in Stan code as follows.

<!-- cat(readLines("noreading_1.stan"), sep = "\n")
 -->

```{r no_reading_3_stan_code, tidy = TRUE, comment="", echo=FALSE}
cat(readLines("noreading_3.stan"), sep = "\n")
```

Save it as `noreading_3.stan`.

```{r, message=FALSE, warning=FALSE, results="hide"}
fit_noreading_3 <- stan(file = 'noreading_3.stan',
    data = noreading_list)
```

```{r }

print(fit_noreading_3, probs =c(.025,.5,.975), pars=c("alpha","beta","sigma"))

```

```{r, fig.height=2}
traceplot(fit_noreading_3)
```

```{r fit_noreading_3plot, fig.height=2, message=F}
post_noreading_3 <- as.array(fit_noreading_3)
mcmc_hist(post_noreading_3, pars=c("alpha","beta","sigma"))

```


### Summarizing the posterior and inference

How can we answer our research question? What is the effect of pressing the
bar on the participant's reaction time?




```{r, echo=F, results="hide"}
beta_mean <- round(summary(fit_noreading_3)$summary["beta","mean"],2)
beta_low <- round(summary(fit_noreading_3)$summary["beta","2.5%"],2)
beta_high <- round(summary(fit_noreading_3)$summary["beta","97.5%"],2)
```



We'll need to examine what happens with $\beta$. The summary gives us important information, we can learn that the most likely values of $\beta$ will be around the mean of the posterior `r beta_mean`, and we can be 95% certain that the true value of $\beta$ *given the model and the data* lies between `r beta_low` and `r beta_high`. We extract these values by doing `summary(fit_noreading_3)$summary["beta",column_name]` and replacing column_name by either `"mean"`, `"2.5%"`, or `"97.5%"`.

We see that as the number of times the space bar is pressed increases, the participant becomes slower. If we want to know how  likely it is that the participant was slower rather than faster, we can examine the proportion of samples above zero:

```{r beta_samples}

beta_samples <- post_noreading_3[, , "beta"]

mean(beta_samples > 0)

```

We would report this in a paper as $\hat\beta = `r beta_mean`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$, $P(\beta >0)  \approx `r mean(beta_samples > 0)`$




Can we really conclude that there is a fatigue effect? It depends on how much
we expect the fatigue to affect the RTs. Here we see that only after 100
button presses, we'll see a slowdown of 9 ms on average ($0.09 \times 100$). We
will need to think whether the size of this effect make sense considering the
previous literature. Sometimes this requires a meta-analysis. See @JaegerEtAl2017 for an example, and the use of this prior knowledge in @NicenboimEtAl2016NIG.



### Posterior predictive checks

Let's say we know that our model is working as expected, since we already used
fake data to test the recovery of the parameters. How do we know if our model
is "good"?

We will examine the *descriptive adequacy* of the models [@ShiffrinEtAl2008;
@GelmanEtAl2014, Chapter 6]: the observed data should look plausible under the
*posterior predictive distribution*. The posterior predictive distribution is
composed of one dataset for each sample from the posterior. (So it will
generate as many datasets as iterations we have after the warm-up.)  Achieving
descriptive adequacy means that the current data could have been predicted
by the model. Passing a test of descriptive adequacy is not strong evidence
in favor of a model, but a major failure in descriptive adequacy can be
interpreted as strong evidence against a model [@ShiffrinEtAl2008].


To do posterior predictive checks we need to add the `generated_quantities`
block at the end of our Stan model and we'll store the predictions in a vector
called `pred_Y` with length `N` (number of observations). We do this with a
function called `normal_rng(mu, sigma)`; this function generates a random
number (*rng* stands for random number generator) based on the two parameters
of the function (similar to the R function `rnorm(1, mu, sigma)`). Notice that
`pred_Y` will iterate over the entire dataset of length `N` on *each
iteration of the sampler*. This will become clearer after running the code.

Save the model as `noreading_4.stan`.

```{r no_reading_4_stan_code, tidy = TRUE, comment="", echo=FALSE, warning=F}
cat(readLines("noreading_4.stan"), sep = "\n")
```


We fit the model, and check its convergence as usual.

```{r, message=FALSE, warning=FALSE, results="hide"}
fit_noreading_4 <- stan(file = 'noreading_4.stan',
    data = noreading_list)

```
```{r, eval=F}
traceplot(fit_noreading_4, pars=c("alpha","beta","sigma"))
print(fit_noreading_4, probs =c(.025,.5,.975), pars=c("alpha","beta","sigma"))
```

But now, once we have fit the new model it is possible to extract the predicted RTs:

```{r, message=FALSE, warning=FALSE, tidy=F}
matrix_pred_Y <- rstan::extract(fit_noreading_4)$pred_Y
dim(matrix_pred_Y)
# first dimension: number of samples = 2000 iterations per chain /2 * 4 chains
# second dimension: number of original datapoints
```

This means we have `r dim(matrix_pred_Y)[1]` simulations, each simulation
consists of an entire dataset that is created with the values of `mu`, `beta`
and `sigma` that the corresponds to a given iteration (after the warmup). For
example, `matrix_pred_Y[1,]` is a complete simulated dataset generated from
the first sample of the parameters.

\begin{nbox}{We could recreate it in R with the code that follows.}

Notice that this won't return the exact same values than Stan gave us, however, because we
are generating random numbers based on the distribution.

\end{nbox}


```{r, message=FALSE, warning=FALSE, tidy=F}
alpha_samples <- rstan::extract(fit_noreading_4)$alpha
beta_samples <- rstan::extract(fit_noreading_4)$beta
sigma_samples <- rstan::extract(fit_noreading_4)$sigma
rt_gen <- c()
for(i in 1:noreading_list$N) {
  mu <- alpha_samples[1] + beta_samples[1] * noreading_list$presses[i]
  sigma<- sigma_samples[1]
  rt_gen[i] <- rnorm(1, mu , sigma )
}

length(rt_gen)
head(rt_gen)
```

-----



We'll use the values generated by our Stan model to verify whether the general
shape of the actual distribution matches the distributions from some of the
generated datasets. Let's compare the real data against 11 of these 4000
datasets (called here `y_rep`):

```{r, message=FALSE, warning=FALSE,fig.height=4}
# Let's pick 11 random simulations over the 4000 that we have
pick <- sample(1:4000,11)
ppc_hist(noreading_list$Y, matrix_pred_Y[pick, ])
```

*Is the simulated data similar to the real data?*

Our dataset seems to be more skewed to the right than  our predicted
datasets. This is not too surprising, we assumed that the likelihood was a
normal distribution, but latencies are not very normal-like, they can't be
negative and can be arbitrarily long.


### A better probability model using the log-normal distribution

Since we know that the latencies shouldn't be normally distributed, we can choose a more realistic distribution for the likelihood. A good candidate is the log-normal distribution since a variable (such as time) that is log-normally distributed takes only positive real values.

If $Y$ is log-normally distributed, this means that $log(Y)$ is normally distributed.^[In fact, $log_e(Y)$ or $ln(Y)$, but since it is the most popular logarithm in statistics we'll write it as just $log()$] Something important to notice is that the log-normal distribution is defined using again $\mu$ and $\sigma$, but this corresponds to the mean and standard deviation of the normally distributed logarithm $log(Y)$.  Thus $\mu$ and $\sigma$ are on a different scale than the variable that is log-normally distributed.

This also means that you can create a log-normal distribution by exponentiating the samples of a normal distribution:



```{r lognormal, fig.height=2,fig.width=3.5, fig.show='hold', message=F, warning=F}
mu <- 6
sigma <- 0.5
N <- 100000
# We generate N random samples from a log-normal distribution.
sl <- rlnorm(N, mu, sigma)
lognormal_plot <- ggplot(data.frame(samples=sl), aes(sl)) + geom_histogram() +
      ggtitle("Log-normal distribution\n") + ylim(0,25000) + xlim(0,2000)
# We generate N random samples from a normal distribution, and then we exponentiate them
sn <- exp(rnorm(N, mu, sigma))
normalplot <- ggplot(data.frame(samples=sn), aes(sn)) + geom_histogram() +
      ggtitle("Exponentiated samples of\na normal distribution") + ylim(0,25000) + xlim(0,2000)

plot(lognormal_plot)
plot(normalplot)
```



### The slightly more realistic probability model

If we assume that RTs are log-normally distributed, we'll need to change our model:

\begin{equation}
Y_i \sim LogNormal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


But now the scale of our priors needs to change! They are no longer in milliseconds.

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 10) \\
\sigma &\sim Normal(0, 2) \text{ truncated so that } \sigma > 0 \\
\beta &\sim Normal(0, 1)
\end{aligned}
\end{equation}

The interpretation of the parameters changes and it is more
complex than if we were dealing with a linear model (that is, with  a normal distribution):

* $\alpha$. In our previous linear model, $\alpha$ represented the grand mean (or the grand median since in a normal distribution both coincide), and was equivalent to our previous $\mu$ (since $\beta$ was multiplied by 0). But now, the grand mean needs to be calculated in the following way,  $\exp(\alpha +\sigma ^{2}/2)$. Interestingly, the grand median will  just be $exp(\alpha)$,^[You can check in Wikipedia (https://en.wikipedia.org/wiki/Log-normal_distribution) why.] and we could assume that this represents the underlying time it takes to press the space bar if there would be no noise, that is, if $\sigma$ had no effect. This also means that the prior of $\alpha$ is not in milliseconds, but in log(milliseconds).

*  $\beta$. In a linear model, $\beta$ represents the slowdown for each time the space bar is pressed. Now $\beta$ is the effect on the log-scale, and the effect in milliseconds depends on the intercept $\alpha$: $exp(\alpha + \beta) - exp(\alpha)$. Notice that the log is not linear and the effect of  $\beta$ will have more impact on milliseconds as the intercept grows. For example, if we start with (i) $exp(5) = 148$, and we add $0.1$ in log-scale, $exp(5 + 0.1) = 164$, we end up with a difference of 15 ms; if we start with (ii) $exp(6) = 400$, and we add $0.1$, $exp(6 + 0.1) = 445$, we end up with a difference of 45 ms. You can also see it graphically below.

*  $\sigma$. This is the standard deviation of the normal distribution of $log(y)$.

<!-- ($exp(10+.1) - exp(10) >  exp(1+.1) - exp(1)$, that is, $`r exp(10+.1) - exp(10)` > `r exp(1+.1) - exp(1)`$). -->

```{r, echo=F}

ms_diff <- function(Intercept){
  exp(Intercept + .1) - exp(Intercept)
}
df <- tibble::data_frame(Intercept=seq(.1,15,.01), ms= ms_diff(Intercept))
ggplot(df, aes(x=Intercept,y=ms)) + geom_point() + scale_y_continuous("Difference in milliseconds")

```

What kind of information are the priors encoding?


* For $\alpha$: We are 95% certain that the grand median of the RTs will be between $\approx `r exp(-10*2)`$ and $`r round(exp(10*2))`$ milliseconds. This is a (very-)weakly regularizing prior  because it won't affect our results, but it will down-weights values for the grand median of the RTs that are extremely large, and won't allow the grand median to be negative. We calculate the previous range by back-transforming the values that lie between two standard deviations of the prior ($2 \times 10$) to millisecond scale: $exp(-10 \times 2)$ and $exp(10 \times 2)$).

* For $\beta$: This is more complicated, because the effect on milliseconds will depend on the estimate of $\alpha$. However, we can assume some value for $\alpha$ and it will be enough to be in the right order of magnitude. So let's assume 500 ms. That will mean that we are 95% certain that the effect of pressing the space bar will be between $`r round(exp(log(500) - 4)) - 500`$ and $`r round(exp(log(500) + 4)) - 500`$ milliseconds. It is asymmetric because the log-scale is asymmetric. But the prior is weak enough so that if we assume 1000 or 100 instead of 500, the possible estimates of $\beta$ will still be contained in the prior distribution.  We calculate this by first finding out the value in milliseconds when we are two standard deviations away in both directions: ($2\times 2$), that is $exp(500 - 2 \times 2)$ and  $exp(500 + 2 \times 2)$, and we subtract from that the value of $\alpha$ that we assumed, $500$:   $exp(500 - 2 \times 2) - 500$ and  $exp(500 + 2 \times 2) - 500$.

*  For $\sigma$. This indicates that we are 95% certain that the standard deviation of $log(y)$ will be between 0 and 2. So 95% of the RTs  will be between $exp(log(500) - 1 \times 2) = `r round(exp(log(500) - 1 * 2))`$ and $exp(log(500) + 1 \times 2) = `r round(exp(log(500) + 1 * 2))`$.

What happens if we replace 500 by 100, and by 1000? What happens if it is 10 instead? Does it still makes sense?



We'll code the model as follows. Notice that we can use the `generated quantities` also to transform our parameters in a scale we can understand more easily (milliseconds).


```{r no_reading_log_stan_code, tidy = TRUE, comment="", echo=FALSE, warning=F}
cat(readLines("noreading_log.stan"), sep = "\n")
```
Save the above code as `noreading_log.stan`.

We fit the model, and check its convergence as usual.

```{r, message=FALSE, warning=FALSE, results="hide"}
fit_noreading_log <- stan(file = 'noreading_log.stan',
    data = noreading_list)
```
```{r, fig.height=2}
traceplot(fit_noreading_log, pars =c("alpha","beta","sigma"))
print(fit_noreading_log, probs =c(.025,.5,.975), pars=c("alpha","beta","sigma", "RT_under", "effect_of_1_press"))
# We'll need more digits to see what's going on with beta
print(fit_noreading_log, probs =c(.025,.5,.975), pars=c("beta"),digits=5)
```


### Summarizing the posterior and inference

```{r, echo=F, results="hide"}
options(scipen=999, digits=6)

beta_mean <- round(summary(fit_noreading_log)$summary["beta","mean"],4)
beta_high <- round(summary(fit_noreading_log)$summary["beta","97.5%"],4)
beta_low <- round(summary(fit_noreading_log)$summary["beta","2.5%"],4)

post_noreading_log <- as.array(fit_noreading_log)
beta_samples <- post_noreading_log[, , "beta"]

```


We can summarize the posterior and do inference as before. If we want to talk about the effect estimated by the model, we summarize the posterior of $\beta$ in the following way: $\hat\beta = `r beta_mean`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$, $P(\beta >0)  \approx `r mean(beta_samples > 0)`$

```{r, echo=F, results="hide"}

beta_mean <- round(summary(fit_noreading_log)$summary["beta","mean"],4)
beta_high <- round(summary(fit_noreading_log)$summary["beta","97.5%"],4)
beta_low <- round(summary(fit_noreading_log)$summary["beta","2.5%"],4)

post_noreading_log <- as.array(fit_noreading_log)
beta_samples <- post_noreading_log[, , "beta"]

```

```{r, echo=F, results="hide"}

effect_of_1_press_mean <- round(summary(fit_noreading_log)$summary["effect_of_1_press","mean"],2)
effect_of_1_press_high <- round(summary(fit_noreading_log)$summary["effect_of_1_press","97.5%"],2)
effect_of_1_press_low <- round(summary(fit_noreading_log)$summary["effect_of_1_press","2.5%"],2)

options(scipen=999, digits=3)

```


But sometimes, the effect in milliseconds is easier to interpret. We generated
`effect_of_1_press` in the generated quantities block, which is not the same as
the linear model's $\beta$. Our generated estimate will tell us the estimate
of the slowdown produced by pressing the space bar in the middle of the
experiment once, assuming that the RTs are log-normally distributed:
$`r effect_of_1_press_mean`$ ms, 95% CrI = $[ `r effect_of_1_press_low` , `r effect_of_1_press_high` ]$.
Coincidentally, it is the
same value as before, but this is not always the case, and since it's not
linear the effect won't be the same across the whole experiment; see Exercise
\ref{ex:diff-eff}.



### Posterior predictive checks and distribution of summary statistics

We can now verify whether our predicted datasets look more similar to the real dataset.

```{r, message=FALSE, warning=FALSE, fig.height=4}


matrix_pred_Y_log <- rstan::extract(fit_noreading_log)$pred_Y
dim(matrix_pred_Y_log)


# Let's pick 11 random simulations over the 4000
pick <- sample(1:4000,11)
ppc_hist(noreading_list$Y, matrix_pred_Y_log[pick, ])
```

*Is the simulated data now more similar to the real data?*

It seems so, but it's not easy to tell. Another way to examine this would be
to look at the  *distribution of summary statistics*. The idea is to compare the
distribution of representative summary statistics for the datasets generated
by different models and compare them to the observed statistics. Since we
suspect that the log-normal distribution may capture the long tail,  we could
use the maximum as a summary statistics.

We do it using the package `bayesplot`:

```{r, message=FALSE, warning=FALSE, fig.height=2,fig.width=3.5,fig.show='hold', tidy=F}
normalsum <- ppc_stat(noreading_list$Y, matrix_pred_Y, stat = "max") +
             ggtitle("Normal model")
lognormalsum <- ppc_stat(noreading_list$Y, matrix_pred_Y_log, stat = "max") +
                ggtitle("Log-normal model")
plot(normalsum)
plot(lognormalsum)
```

Here we see that both distributions are unable to capture the maximum value of the observed data, and that there's still room for improving the model. Another more advanced possibility is to compare the models using cross-validation techniques; see for example, @GelmanEtAl2014understanding; @VehtariOjanen2012; @VehtariEtAl2017.



### General workflow

This is the general workflow that we recommend for a Bayesian model.

1. Define the full probability model:
    a. Think about the likelihood.
    b. Think about the priors.
    c. Write the Stan model.
2. Fake data simulations:
    a. Simulate data with known values for the parameters.
    b. Fit the model and do MCMC diagnostics.
    c. Verify that it recovers the parameters from simulated data.
3. Fit the model with real data and do MCMC diagnostics.
4. Evaluate the model's fit (e.g., posterior predictive checks, distribution of summary statistics). This may send you back to 1.
5. Inference/prediction.
6. Sometimes model comparison if there's an alternative model.





\newpage

\begin{Sbox}{\subsection{Key concepts}}
\begin{itemize}
\item Stan basic blocks and data types.
\item How to identify convergence problems (MCMC diagnostics).
\item Normal and log-normal distributions.
\item Summaries of the posterior and inference.
\item Model evaluation using posterior predictive checks.
\item General workflow for doing Bayesian analysis.
\end{itemize}
\end{Sbox}

\begin{qbox}{\subsection{Exercises}}
\begin{enumerate}
\item For the  model  in Section \ref{sec:first}.
  \begin{enumerate}
    \item Recall that when we use the Beta distribution as a prior for the $\theta$ parameter in a Binomial, the two parameters of the Beta indicate previous outcomes and not only the most likely value of $\theta$. For example, $Beta(2,2)$ and $Beta(50,50)$ both indicate that $.5$ is the most likely value of $\theta$, but $Beta(50,50)$ indicates that we are more certain about it than $Beta(2,2)$.
    Assume that you are quite certain that the average accuracy in the task is 80\% for individuals with aphasia. How would you change the priors for the model in \ref{sec:first}? Fit the model several times, assuming the same average accuracy as prior information, but varying the amount of uncertainty? When do the results change? \label{ex:firstpriors}
    \item Change the data to \verb|qresp_data <- list(c = 460, N = 1000)|, and repeat \ref{ex:firstpriors} with the same priors.
    \item Simulate data assuming that the true $\theta$ is exactly $.5$ and (i) \verb|N = 100|, (ii) \verb|N = 1000| and run the \verb|firstmodel.stan|. (Tip: use the function \verb|rbinom| to simulate data.) Is the true value of $\theta$ inside 95\% credible interval? \label{ex:first-fake}
 \end{enumerate}
 \item For the  model  in Section \ref{sec:small}
  \begin{enumerate}
  \item Change the priors of $\mu$ and $\sigma$ in \verb|noreading_1.stan| to a Cauchy and a truncated Cauchy distributions. A Cauchy distribution is basically a bell shaped distribution with very fat tails.\footnote{You can read about this distribution in \url{https://en.wikipedia.org/wiki/Cauchy_distribution}.} What are reasonable values for the location and scale? What's the difference between having a Cauchy or a normal distribution as priors?
  \end{enumerate}
\end{enumerate}
Continues in the next page.
\end{qbox}

\begin{qbox}{}
\begin{enumerate}[3.]
\item For the  models  in Section \ref{sec:moreinter}.
  \begin{enumerate}
  \item Edit the \verb|generated quantities| block in \verb|noreading_log.stan| to estimate the slowdown in milliseconds (mean and 90\% credible interval) for the last time the participant  pressed the space bar in the experiment. In addition, predict the slowdown if the experiment would have had 500 observations.\label{ex:diff-eff}
  \item Optional but very recommended: Simulate data that assumes a value of $\alpha$ of 400 and $\sigma$ of 125 and a practice effect of (i) 0.00001  and (ii) 0.1 (both in log-scale). Fit these data with \verb|noreading_log.stan|. Is the true value of the effect in the 95\% credible interval in both cases?
  \item Add word length as a covariate in \verb|noreading_log.stan|, summarize the posterior. How does the length of the words affect RTs?
   \end{enumerate}
\end{enumerate}
\end{qbox}



\newpage

## Appendix - Troubleshooting problems of convergence \label{sec:Appendix}


1. Rhat > 1.1 First of all check that there are no silly mistakes in the model. Forgetting to put parenthesis, multiplying the wrong parameters, using the wrong operation, etc. can create a model that can't converge. As our models grow in complexity there are more places where to make mistakes. Start simple, see if the model works, add complexity slowly, checking if the model converges at every step. In very rare occasions, when Rhat >1.1 and the model is correct, it may help to increase the number of iterations, but then it's usually a better idea to re-parametrize the model, see 3.

2. Stan gives a warning. The solution may also be point 1. But if the model is correctly specified, you should check  Stan's website, there is a very good guide to solve problems in: http://mc-stan.org/misc/warnings.html. If this doesn't work, you may need to re-parametrize the model, see 3.

3. Some models have convergence issues because the sampler struggles to explore the parameters space. This is specially relevant in complex hierarchical models. In this case, the solution might be to re-parametrize the model. This is by no means trivial. However, the simplest parametrization trick to try is to have all the priors on the same rough scale, that is priors shouldn't have different orders of magnitude. You can find some suggestions in the chapter 21 of Stan manual [@Stan2017], and the following case study: http://mc-stan.org/users/documentation/case-studies/qr_regression.html.


<!-- https://cran.r-project.org/web/packages/bayesplot/vignettes/PPC.html -->


<!--chapter:end:bayes.Rmd-->

---
title:
output:
  revealjs::revealjs_presentation:
    reveal_options:
      slideNumber: true
      previewLinks: true
    theme: white
    center: false
    transition: fade
    self_contained: false
    lib_dir: libs
    css: style.css
---

## Factorial Designs & Interaction Effects
<!-- make a better cover slide -->

```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(fig.height=4.5, comment=NA,
               warning=FALSE, message=FALSE,
               dev="jpeg", echo=FALSE)
#read_chunk("./22_factorial_anova.R")

library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(car)
library(lsmeans)

```

```{r plot_algae}
algae <- read.csv("./data/22/18e3IntertidalAlgae.csv")

algae_plot <- qplot(height, sqrtarea,  data=algae, geom="boxplot", fill=herbivores) + theme_bw(base_size=16)
algae_plot
```


```{r lecture22prep}
```

## Outline
1. Factorial ANOVA
\
2. Posthocs
\
3. Unbalanced Designs

## Is the world additive?

> -   Until now, we have assumed factors combine additively\
\
> -   BUT - what if the effect of one factor depends on another?\
\
> -   This is an **INTERACTION** and is quite common\
\
> -   Yet, challenging to think about, and visualize



## Intertidal Grazing!

![image](./images/22/grazing-expt.jpeg){width="70.00000%"}\
Do grazers reduce algal cover in the intertidal?



## Experiment Replicated on Two Ends of a gradient

![image](./images/22/zonation.jpg){width="70.00000%"}\
What happens if you fit this data using \* instead of + in the linear model?

## The Data
```{r plot_algae}
```

## Humdrum Linear Model

<h3 style="text-align:left">sqrtarea ~ height + herbivores</h3>

```{r graze_linear}
algae <- read.csv("./data/22/18e3IntertidalAlgae.csv")
graze_linear <- lm(sqrtarea ~ height + herbivores, data=algae)
knitr::kable(Anova(graze_linear))
```

## Residuals Look Weird
```{r graze_assumptions, fig.height=7}
par(mfrow=c(2,2))
plot(graze_linear, which=c(1,2,5), cex.lab=1.4)
par(mfrow=c(1,1))
```

## Group Residuals Look Odd
```{r zoop_group_assumptions, fig.height=7}
residualPlots(graze_linear, cex.lab=1.4, test=FALSE)
```

## Pattern in Fitted v. Residuals

```{r graze_linear_diagnostics}
residualPlot(graze_linear, variable="fitted", type="response")

```



## Nonlinearity seen in the Tukey Test!

```{r graze_linear_tukey}
knitr::kable(residualPlots(graze_linear, plot=F))
```

(Note: This test is typically used when there is no replication within blocks)



## Factorial Blocked Experiment

![image](./images/22/factorial_blocks.jpg){width="80.00000%"}\



## Factorial Design

![image](./images/22/factorial_layout.jpg){width="80.00000%"}\
Note: You can have as many treatment types as you want (and then 3-way,
4-way, etc. interactions)



## Problem: Categorical Predictors are Not Additive!

```{r plot_algae}
```

You can only see this if you have replication of treatments (grazing) within blocks (tide height)

## The Model For a Factorial ANOVA/ANODEV
$$y_{ijk} = \beta_{0} + \sum \beta_{i}x_{i} + \sum \beta_{j}x_{j} + \sum \beta_{ij}x_{ij} + \epsilon_{ijk}$$
\
$$\epsilon_{ijk} \sim N(0, \sigma^{2} ), \qquad x_{i} = 0,1$$

\

> - Note the new last term
\
> - Deviation due to treatment combination

## The General Linear Model

$$\boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon}$$

> -   $\boldsymbol X$ can have Nonlinear predictors
\
> -   e.g., It can encompass A, B, and A\*B



## How do you Fit an Interaction Effect?

```{r graze_interaction, echo=TRUE}
graze_int <- lm(sqrtarea ~ height + herbivores + herbivores:height,
                data=algae)
```
\
\
<div class="fragment">
```{r graze_interaction2, echo=TRUE}
graze_int <- lm(sqrtarea ~ height*herbivores, data=algae)
```
</div>


## No More Pattern in Fitted v. Residuals

```{r graze_int_resplot}
residualPlot(graze_int, variable="fitted", type="response")
```


## Other Assumptions are Met
```{r graze_assumptions_int, fig.height=7}
par(mfrow=c(2,2))
plot(graze_int, which=c(1,2,5), cex.lab=1.4)
par(mfrow=c(1,1))
```


## F-Tests for Interactions

$SS_{Total} = SS_{A} + SS_{B} + SS_{AB} +SS_{Error}$\
\
$SS_{AB} = n\sum_{i}\sum_{j}(\bar{Y_{ij}} - \bar{Y_{i}}- \bar{Y_{j}} - \bar{Y})^{2}$,
df=(i-1)(j-1)\
\
\
<div class="fragment">
MS = SS/DF, e.g, $MS_{W} = \frac{SS_{W}}{n-k}$\
\

$F = \frac{MS_{AB}}{MS_{Error}}$ with DF=(j-1)(k-1),n - 1 - (i-1) -
(j-1) - (i-1)(j-1)\
</div>


## ANOVA shows an Interaction Effect

```{r graze_interaction_anova}
knitr::kable(Anova(graze_int))
```


## What does the Interaction Coefficient Mean?

```{r graze_interaction_coefs}
```

## What does the Interaction Coefficient Mean?

```{r algae_plot}
knitr::kable(summary(update(graze_int, .~.-1))$coefficients)
```

## Outline
1. Factorial ANOVA
\
2. <font color="red">Posthocs</font>
\
3. Unbalanced Designs

## Post-hoc Tests!
![image](./images/22/gosling_p_value.jpg){width="70.00000%"}

## Posthocs and Factorial Designs

-   Must look at simple effects first

    -   The effects of individual treatment combinations

    \
    \

-   Main effects describe effects of one variable in the complete
    absence of the other

    -   Useful only if one treatment CAN be absent



## Posthoc Comparisons Within Blocks

```{r graze_posthoc_trt, warning=TRUE, messages=TRUE}
contrast( lsmeans(graze_int, "herbivores"), "tukey", adjust="none")
```

## Posthoc Comparisons of Blocks

```{r graze_posthoc_block, warning=TRUE, messages=TRUE}
contrast( lsmeans(graze_int, "height"), "tukey", adjust="none")
```


## Posthoc with Simple Effects Model

```{r graze_posthoc}
contrast( lsmeans(graze_int, c("height", "herbivores")), "tukey", adjust="none")
```


## Posthoc with Simple Effects Model

```{r graze_posthoc_plot}
plot(contrast( lsmeans(graze_int, c("height", "herbivores")), "tukey", adjust="none"), cex.lab=1,2, cex.axis=1.3, cex=1.2)
```

## Outline
1. Factorial ANOVA
\
2. Posthocs
\
3. <font color="red">Unbalanced Designs</font>

## Oh no! I lost a replicate (or two)
\
\
\
```{r graze_unbalance, echo=TRUE}
algae_unbalanced <- algae[-c(1:5), ]
```


```{r}
graze_int_unbalanced <- lm(sqrtarea ~ height*herbivores,
                           data=algae_unbalanced)
```



## Type of Sums of Squares Matters

Type I
```{r graze_unbalance_anova1}
knitr::kable(anova(graze_int_unbalanced))
```

\
Type II
```{r graze_unbalance_anova2}
knitr::kable(Anova(graze_int_unbalanced))
```


## Enter Type III

```{r graze_unbalance_anova23}
knitr::kable(Anova(graze_int_unbalanced, type="III"))
```

\
<div class="fragment">
Compare to type II
```{r graze_unbalance_anova2}
```

</div>

## What’s Going On: Type I, II, and III Sums of Squares
<p align="left">
**Type I Sums of Squares:**
<span class="fragment">&nbsp; &nbsp; SS for A calculated from a model with A + Intercept versus just Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for B calculated from a model with A + B + Intercept versus A + Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for A:B calculated from a model with A + B + A:B +Intercept versus A + B + Intercept</span>
\
\
<span class="fragment">This is **fine** for a balanced design. Variation evenly partitioned.</span>
</p>

## What’s Going On: Type I, II, and III Sums of Squares

<p align="left">
**Type II Sums of Squares:**
<span class="fragment">&nbsp; &nbsp; SS for A calculated from a model with A + B +  Intercept versus B + Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for B calculated from a model with A + B + Intercept versus A + Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for A:B calculated from a model with A + B + A:B +Intercept versus A + B + Intercept</span>
\
\
<span class="fragment">Interaction not incorporated in assessing main effects</span>
</p>

## What’s Going On: Type I, II, and III Sums of Squares

<p align="left">
**Type III Sums of Squares:**
<span class="fragment">&nbsp; &nbsp; SS for A calculated from a model with A + B + A:B + Intercept versus B + A:B + Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for B calculated from a model with A + B + A:B + Intercept versus A + A:B + Intercept</span>
\
<span class="fragment">&nbsp; &nbsp; SS for A:B calculated from a model with A + B + A:B +Intercept versus A + B + Intercept</span>
\
<span class="fragment">Each SS is the unique contribution of a treatment</span>
<span class="fragment">**very conservative**</span>
</p>


## Which SS to Use?
> -   Traditionally, urged to use Type III
\
> -   What do type III models mean?
>       -   A + B + A:B v. B + A:B
\
> -   Interactions the same for all, and if A:B is real, main effects not
    important
\
> -   Type III has lower power for main effects
\
> -   Type II produces more meaningful results if main effects are a concern - which they are!


## Many Treatments
![image](./images/22/4_way_interaction.jpg){width="60.00000%"}


<!--chapter:end:FactorialANOVA.Rmd-->

---
title: "GLS"
author: "Andrew JH"
date: "November 29, 2018"
output: html_document
---
   
   
```{r library-load}
source("http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/My_R_Scripts/dendroCol.R")
library(tidyverse);library(broom);library(mnormt)
library(igraph);library(ggplot2); library(gplots)
library(pvclust);library(Hmisc); library(ellipse)
library(stats4);library(stats); library(car)
library(limma);library(DESeq2);library(edgeR)
library(cummeRbund);library(ArrayExpress)
library(party); library(rpart)
library(nlme);library(lmtest)
require(multtest)
library(CopTea)
library(rafalib)

```

```{r}


clams<-read_delim("/home/drew/Downloads/BIOL607-Introduction-to-Biological-Data-Analysis/BIOL607-Intro-to-Biological-Data-Analysis/data/Clams.txt", delim="\t")
 
clam.lm<-lm(clams$LENGTH ~ clams$MONTH )
clam.lm
plot(clam.lm)
bptest(clam.lm)

bptest(clam.lm, varformula = ~ MONTH, data=clams)
bptest(clam.lm, varformula = ~ LENGTH, data=clams)

clam.wlm<-lm(clams$LENGTH ~ clams$MONTH,data=clams, weights=c(1/clams$MONTH))
clam.wlm
plot(clam.wlm)


plot(clams$LNLENGTH ~ clams$LNAFD)
pairs(clams)



clam.lm<-lm(clams$LNLENGTH ~ clams$LNAFD)
plot(clam.lm)
summary(clam.lm)

clam.gls <- gls(LENGTH ~ MONTH, data=clams, weights = varFixed(~MONTH)) 
clam.gls
plot(clam.gls)
clam.gls1<-gls(LENGTH ~ MONTH, data=clams, weights = varComb(varFixed(~LENGTH), varFixed(~MONTH)))
clam.gls2
plot(clam.gls2)

vMonth <- varIdent(form = ~ 1 | MONTH)
clam.gls2<-gls(LENGTH ~ MONTH, data=clams, weights =vMonth)
clam.gls2
anova(clam.gls1, clam.gls2)

```























<!--chapter:end:GLS.Rmd-->

